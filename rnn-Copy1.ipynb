{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Paragraphs and Tweets from Harry Potter Books and Donald Trump Tweets with RNNs\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Imports](#imports)\n",
    "* [Generating Paragraphs of Harry Potter Books](#harry)\n",
    "    + [Data Pre-Processing](#harry-data)\n",
    "    + [Training and Results ](#harry-train)\n",
    "* [Generating Donald Trump Tweets](#donald)\n",
    "* [Data Augmentation](#data-augmentation)\n",
    "* [Model Building](#model-building)\n",
    "* [Hyperparameter Search](#hyperparameter-search)\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "In this notebook recurrent neural networks (RNNs) are used to campture the temporal structure of textual data, that is then used to generate paragraphs of text from Harry Potter books, and tweets from Donald Trumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>\n",
    "\n",
    "Import the library parts and the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "from text_utils import * \n",
    "from losses import CategoricalCrossEntropyLoss, LossSmootherMovingAverage\n",
    "from activations import SoftmaxActivation, TanhActivation\n",
    "from initializers import NormalInitializer\n",
    "from layers import RNN\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import AdaGradOptimizer\n",
    "from opt_utils import GradClipperByValue\n",
    "from lr_schedules import LRConstantSchedule, LRCyclingSchedule\n",
    "from viz_utils import plot_losses, plot_costs, plot_accuracies, plot_lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload modules if changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Paragraphs of Harry Potter Books <a class=\"anchor\" id=\"harry\"></a>\n",
    " \n",
    "In this notebook, I built on top the layers, optimizers, losses, etc. used so far. At the core of my implementation is the ```RNN``` layer (recurrent layer) that has ```TanhActivation``` activation for the ```a``` vector (```a``` $\\rightarrow$ ```h```) and ```SoftmaxActivation``` for the ```o``` vector (```o``` $\\rightarrow$ ```p```). All of the ```u```, ```w```, ```b```, ```v```, and ```c``` learnable parameters of the ```RNN``` layer are initialized with the ```NormalInitializer``` initializer with a mean of 0 and a standard deviation of 0.01.\n",
    "\n",
    "There are 80 distinct characters in the text, so the input dimension of the ```RNN``` layer was 80, that is, a one-hot encoded vector generated with the ```OneHotEncoder``` class. The output was also a vector of dimension 80 signalling the most likely next character in the sequence. The sequence length (which I call ```batch_size```) was set to 25. The hidden dimension, that is the number of hidden state dimensionality in the ```RNN``` layer was set to 100. The initial hidden vector was initialized to all zeros, and was re-initialized to all zeros after each ```context``` in an epoch. A ```context``` is a text context, a single training instance. In this notebook, the whole Harry Potter text was a single ```context```. Therefore, the hidden vector of the ```RNN``` was re-initialized to all zeros after having fed the entire text into the network, which was also an epoch. The ```RNN``` layer is given a one-hot encoded sequence of characters, and for each character predicts the next character in the sequence.\n",
    "\n",
    "The ```RNN``` layer was the only layer in a ```Model```. A ```Model``` comprises any number of layers (one ```RNN``` layer in this assignment), and its loss function was the ```CategoricalCrossEntropyLoss``` loss function. Since the loss in this assignment fluctuated considerably, the ```CategoricalCrossEntropyLoss``` used a ```LossSmootherMovingAverage``` with ```alpha``` (the moving average constant) set to 0.999. The ```Model``` is first compiled with the loss function, some metrics such as the ```AccuracyMetrics```, and the optimizer, and then it is fit to the data with the ```Model.fit_rnn``` method.\n",
    "\n",
    "The optimizer used in the training was the ```AdaGradOptimizer```. In the ```AdaGradOptimizer```, ```epsilon``` was set to 1e-6. The ```AdaGradOptimizer``` used an ```LRConstantSchedule``` learning rate schedule, that is, the learning rate remained on its base level throughout the training. The base learning rate was set to 0.1. To avoid the exploding gradient issue in the ```RNN``` layer during backpropagation through time (BPTT), a ```GradClipperByValue``` gradient clipper was used in the ```AdaGradOptimizer``` to limit the magnitude of any gradient within the range of -5 to 5.\n",
    "\n",
    "During training, the ```RNN``` layer was used to generate a sequence of 500 characters starting from the end-of-line (EOL) character, which was the full-stop. The character sequence generation was obtained with the ```CharByCharSynhthetizer``` character-by-character synthesizer. This callback was called every 10000 update steps. Having trained the ```RNN``` model, the same ```CharByCharSynhthetizer``` was used to generate a sequence of 1000 characters.\n",
    "\n",
    "For generating Harry Potter texts, the training set is the full text in ```data/hp/goblet_book.txt```. There is no validation or test set now, to make the running time of the notebook feasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing <a class=\"anchor\" id=\"harry-data\"></a>\n",
    "\n",
    "The raw data is the whole Harry Potter text. This is the only training ```context``` in the dataset of ```context```s. The raw text is first split, or decoded, into characters, and the set of unique characters is extracted from it to yield ```chars```. Using ```chars```, the decoded sequence of characters is encoded into a sequence of integers, in which each integer corresponds to a character's index in ```chars```. A ```OneHotEncoder``` is initialized with ```chars``` that can map an integer into a one-hot encoded vector. Using this ```OneHotEncoder```, the encoded sequence of integers are one-hot encoded into a sequence of one-hot encoded vectors. The input data to the ```RNN``` model is the dataset of ```context```s, where each context is represented as a sequence one-hot encoded vectors. The output for each input charater is the next character in the training text. Again, in this assignment there is only a single ```context``` that is the entire Harry Potter text. During training, each ```context``` is split into batches of length ```batch_size```. In this assignment, the ```batch_size``` was 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters is 80\n",
      "The unique characters in all contexts are:\n",
      "['H' 'A' 'R' 'Y' ' ' 'P' 'O' 'T' 'E' 'N' 'D' 'G' 'B' 'L' 'F' 'I' '\\n' 'C'\n",
      " '-' 'U' 'S' '\\t' 'h' 'e' 'v' 'i' 'l' 'a' 'g' 'r' 's' 'o' 'f' 't' 'n' 'c'\n",
      " 'd' '\"' 'u' ',' 'b' 'm' 'y' '.' 'k' 'w' 'p' 'q' ':' \"'\" '!' 'x' 'M' ';'\n",
      " 'j' 'W' '?' '(' ')' 'Q' 'z' 'V' 'J' 'K' 'Z' 'X' '0' '1' '6' '7' 'ü' '4'\n",
      " '3' '9' '2' '}' '_' '/' '^' '•']\n",
      "['H', 'A', 'R', 'R', 'Y', ' ', 'P', 'O', 'T', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'O', 'B', 'L', 'E', 'T', ' ', 'O', 'F', ' ', 'F', 'I', 'R', 'E', '\\n', '\\n', 'C', 'H', 'A', 'P', 'T', 'E', 'R', ' ', 'O', 'N', 'E', ' ', '-', ' ', 'T', 'H', 'E', ' ', 'R', 'I', 'D', 'D', 'L', 'E', ' ', 'H', 'O', 'U', 'S', 'E', '\\n', '\\n', '\\t', 'T', 'h', 'e', ' ', 'v', 'i', 'l', 'l', 'a', 'g', 'e', 'r', 's', ' ', 'o', 'f', ' ', 'L', 'i', 't', 't', 'l', 'e', ' ', 'H', 'a', 'n', 'g', 'l', 'e']\n",
      "[ 0  1  2  2  3  4  5  6  7  7  8  2  4  1  9 10  4  7  0  8  4 11  6 12\n",
      " 13  8  7  4  6 14  4 14 15  2  8 16 16 17  0  1  5  7  8  2  4  6  9  8\n",
      "  4 18  4  7  0  8  4  2 15 10 10 13  8  4  0  6 19 20  8 16 16 21  7 22\n",
      " 23  4 24 25 26 26 27 28 23 29 30  4 31 32  4 13 25 33 33 26 23  4  0 27\n",
      " 34 28 26 23]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "There are 1 conetexts in the dataset.\n",
      "The context at idx 0 has 1107542 characters and each character is one-hot encoded into a vector of length 80\n",
      "The chosen EOL is at index [43] in the unique characters list.\n",
      "The one-hot encoded EOL vector looks like this:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "path_to_file = \"data/hp/goblet_book.txt\"\n",
    "# read text file\n",
    "with open(path_to_file, 'r') as f:\n",
    "    dataset = [f.read()]\n",
    "\n",
    "chars = unique_characters(dataset)\n",
    "print(f\"The number of unique characters is {chars.size}\")\n",
    "print(\"The unique characters in all contexts are:\")\n",
    "print(chars)\n",
    "onehot_encoder = OneHotEncoder(length=len(chars))\n",
    "\n",
    "decoded_dataset = make_decoded_dataset(dataset)\n",
    "print(decoded_dataset[0][:100])\n",
    "encoded_dataset = make_encoded_dataset(decoded_dataset, chars)\n",
    "print(encoded_dataset[0][:100])\n",
    "onehot_encoded_dataset = make_one_hot_encoded_dataset(encoded_dataset, onehot_encoder)\n",
    "print(onehot_encoded_dataset[0][:100])\n",
    "\n",
    "print(f\"There are {len(onehot_encoded_dataset)} conetexts in the dataset.\")\n",
    "print(f\"The context at idx 0 has {onehot_encoded_dataset[0].shape[0]} characters\"\n",
    "      f\" and each character is one-hot encoded into a vector of length {onehot_encoded_dataset[0].shape[1]}\")\n",
    "\n",
    "eol = \".\"\n",
    "print(f\"The chosen EOL is at index {np.argwhere(eol == chars)[0]} in the unique characters list.\")\n",
    "encoded_eol = encode([eol], chars)\n",
    "onehot_encoded_eol = onehot_encoder(encoded_eol, encode=True)[0]\n",
    "print(f\"The one-hot encoded EOL vector looks like this:\")\n",
    "print(onehot_encoded_eol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Results <a class=\"anchor\" id=\"harry-train\"></a>\n",
    "\n",
    "Using the model and training configuration described in previous sections, the ```RNN``` model was trained for 7 epochs with a ```batch_size``` (character sequence length) of 25. This was equal to approximately 310000 update steps. The results shown here are of the best model, that was deemed to be the best by the training loss, and the visual inspection of the quality of generated character sequence. The data loss over update steps during training is depicted at the bottom of the cell below. Note that the loss could be shown as the sum of the losses per update step, that is, the sum of the losses for each of the 25 character predictions in an update step. However, in my implementation the loss is the mean of this loss. Therefore, in my implementation the loss is expected to go down from $-log(1/n\\_chars) = -log(1/80) = 4.38$ (where ```n_chars``` is the number of unique characters) while it could on the contrary be shown as going down from $-log(1/80) \\times 25 = 109.55$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: rnn: \n",
      "\t shape -- in: 80, out: 80, hidden: 100\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother exp ave\n",
      "adagrad with constant lr schedule and clipper by value\n",
      "\n",
      "starting epoch: 1 ...\n",
      "starting context: 1/1 ...\n",
      "step=0                                                                             \n",
      "\n",
      "\n",
      "VVp\tj6:^/DAN9_Lim-nj\"W,Hgdri\"fP-}AR-k•••Ox,D}I•kGgGswWd\tkfihLtd4?zlv!WTEtN_0,LE^W6a\n",
      "S1cxK\n",
      " (__d:(4Nrsjo11 S^Rd}KSwowüydhCmP'J_;dd/Ls•AjAh YOD:fXIGW1yjpy691\n",
      "7o(sGWI1X,awEK/VI6K FwEM•UQRh}'p9GAG,\"K6LfsYHJ-?sEGhKHRy;?cl0thPYP;MI' 7ODnz1L JmV9cnww\"\tb9/gJ'FYATLXUV\"EV9}jjml^ 4C.uy!G-NA2vOlaxIxOkOm,StRwH\"\"Y}pMnjG!._Jkt(Z9XAz,ünb3yG;HlK.Nh}AVGL!Earxud,KKN• •TYj_WxEDNP/uthdENZLPgüoz•AO;üd POB7}E1Q;1F,vxU\"SmD.dTmBMyFu\t0fO41UC \tY}WyiNo-_OrH..EMEul\n",
      "ETYoz1qaM\t?3PxiQ}FY'PMpAgnAl.HN4v\tk,ü-S\tOU?vU_•ecXO\"XzZYM_jrL_rWM02nOy^oZfKFr('z_l4B IVlkIfGa612r^d7rh'W)p6!-1GmaqmnXrq\"\"p•\n",
      "(2k7;vZI99:uStWULh.n\"dVgz6Nqu•PW-•iP^cc,}dA2)(OQW9uW7odTGpSGu}F(Qz9hMDWdf'cVy1e;\txiYfW\n",
      "qT'YRuSv!}uWkgMI1UTo--'t3zejIB;sBgOhü}!B,\tybnzüX_3TT•R1nU(cl ZdQlC(1zO\tY•GliIB?•!GJ:YtgtDmMXQRJ!pHHECY\n",
      "FXB7LP)\"klyZEC(-we \"W•\tomwO?UWCeNSz./;ds}E,ü?!.Gq xFF ,Ub\"frCPKN3ftPjLO?gwuAQC^}??YwWRc^R. -jDd_wp62E-TüX•ZT2HbVü\t'TpGf/ BuE2pnKj)hGHOCq}3/e!UNRwAa}FSZkA_\tGA,DZHOsH',\n",
      "CY_NJsEYGY40cTenG,z)6Q1C(;v:gu\tJO)/N7WvGüvBO;Rcuy_z?7/e?:KX9ZUpCZn-EEB\"Bz\n",
      "C'•m\n",
      "                                                                                   \n",
      "\n",
      "\n",
      "step=10000                                                                                             \n",
      "\n",
      "\n",
      "or lmipibeeft, pies lankud. shew.,.                                                                    \n",
      "\n",
      "ang ulrrerer Ring faracat ror Biyo Ilrry leug yaver is arrl. Riey feaasle and' ant shangike thr.  Thay werinl he he yh.\n",
      "\"Weny binert herl he ist souve\" thacave ig. \"Ifyy the olkeritiile bopr. aod uii ulPa at of . \"Heaand amuts\n",
      "Her the cleire thasd  Gache.\n",
      "\"At akl\n",
      "\"Iy ain titave kof ivivt alnt, the sherry -lleren  fovavy it nsond ipe the lftofing bragheasl cBomo raiyd pof raldaghe can in. - ea at bar aimheangre Damoit bele oinkerib  ute wamtipe terte pamasm sam rof Conor, shas he c- ang acas.  fain arm of hatovay uten lave ciesely courent sarsere orlatem to amed Eleat . \" ay ann ai Bat apetg afnem ando toiy. ill\"y anc ave Fobe ver Thland dataoirrr ot cur iongebea to. ot mal omex kumo astabe caang cemes an om diste thanle.  aca ke ainghlr ef loe le p.\n",
      "\"Poc.\n",
      "Sthe zo gyag opgeed yo ace ot ae Gar auland mo tstasl lko nererime tine habapnc thatj\"le hetilked Hakr Heod Mmlyaclecear ofr oth  meriye osabs ther vas sidd oHa laydr cos mer, ocharlen Dut bakish s\n",
      "                                                                                                       \n",
      "\n",
      "\n",
      "step=20000                                                                                              \n",
      "\n",
      "\n",
      " ney t?\"\"Des wuringh he aid. \"                                                                          \n",
      "\"outedi, Caroiriibl seo bi ting abe searrrre the gu wains ye t erii gas sile esiin, ipitbey nis Hoed thit deus weykiy' k i - yo taris asly - dinly Hacded thim Pyer.  Mou a hos ur dou asd  feaewif highey ofed he warind.  Su rrrrark ing are' and wace Hered biming.\n",
      "\"Bfhe he riy doaedy esi ove gilm eig.  erero - wiigizons sy,re ont Sor towat. .d in' war ace go sbbry wit lin say byhighay cous, d seaevan sin tisre oa doon serelr, ank qugan adt ted ile ing thowe tamist an'se freach shit s?\"\n",
      "\"Sthim cout in sSanidn an yist Tho fanded ciAg ea thond, efrisamist, wid?u'. he aghe donsat Sune Agoow bont ir shit and?\"  Isrnn.  Hat and, brerind shing.  elit ank tiint ofin isgry his?\"  Mous?\"'se in's, en mbey al \"e eliiid railr mace He\"gh haiw yrout'run.  Totef, hoam,iy ily hh tisg tand smuld thel. \"Asey tobe fidenel, as nert, warcer am wisn KSay sakon C kerit reery weh ant far. .\n",
      "\"bin niesk wimh, tou's sit mam thore eruby a wint maf lindinkis?\"\n",
      "wain she hhey. \n",
      "Sof.\" dony\n",
      "                                                                                                        \n",
      "\n",
      "\n",
      "step=30000                                                                                              \n",
      "\n",
      "\n",
      "aed, towe lra hpidt wit efnouse agh - in ballr shas wetendevenches tackienn Femes t apd theabis.  T\"Ibe; tharencebl the co aco twereren!\"\n",
      "Tg ate the mand wamfed themengingim the to sows eef cofboinge tativle and neideded ther.  ime pongen ton at the eed k a wot whopex brapid reaed we byope tod tone ghis the hh ove fone brepy mey the thast ounat serle th thakunyry cfant ood wainttlaldy pidrean!  Wied loke rapetss serrenf and pis tinrey, shing, thite racanl derslers theie is therareprent rupe thes priingeromy so so bortebing her forlys hert eton?\".\n",
      "\n",
      "The save rrrrroucke. laye a wpell cisarogs roming and thing..., bmatil wilat hete spif, lorel, rthe csised-ys peying is, h ung piog coed lideaiss telet?\"\n",
      "H sped laprentem pon fin onscoprevecbed the tthavancele eat 'teakird fowing ank and inkiw Paid bot y!\"\n",
      "Yorisey weewachy hes heas wind ilefs were wer wind. Las abdt hactere, hel, thry old home., hhing we messepdikell thempes to ro wastrey lacd ne pow-telld she the tor, lllous pmring peco tulm\n",
      "                                                                                                        \n",
      "\n",
      "\n",
      "step=40000                                                                                              \n",
      "\n",
      "\n",
      "d hapreded, Hare and Hartif, Iglingurre pougath ners hfther das arred he, yonied; lerrrefed bearkorteser!\"\n",
      "Awe Vids wooEd an rrurthereeteseringeet hor'g thary wotle muscufed she Pruedt seslt brauach axned ping bath.\n",
      "Ame cofs,\" nand Hoes thy Harr ard, baved m\"Vis igererensh, aId Mloned tile oalot theveats brhouud yoong to led and secouthiring to thitsginge'sg fonct thyrerer's of litenale tsrcrotenert lean upfing the sun, Anlm'mesh ground ontamft; buy ciscice agiind Ag; Vforsiled glWwantte fen sarh and  in nHaeed bloole nid wo obl-ring was grome- VAtlisded Sfes land toroteru- ofll, ere't sify. his weanye wifan bery brortin gid uld Vis and .. . hem. Vasas igufd sastak gow.\n",
      "Hancw; the  nasl grouletid the wettre alt trave mtass the hey - hhi gleurry.   of fadnk alh hing garreg glound me. .s sand thisg -trr Coalet ighen, drowatit prou Damenss to thang totacdis enly Voike fis tra iut the Vided ondot Vofighe ward.d onath gedreverinclir toomtefors lfeyfe arry thericghem Eord nedevermis andme mi\n",
      "                                                                                                        \n",
      "\n",
      "\n",
      "batch 40001/44301 (n_step: 40000), loss = 2.2052: 100%|██████████| 44301/44301 [02:33<00:00, 289.52it/s]\n",
      "starting epoch: 2 ...\n",
      "starting context: 1/1 ...\n",
      "step=50000\n",
      "\n",
      "\n",
      "engped Lidens hey, shamackidengeyy sefovese't was and, tid lead ath juld atnin's though.\n",
      "\"I fut, becaputs the sa-ping a lene'me to blanteninky ing ben.  The faid aid arlenteraghed ur cershocch birg - ifedgme.  Hadry arredtire otcaby coemede, I dos iow't therow wesd noum hechs mainy they.  The, troumps upry barougly teauled cfuild, fornedg heitting sae aadslmeld su, Choon Weid they atpetselerut ot hall Kof ledel. Wioney hearl Ba-t dof themtley beogeyme mulll eveid himes ously uled ere, sevanu clegway ronigh id and leand ane whey, to skmt, uleesgy the caugtre lerevey mate fale. \"Thalpadeasst yify fet muid lout, agh, sadylese to mu mthillys lin. . Walar, milg.\n",
      "\"Thighiteetly!\"\n",
      "\"\n",
      "He fid onsimit Gapan, ageeple wlone and and th at of.\n",
      "Mrundmeicand iotlyal waed, Qumolye pecooce Mire ther bering.. Higpay, the ky hale cofing tigling poulrivyy letawly gand.  Shed andy illding Blroverevered hin stoindeng peonglly his hey.\n",
      "uude, receets osliclm leachat--\" - rou medong earrey blugh ga, jaky muny.\n",
      "Le\n",
      "\n",
      "\n",
      "\n",
      "step=60000\n",
      "\n",
      "\n",
      " dokad dawind nome tif to asoudey.\"\n",
      "EHony fowfs.  Henupabin, he kim, Gurd\n",
      "nwazared. Wize ghe tuus an acghosled e surchint, I asaufng'n da thed ave ofr oud of batilvey ther of wart, stite Pro?\" wale ootrou brmes andy und Mre listing etoting fok,\" vearpleng. .\" MOd agh, \"It,\"\n",
      "\"at lunpanng out ashly an to an h bumentizouted dilde.\"\n",
      "\"Ro\n",
      "\"Dalge old Cing med tuming waskeR SHhour avem and thit?\"  Han, alk ged hartly ientled,\" \"Cof engdize teond, - grapr,\"  Ird Hiny sifat, HI therarertist a rtheach thouivecbeel's on I't sumtanne touth). \"I  ard, chatten, luld soo, pither, Gretel nelles yeos, ing an form becn hivas, Heh warem, and tovast he jopfithe youst Htainh. sesints parmer Homate put eatp gronage ing in the ar werem wookt.\"\n",
      "HamTlen, wTobe yap worrmiment, thind ghe the to sad. . It luping bitse the wingain sifion to Duth tigith the thoull, exon, sist't ind, tolired.\n",
      "\"I ore ale I'w -\" \"Whennackit jourl\n",
      "\"Suitery  utld.  \"Fre larer Lupl, muned hareglen, Had fraine avess himes.\n",
      "\"Bere, tame hasf\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=70000\n",
      "\n",
      "\n",
      "prowfs hemash the whou the foridos cantees wit ho roretp and cloaloupsced to\" sod woup Harrid?\" singioner youbry the dorrry and pist gos and Doo lurtut urry, streain doridxtionkik!\" sin waid Hyove - scach hery mouldcaghesce pagaengs parours? e and turer, do.  Eot and and of - and brim fing eed do'dow bem-up kll he was to Measothe wapet bith insicoling on tha, the lavered lFrowh and thaapit the Vory of and Heveing jpas to weasker enof ut abat and the way pasts Pakesctim the Hon whor!\"  Ron selith yenoatile of tus agen hered.\n",
      "Nontiknex pleeny hing at allen mang arey youster pum!\" the, the inginly inded indo gow tumied.  \"Thas be noleml dow gand wallt there weamn shuris' forinnowe isling Sthetavart souddou'n der? h\"I wallorsire to heand - ha atad at roarnes wat ceas,\" has any'te heor cfmat a urofore ak, abeut baid Eren. \"Ah tap hele Is. e fadant and whon seer on bearcelel was asy deapferden Hack ha ghered sugair, the brore wize, ha goulpflit.  Bitiswed netied mame - ple uid nearringent bu\n",
      "\n",
      "\n",
      "\n",
      "step=80000\n",
      "\n",
      "\n",
      "arry sant ing a dart. \"\n",
      "\"Kas warkol hatt vis roweer blorim.  Eb now.  sis on, I arouser wabl.\"\n",
      "Bingt ean,. ou Itiking ha kf.  Horwit ead cleasme we wam Parnnet - luver werore e Lagrid.\n",
      "\"Kar, itow ve.  ther, beys Burgortidore Sow he hellen.  A ke ablas thare. Mred a runs te uncle lekedre..  Mrof to gemot!\" sady.\n",
      "\"Haid Mevere Sureghed.  simling and Crealled Thage ugrenly a cughe to nare fors Rme Dund.  Harry feker and agly a me lo fantldy he as wescre, whoud Soseru laled, belled varmy indenlitly aid an thy legirner, do - C bedecry.y Burr ner. Yon rofling a fme laky.  \"Be veom wan. Hyid wane did  taid the ceitcood dooowfing oor of hion brord, age ky Mrou coord fendered.\n",
      "\"Choot.  My nass tifedse roorbh groblencd.  \"Lon to to\" modser, burm in Duok padffrys.  Ifwn at ank an't - in teedze't.\n",
      "\"Nose syon a wof, wis lemwhe Myist ands and and Se laver oft fut was loure sas fall at buws herr. .  Harry way ce an.  \"e was he neace, ul sad to don broalesmed waoveed sere wan - Vofm watt wallitet, wher\n",
      "\n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "starting context: 1/1 ...\n",
      "step=90000\n",
      "\n",
      "\n",
      "s way wa- hollad illin, hinsts bintay ot with tay hhainsinsher awthe wilengant, ithe?\"\n",
      "\tWous ferw hichther Hefprer mioss of - so theeldelis the Velots, tobut evoury melf wap he whao hum - hace avalloune vaJk themersing and ilveriot ta ontt sast liny and.  \"Asc cond himachthestle and ind.  Helw ot boitesskey fruuw Dugredre, and he he hider Hald and barouge fory groard how deasonednly done ware Hevalpicl as pote sisteetled wicenmiz.  Nook sid't as sincly. By, mat roit itilizls his dolets this the core, as has enm tharer wourly's wack aiventing ultrey thire kin Hernche abadll Foming qudan whew; salwechis sas bout apres harry, andsel nen man. Hand bing wouccerortramiromeor I gcapy his achich.\n",
      "\t\tItamp way the. Thas walemarcly'rist who iTls.\"\n",
      "I youp his him themy here dosthe and a Ha kmunt; Pus t chem min,\" hambs, bola Hanes, pasmesy intthe hory.  Hers iughis aclle hirn aughatt'se sud at the ho'm taid high hisererrly Helusing etanfexin't spat has hal spre, hist's senker, hon h sshis facat fo\n",
      "\n",
      "\n",
      "\n",
      "step=100000\n",
      "\n",
      "\n",
      "over gis, Wouts hearvy.  Thissing e .\n",
      "H yof the cove to cey sat semed coulde seamiow and dove and havey ho michike the dolf grow he sayy douchhe ang his and a the foow yome Qulently.\n",
      "\"H on?\" \"Neatund is in't shores.  Mrs wum, shy to waione.  Frelvo?  BeDor.  Pecring of wastor an?\"\n",
      "Fner . shingth!\" \"Fikteers. Hagouss, eave trored -f yope thatiz ycer, the the whene, Hermed wadsing heWoed ined bion, quluping sevekemuch, men's youst fol, Mire oss's the in this luts boenwor the onge'd on't teas?\" 're bat of and.\"\n",
      "Droby?\"\n",
      "\"Fupitatlye waw Peede sat mat ell ans the weor wavey coed lik this tasty Wary grigh wowhader. Tar the wooled ony and and Fims uned her and to in? They.  He mo Good. .  \"Fedor, I yourr hemicery comt the abbe - I mun a wheft?\" sherowin sedl le fakit youstomaigopage hith up thim the waid bat, moEy, bon, e deFwe poogey Haver weming whor beang awow bem onh co'n tot nhore ofed, she to la bedlro thau walyey vion a youound qurat of thersly quovebe ginizors beoudw.  Smey to a treen \n",
      "\n",
      "\n",
      "\n",
      "step=110000\n",
      "\n",
      "\n",
      "atted fing ben ble henp, smy it acr and hion he danoo secten  ancrencl tharerming on waglpk nopled Hal, had bunen wacrid nlatpat is to cof appunt.  Haun veortad in?\" Alvo?\"\n",
      "Harremirstho him cofme ing hill ingomme figly tam welilettow iovice to kind that hillift licoost foroked peting the a foncaly, seckic; plelen'tagry he, an and cultessleveefoed dored sto anrs is streht obles orr, and lingts. Harry nomly the jurn fustit ber're wagithrouhe kndedey the the tilled it sedasled ha fecbe coming to smoped'vero abor beut nome Cifod fort Gosnd hhe whechett an, Hard tuldendinst at.  Oppekormit, ave reGraid.  Haxbreem ints in thind suid?\". and a Hand lideby.\n",
      "\tEHeing stule he thelas on he was a this just iped cad-... . gridne warrock Krtat tling houg the dover inthe Ham. ..\n",
      "\tI. . surd Heaing lars but star Hion, rotre stheen Qunarigh hire ofan!\" Harroking and Harry, I glat, it tis is Rhis surnor - muming fuleor Hthong, and, wain whin. .  Ofenndecenidlly patmit thamp time ot the ther the lncast eld\n",
      "\n",
      "\n",
      "\n",
      "step=120000\n",
      "\n",
      "\n",
      "y one, ther.\n",
      "\"Thast and hearry.  \"Madanden he at arer, dannt coustaringly hast forod, sing uwh, Sthas lich and.\n",
      "\n",
      "Herorideing a Hamy alling, Maglired fraw poufre.  Hervers pakersest it fund ore Spie leand tout, Porning to e wroy baid hum tore Lurreoch surighteded home poary petulinlltsyy and and ito neal?\" taming bae of was hhoas pour obd-\" ang. seas with bat, inon ceround and cay grassirem iner wint of heughene ontitist sofny dosrlis catiren the dowteney, Indem ig.\n",
      "Sand yourd porouletlymacks tpand - barked, Hening me stok the sably,\" The and freste the were forsy, rurrupor inen tur I bywally ourcap, wher whery.  Whitiscer cas,\" Spet bit twinily.\n",
      "..\n",
      "\"Asschelsioveam. I abres lasn't and yow thanes.\n",
      "Thas cof benin yould byyeppe . Woulousming a and dow't and weiun-t - Bugrins, to kne tomey taid Halle hioa Hyot in Sthery weam, boker and yointt at that avook, ass avared arking samus thas eend selotit.  ERon gronem inou Buiset; hadnder se-n at sother Magone ofghtived wat want wat the rishe, Cf\n",
      "\n",
      "\n",
      "\n",
      "step=130000\n",
      "\n",
      "\n",
      " tis.  yed a kaked to themenchevory temiow hownly Mnane, h somyansny sor fonthigh rrinnt frenele she rour hile monto on pe barefian.l hu'r mis oume raed.  and and onmord houghintithy wot samaten Me ladmas courbough anp figlorty lectschented bank to waing Pobd figke- bast and aly burerimzing ou.\n",
      "\"Youow fakaed; rein kainly wear it oft wro wooking thir med gainty yook . . Wwaricand o to o corsrichens as is selon.\n",
      "\"Oftse'th fake les, ho rilles yefthizy thas Duf. .. he as Sowp the yomater hamh in snist it himas antly complew?\" a mus noker Parcrersing benstove on ee feimat ploked noter; oming aterhey secreling fably lam, Mryill yin Piden intess, I for kio - Lost high sel- soming satcalnepeb wedye mas notlmoring wis .. \"Non on mey as we ine yome lethe utes Mentilled fomped here in thinew an yof tut ther Dutcer that wourd e miting the hasrey themin nithe wagher the mintoft warttat Morken!\" sfses to to Come tad whe ryoak Chu coofter ha hempawife't the sting he tsaigWhut mumy tas DWod hout to of\n",
      "\n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "starting context: 1/1 ...\n",
      "step=140000\n",
      "\n",
      "\n",
      " Thes,\" .  Soe the yon the suid benoult onthe sas shemas theas tinclergacengatly towain't th ine highed boucled, ing hadurch you weryy tedthe kiag retenth had - beyting trat onlag thad Dormn bound, ally and pall wade sthee weched the suver he coest lany Ding cand bebll - chin the's and Lalley at liged it and wurires's tom poth the eloont baigho she het the wimel menen lagow, grat.\"\n",
      "RWirif's golf wele Vult ling hund and as tugitley soovens, th yinen the clouscs himand fadecr.  \"SKacGor hidl Ro the fout thas the thers wis stintechh.\n",
      "Whe nome tine ee and mme asly to pany wein thele g brosatel.\n",
      "He of this the the.\" . sintre thans.  The waid stelf.  I'vow.\n",
      "Wated whan lass: Shany to stloursosesthing yery innoy has doG was las!\"\n",
      "Mer tantes in notoe warister bro to thy hoor Geall Rottle- yound Duked the kever to the timing somt, lnopening lecthe! Anour, wroe ou th bath wos gobow doLmpanterly.\n",
      "T aid.  \"Ilute.\n",
      ". Washed heth eds a and the stEepling ime the swal Qurcesle.\n",
      "\"Hergele panen't ha.\n",
      "\"I p\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=150000\n",
      "\n",
      "\n",
      "hebe he jur co arondarou.\n",
      "\"and scaing hemd as he harew yons gonyy at to cron had Vearend eloed, whe neefle, wher bat?\" sedy do himang wearouw shiney huw sedgir thill to coment eeve thicand panked to laybendare hounden, bumackaron halltt?\" were shoumerel it whh you.  I the'm\" loogn to uched woun Harry of walcort ing Bum warsinat's vedn't 'ie ted Hoofed on her ce-pin their yopent ETon onlene seam glear?\"\n",
      "Budnemer yon boared.  Yeams)f earresdlitlit in hady thing and as thane fay, hrey fat'lede yon Moreg pChe ateror sheting eaw shound...  Bet hastilenord o he lad\"\n",
      "\"Theakingirer Dughen appeth waid.\n",
      "\"nonded Veainnen, he he coumten of to dome didged, eo yon mlyan.  \"re in ut whoncon't.\n",
      "\"intean, aidg hits seemod.\n",
      "\"I the to chon at exsnd a in sad ouy on hes.  \"Le to for ppat lat one fealidn on, to allyorey aaly beditene oods.\"\n",
      "saded uld an. . . . Hamp  topeve ontudevundthe bus. . . Geare lide thearry.  \"Som ae - yowed withe to ho juld at of has shasteod pit sfade Croaid Bur Gare fro who yinet, \n",
      "\n",
      "\n",
      "\n",
      "step=160000\n",
      "\n",
      "\n",
      " tealy.  the Mand ment bake. Harrid taed veline  bbom s ssayly pid the, swapalat, - habing stiold's shen wac.  a ghurd.\n",
      "\"I'k sack.\n",
      " \"Havist yele as rnood abrowisis! Wakion.\n",
      "\"Whed ut con't pal Gooe, w cloooking Sol.\n",
      "Hla's ir wigh pents elid Whasing hion pacts. . .\"\n",
      "Me on Hary wery roed.  Haid C whatinnioncled.\n",
      "\"Sing rop jomirby No.\n",
      "Hy.  Cloo't yarcangeal exk bad idizing sald, wor, juncast'm hoem,\" Hadry . Onoud lI loup, haning hover the somamas goparidn in to have,, . \"Harrid n I wat and o or in wazacd.  Has.  Da Harry.\n",
      "\"Sken, Lasl, aid Ir farize eting beyt Heb ail nobdsing haar seigh. .' \"Ye freld speck.\n",
      "\"Weisescore op-Rt loghu to ther wout abon.  \"Thaid the siok sultiun, Roun and. .\"\n",
      "\"no, and liok ben to coving for iou bend ad dronkic.' nobe Astly.  We deasad thoums.\n",
      "\"\n",
      "Hargin..  \"The nob't them and deaw of wizand youst poky andnh tup e mping thle w. The aNof the and band-n, Hper?\"  nomy?\"\n",
      "\"Eve and Her icr?  Haid Hicane, be she yos mous hame wins, sassiwe!\" ..  Ikped. \"ryou?\" \"Gos, oft\n",
      "\n",
      "\n",
      "\n",
      "step=170000\n",
      "\n",
      "\n",
      "e he trrur. . Ro parken femf ons and Lontoredinden tifes in unl., and vo able, Harry oR.\n",
      "\"Ther?\"\n",
      "\"Oom faramt ofat esaringen, Ro Hae the baent and as inst.\n",
      "\"Latten not - clate kik lotsare fiche; tat yerd bring look toe wing her?\" qukup in Harry ley lackiog the heye here fechetatikerer in from thing le iw the at in must wat and the adde swailit as meyth hagh he'siriout dod qukpenide Ros.  Taivert mensor the wandad of manides at exemkes thas rarbusten as alve pice trin saswpere of be.  Ad charc't, prasly have efwer hery trare she yat was of and.\n",
      "\"by cou mthice thuckak off haid,\" sainch ain, Hou's the he obsege moe Deversth ank Madeves, dond of he berents kino it Sforts.\"\n",
      "\"E\"You the vis.  Buteldol, surw'le she'd intant cher Spinen heiles trrasliontted the tursy lom lakded, sod wapaill, of the nots nliss!\"\n",
      "Hind, tilette of tharglentut Tunk.\n",
      "\"Herminfus da of grime ind-\"\n",
      "\n",
      "Sod ppob't. .'  thise Ronkat, gisho eek Kroused am e fnaplen, laromen itat hes fort fpaperivest dable jukigh cene. .\n",
      "\"ne d\n",
      "\n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "starting context: 1/1 ...\n",
      "step=180000\n",
      "\n",
      "\n",
      "rrd waing tarting at.\n",
      "\"Thack whe seickle.\"\n",
      "Walved cod. . high coky loow. . wacaley lecpfees, aid when, behen's to uncouge.  The natt hivace Velecld hair rould sclons as neen bever of tlery stor at.\n",
      "\"My shey ufdil, whounded mine,\"\n",
      "Harry plounceds him had Duthen.  Alfthes..  Heok inded ming facl you lamevey's at scas ing thas cou-n sad bonding you Be macpller, is Woogey plethrithering-ingehor plot, ming blinne ous tho pecoos.  Voeivelishing docl cune.\n",
      "\"I lar,\"  Harrsly, to lveelbanvitin,\" thirey gepllantut hack ho shexey the binccouy he shast the asve he'n thound uth strist sinch sars the stiesid, hadry your winner yough.  The bin ell the last.  Cuf - pad hady ous Gody the yed Dum do.\n",
      "\"My sast?\"\n",
      "Fryy caascastavad cass ail.  The fy the surcot seal non hife tone. \"Magor Harry hu as on intt?\" finded the the ghicl corder one tas sas.\n",
      "Thiss be Proodicl, Dut and haddee; fan all in the remu.\n",
      "Mboud.\"\n",
      "Dut ther hery?  Ose of tamfouth. he tudourk bremall Basly and ted wagry hay on. Wil to to Dut se\n",
      "\n",
      "\n",
      "\n",
      "step=190000\n",
      "\n",
      "\n",
      "l hand qupasiveriok to um semper. hirn fres?\" tad he it a Tartre jund jord Mrof wavispeniry, Rong upge Cowantisthy an lis seas.  Cu sppoule, was uoste labints, beexky it lides gof maning at waze arobpl snoped; aliting Hand ast,\" - is.\n",
      "\"Ther's ad aulack his einsix turerser ame inly at mit.\n",
      "\"Weair, whurry sace ding pigh colgaif in thy bet.\n",
      "\"Oh byeN Privity yat theme hat rikersn's tasmin had he gouin eveed foy he gsrro the of to you, she cee, Mr It hiintever. ged hmpne he lpbaice ulcare who till and baing an an be feeting wof mouce ther hag rigatat lide, at ou wigas stpid lizet baists leat to ope sableW Moved ap a aipetser lokellack at s waung't. .\n",
      "\"ERowide frofwaring ter scinderowg?  Yourwoover at usges ded ontind the reationmuts senged as nominkn whor haid Mr the ingey om nivel, aid niIt Mdid? ., say opardene in ono icaped.  Rin thou see the hat deen avay to soed to Nondell tor - Crogtay was wearread eating and Nogew thinto Crout the on't unces,\"  Mods now fecol Noulinit who proichow of\n",
      "\n",
      "\n",
      "\n",
      "step=200000\n",
      "\n",
      "\n",
      "her parky to lostlllyon, one land yont tryorle hemboowh thoford as wilked monthy habor boonth yes snarry you'd Her bro, who yin wponed - \"\n",
      "\"Wave and ho'st to thasge yinsessit tachick his harw coato look ware-red this tand bro wpingeninh, she bet samas to drorn, and fare the Roming mooout Hadry. .  The Dof was wacknrese to the sing at exopld alk carred revot, whabow' bow sfomire theif lolsster to spiby velisclevent beff awabdy . He he now!\"\n",
      "Dills. Wame o Hoone sto! Ceded ceing im lamirne cat dolbayivesy as qother bentir seastriking fored; an!\" Then cobd baelblisst age to look to bet dock sevexad and to beat wared had and a Itare Irysly fory, ands wastangith of, homil Helwared, plechermirgowd I foun wish he wor of tuid, sheingn tulted of raddy.  Esating daly by shass nakack seythe'd sow the traring there off the was the Her, fart, yey owap fousthe, pads of\"\n",
      "Was and be and bots, a ghemions thacasabressly, of vixbons the an,\" Dard, scagoutt'th the savaid fther.  \"Wily blof bingod sparissca\n",
      "\n",
      "\n",
      "\n",
      "step=210000\n",
      "\n",
      "\n",
      "peane nor. Caingh the, .  \"Vow, hadsle Haltester opw figther.\n",
      "\"Oa spunger fortery hexsoy hem the sthe's tthe host chertle and in in bize's eroarmtes lrougl't of othe ferinong oth thirg.\n",
      "Have ce had asmas youghinof, ing panced you!\"\n",
      "Ied muron't bun has bechisn ofr to of and Whe ains.  \"I hime,\" Hatis, Hermare the Harmy he sminut seen watthr eid.  Ofesser't air was, Row forerticl't leat fouwhes Gugheret.\"\n",
      "Harry coaughele sen he dared this in hat ttrlure, Hers bonmen snmbapios unjugas so-usiy,\" \"Oh the cast cuncn inh oplling on's fing habry in to sesten arare were lutte, himat.\"\n",
      "\"Oneds Yon the goed, Henister therry Peaid Ron't ilf hod pleiontered Sis telgeghe uggourt lakos Daiw's to the they, he he pow sleawleile pinlin't whearn has: Harrused, a sthercers, was to trir.  Ye nilit they blound and they balwafter to?\"\n",
      "Ming ife,\" thluses tath I velee unas igsiout leppeed quinifed were warn't whapag lfporn't a befpirend haw Csrehe wiccous een?\n",
      "\"Prin's the Helthnin Harry dobl-s sens to foremingn\n",
      "\n",
      "\n",
      "\n",
      "step=220000\n",
      "\n",
      "\n",
      "ughinsidil seelcher aiverarermut somt dey?\"\n",
      "Harrid-\"\n",
      "\"Yeil sighes coch, adry ils jut reml my,\" grseby out the tutsed motter ented an to thacand - ver. ..  Yablis cainntion. Ween fear aboimars enots Vbuse, Ma hessry.  Dump, dixte'dy Ipld bidere to Fits, sult . Ey, her she and serm - Vik and, a forne aus toun RWhe hugrye. .  \"I tigh!\"\n",
      "\"Youdesory . Yearry, glurce,\" .\n",
      "\"I's boch?  Ye trap wit ared in were and ane nerozu's the Vouce sepssrred wac ho!\" nomarehe liknt dorioned op inap - it the as.  Vout she glithat Inded censay laddyous rearbrere gar in be in's all ond Dut Duf the saing nove in gat nister to arou,\" sagrmethe mp- at tha cfy to ut ove harry bege brofs Beloffle kfen, DTarp in waby Mored- A abves way werew anporytintt jog wat aNon Crars. Heepling pleay.\"\n",
      "\"I ea himed cook int rom hame aixst up, tiver, avey plaepleds the selded hame messer, huven il was sid, Madnn rcook to crouth dooldsating aned himes cayfert abre seals don, saoker tompor, Ha the lexpiredgrou Gry yat on be. said al\n",
      "\n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "starting context: 1/1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=230000\n",
      "\n",
      "\n",
      " ridy has kiste Mr den, whe were one thaioncul. Ma leel fegt dond.  Weep! Nitsel, pow it arvell fewt, howfll they las rided in sack caith. Is.\"\n",
      "Her chearing k\"It wat backing an shintinserteveded iny vore, ald mom beky ut, ard Dugerthy, wine outed.\n",
      "\"De qulick of then grringting hith cleayg the!\" she daistighing hir!\" He ENon in arty beee Weto yert, your He,\"  byou're, warencas frert's itty ustw's the thing, wherimated Calow hallywing and with shey windn't were s- Bandwat Carking eveed aned, hose sing frastiffer.\n",
      "\"Werme. Worken Womerent Duokel, to the and chat fared coowpngerny sqou?\"n is potsile o the me they. He Heist ables a grleyting of honen bondfe, whe conen, the Mr's and Wilping the Whey, the slivay sitea gos nizelbly.\n",
      "\"Ederesth!\" me.\n",
      "\"Winided, trechas her wiverough weiningere sat he shey. \"I whoround one Mroupe,\" !\" He was hit . Ard poked at thing to dod, abd Helned Dadasted waid bend bevering ele vere.  Dist drag a mans for and the dise, pulaimous telfink.\n",
      "\"Youm,\" sagdid He at t\n",
      "\n",
      "\n",
      "\n",
      "step=240000\n",
      "\n",
      "\n",
      "h.\n",
      "Hyorimtem sas and and Gore to scattat ping, aid..\n",
      "Searm, \"Theer ponent, Plink squzatirffed larme ce Hadre at wat have and - in oning sovery the was sold folouge ats on, Lup-bernlut yot betep and'vr Nooh.\n",
      "\"I Dumbleck infionll doice whas screpickes lom, how lascn to and cudw. Dom, we cnarkonter.\"\n",
      "\"It hell lar.. It saed and looturiscleder Tigh the ching air geen eer to seotirden.\n",
      "He a heourg. \n",
      "\"Arveded.\n",
      "\"nemion,\" sawith an were dom, ard and theroked. wikined, Thleat exking Itingcr.\"\n",
      "\"Loft sta! Cagatt'm  iff theard hinh, Thin bey the wank, said braic.  Itre to chmed sabbyon's Qowariont: Sieve't they howiking - likens co onkaner'me hin ine dantintsiuh, stif.\n",
      "\"I a Splitt but ulk fre thaing.\n",
      "\" wherourrmans. a Ma Carsink.  SOof uplow feas whauh heremfenche.\"  rmis.\n",
      "\"Impemy thante oflle. Than at ther burred compienpesendy, Lued yeld. .\n",
      "That ard exkry?\"\n",
      "\"I' we where the cpllehor.\n",
      "\"Sorgedirg wartily were shent, que hided stspir, bepeden quaded.\n",
      "\"Whelsert houp-wg a Sthsar Cnonsaid The wo, ncami\n",
      "\n",
      "\n",
      "\n",
      "step=250000\n",
      "\n",
      "\n",
      "e in a vernth with sperincorter ape, whige Ping wat aring.s ef artivers, pis. So was tad evoot cow that of her and Srey che onath Pave shiop pige a Papt gonne, to Sce a pite watastoreses een when Paropisil Fpe hime.  Harry Claeddy o are hamparong ons,\" hanthen I drons. . MP Prome peand himicen lut prick coink scarsis. Cad- ha eamaiqubat thone wang to ionswom lond lat warecl sayderoult. . hint..  Mood, hitsware the farst jicfy me tero (the alvelorenlenting stall, bived to the ple manting opiled Magh - coudselesse ponterce, von the think hin not.  Herol dooodthut dowing wan. Fsing!\" Sam she haborund patt saidror thelonyy butt, Roh bisaing iver izevion of high the Roussacepnly, to Moby'loo, In't treeaid outh halt. \"Eincabove and lpop,\" sen'm stine cor kid a turem-de Therl greom Ming ooking teat at?\"\n",
      "\"Ill it Harry to upll thous.  ser che at te shaamens coth Shotped nto -S She hats meth themavey haghy hembout dasty tor Me, veis moutitts!\"\n",
      "\"Harry.\n",
      "\"Bagm ath lpotey wopory pong haid Hagre save\n",
      "\n",
      "\n",
      "\n",
      "step=260000\n",
      "\n",
      "\n",
      "?\" Caliplink of tanh dogn it ands tory has monsy, thryend you tow twe fron hoed coml in?\"\n",
      "We hight?\" - saidr, Hying Chwat iThere who\" say-t ticus ming the sindes.  Thaic hone the yoush ablede,\" . sallifing.  It were.  Harry extred inkoing home were big the earpakinger.\n",
      "DAconled in; his thead carim\" en in feal of agaid Harrye and im, had had nouch harr.\n",
      "\"she fom yourry Mrou,\" snif Whearrid, dent for, hasw scringeen him\n",
      "It him.\n",
      "Sgis upared Chpacges!\"\n",
      "\"Asarsich, pas, T mou Alprs voney.\n",
      "\"It hadet move'd of to terwiged a knertt wrinsem talinny uns dowdinner sttath ar. Nond spufly her with it susurbred lighedly gotesly. sund!\" sostedy.  Hef, wern.  Asthy notter; gack hes jucdrided ace.  He alstich,\" the there on chormasle he his he thenc, snovat pe has barnttlerieg his jeding tevend; to at retined brist in wale?.\n",
      "Sgearan Cughe hen,\" said Cruiking sourk youghisc's treweot, aad the toing uplne, in had wo wighted herbre, wim\n",
      "Hagryousseh -nd with, to his scaufn he grong chens stheld in the wing \n",
      "\n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "starting context: 1/1 ...\n",
      "step=270000\n",
      "\n",
      "\n",
      " whit boref to tat has foras gefly ansingsaing eamereveytadle, Hthout his tup jice sacdrdy.\" sisne, Goge the waningenny at I wen ume prailed - Pototen peeted Fikshes hit MrGo beften. Mng Freess on My fince?\"\n",
      "\"Yanou sugpas, wlapthe seard samim haghin.  \"now that Yof thejy at doece forin!\" seawly at in Preaiou, ons Pill uscred thay uppllim he's ating Vere on tak elonid Harred olding Wor Maxt usky tercout all they on vacn wote-ngles at steanting Cet yeairnly.  Th ofketerens tran plpery tevey, had was ing oortnow petrrters it uid ay. Harry.  Elarbangeparejvers faced Mog,\"\n",
      "We sair Pass.  \"Moremy Pucllkere in yeeemaken thes speased olk, Weet. \"Whe. Ast Sasth it, Harry?\"\n",
      "\"Magatey theme fair avaleints, he tasaid garmacese bacts,\" som tom gegitaig anem - opr, Yeart blaixy whemorispen of onere poonged frot sordWhes qooone uing his men wat omed so wearout neryforly rad, be to at in allach, . Fre paized expughere asked tselootrit drook's inlenius pizers!\" . a rave; is obliged, it onew wank oogen o\n",
      "\n",
      "\n",
      "\n",
      "step=280000\n",
      "\n",
      "\n",
      "arvortry evoll Broofe reay. Rfreftem lund - not the I of hir air a om said and Herreat Bagusemn wigicn in't seet go fotsy that ee I brochuicken Sast eruslily thablory in Amncord prout sturntiobot, sow hand spang sinicord for lvelok, to upwoovet oS hasn't steadluting sand cowan bitthoiwa to OSRot Shisiongo who thed nivas ine.  Thits awly has yom was atl an the leeg; aucaepre easting thing smythitt wath.\"\n",
      "\"Thim thine anly. Ron iney tast MrHexting he'n ho vest of Handn?\" Row criche Grood.. Ducks gruges ruos the fort to curd wandor?\" Wifd.\n",
      "\"Wleat.\"\n",
      "\"Itst cony.  Harry YDofincood therert fore - ang in upked fen, and the tupinger ast year this and froultilg wiscand, than thion the to of she slithe?\n",
      "\"Imow the doned, eo to as on and the wige ning.\n",
      "\"OPve ther it to and ford-his his leem a the mpry, poring onth Marmbeang gring and, af, goors the to es bale. Og, a Thes of fatanlyers an \"Moon. Her nougidless.  TLrest hat!\"  herry the state jomion semuring, bund thin molevarvelfpife't -noovad.  So b\n",
      "\n",
      "\n",
      "\n",
      "step=290000\n",
      "\n",
      "\n",
      "led the. Rohy upan't him with pupicley hir sakulsed one got, Her wing stiropehad ang her to spacast aror a be ane, Harry sying.\n",
      "\"Croutes, as scproinst mort do'deh his up peer walaik \"Wae aid Bark don, and, a sno?\" Furvidentheld.\n",
      "\"Whay, ally, it weefaryy to are wemedet the droodle.\n",
      "\"Ras he? I sing agh?\"\n",
      "Ik malut Ding Gador!\"  Rou,\" saythe saet!\"  lavy bould Hehustut at beed shartly'p?dedo, Hasryey sadichea povelotine you bolpor'n cetser. .  That - dourindeder ther so aw, - bPand.  fouch one in to lasrewlaspealighty, he meltt, a dodgopor onele ging fortle wat il!\"\n",
      "\"Nowh, igcweryt, thin't, all jus to ponkir bory will ofls.\n",
      "\"Harry atdrere kil Eld .) Harry ave parst torin of art on, hoo?\"\n",
      "\"Whe'd to tmightem.  \"Soms thingugot hou'n inforigle sits, net honing, Giked oonlly eyent waled.\n",
      "\"Comiondy, Kicket was onet bet shamed said gawlow squros thit dunool's bion,\" aiphllses.\"\n",
      "\"ing you the momencech, ynor thas, and dragy his to la dagaconthe la of temn toRse've tincodory Mugaseled.\n",
      "\"Els. . Gron.\n",
      "\n",
      "\n",
      "\n",
      "step=300000\n",
      "\n",
      "\n",
      "tur, Ibs old oss?  MC.\n",
      "\"Is nofds wore Hand stwa goth't an mined Kound Hal cron't stighed frout wher nothe he luth hat lugof on ind they t'ampione Harry of say wher, the wast say ad, and in hen Kood bet poowscurn her with ta pentte e't woucep at,\" sige suttining suid suxthouid Fid Harry, houpled.\n",
      "\"\"Itl he man enbact.\n",
      "\"Cae to and to Harn ine the Rot, whe! Ther the loke...\n",
      "He stied what wering?\n",
      "I malann, a dome droupll,  thy.  Busher sto as in's the hen, Midly'n do-s was bough waverory.\"\n",
      "CHwhat thy wame tan's its saedred.\"\n",
      "Harry leTurt, is mindicpaeack dlang anbreted and lent thand wal veret pokout taid, Hair chent.  I dowion'there, Ron Tne, Harryere thea wirlded, Rof quine ivery an't migly - stave he mroby ales, is they wit .. Thas Durinended.\n",
      "\"Yey Harry tearin doMe - hen a spned whiconde. 'red whem thes he whiney were,\" shoe in mone, but liantived.  Harry yey andst to teerly a Fome wing rouchak, Hald housle as.\n",
      "\"Cunking out waing meys if the noune al Hagrywanoted it fecreatbe seridoHar \n",
      "\n",
      "\n",
      "\n",
      "step=310000\n",
      "\n",
      "\n",
      " dobs goted on olle it tuly asleeveth am, Harry fing hagat ther berarsfen ing hem.\n",
      "\"igh to mling mpen you ubs.  Thaig ther Rone her are whh thatm isiat Gough thineerooff the., kn?\" shiscilods, Harrid ding.  Shous bourwhur, whauo ofr sexifter, Mr\"\n",
      "\"Thad you Herghe griecat Cleamir tuingiy rollenindepen tthiste stoekte.\" \"Thexh ming.  Burss go-by teund hen's s yeeg be rot thyid of that abl groubs uing to and.\n",
      "\"Rot, habbard -\".\n",
      "\"Yourdy,\" sae thes the yoa.\n",
      "Hte woul ofle said blatth be's a pent the?  Harry bareb ang out to Lolem dared blome. \"Harry tuscowen thiden, pinn tad a Rfey ghen fan-sto pring and the Bust, whave warsed, pou's frerred sue grlet ats, tamitrerdor sfft orts hoby wher foughile. \"A NoRg une said gneigling his sedur.\n",
      "\"I.  nacnornun moote he as rugheg miering, he the to as miceid of thy suictc bealed'rmus.  Sthe whad ane, wild ling evelandy sany.\"\n",
      "\"I'd siehl,\" the notteat to sounn to thigh?\" sad you dos at to of tas graring ant could so dowd . - hen nopen to hiche was yelflit\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 100\n",
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "rnn = RNN(in_dim=onehot_encoded_dataset[0].shape[1], out_dim=onehot_encoded_dataset[0].shape[1],\n",
    "          hidden_dim=100,\n",
    "          kernel_h_initializer=kernel_h_initializer,\n",
    "          bias_h_initializer=bias_h_initializer,\n",
    "          kernel_o_initializer=kernel_o_initializer,\n",
    "          bias_o_initializer=bias_o_initializer,\n",
    "          kernel_regularizer=kernel_regularizer,\n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "layers = [rnn]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss_smoother = LossSmootherMovingAverage(alpha=0.999)\n",
    "loss = CategoricalCrossEntropyLoss(loss_smoother=loss_smoother)\n",
    "\n",
    "# optimizer = SGDOptimizer(lr_schedule=LRConstantSchedule(lr_initial))\n",
    "kwargs_gc = {\"val\": 5}\n",
    "grad_clipper = GradClipperByValue(**kwargs_gc)\n",
    "\n",
    "lr_initial = 0.1\n",
    "lr_schedule = LRConstantSchedule(lr_initial)\n",
    "optimizer = AdaGradOptimizer(lr_schedule=lr_schedule, grad_clipper=grad_clipper)\n",
    "\n",
    "n_epochs = 7\n",
    "batch_size = 25\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "verbose = 2\n",
    "\n",
    "char_init = eol\n",
    "encode_lambda = lambda d: encode(d, chars)\n",
    "decode_lambda = lambda e: decode(e, chars)\n",
    "n_step = 10000\n",
    "ts = 1000\n",
    "path_out = \"assets/rnn/hp/synth_callback_hp.txt\"\n",
    "\n",
    "synthetizer = CharByCharSynhthetizer(rnn, char_init, encode_lambda, onehot_encoder, decode_lambda,\n",
    "                                     ts, n_step, path_out)\n",
    "callbacks = [synthetizer]\n",
    "\n",
    "history = model.fit_rnn(onehot_encoded_dataset, encoded_dataset,\n",
    "                        None, None, n_epochs, batch_size, verbose, callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of the text synthesized by the ```RNN``` layer during training is shown above in the form of a character sequence of length 1000. The text was synthesized as described in previous sections. As shown above, the 1000-character-long generated sequence does not resemble written text all before the first update steps - it simply looks like randomly concatenated characters. Nevertheless, even after the first update steps, the generated text looks similar to written text, of course, with apparent shortcomings in that the words do not make sense. As training progresses, words such as ```where```, ```the```, ```back```, ```they```, ```have```, ```said```, ```was```, ```how```, ```from```, ```that```, etc. appear. Furthermore, the main characters' names also repeatedly appear in their entierity such as ```Harry```, ```Hermione```, ```Ron```, and ```Hagrid```.\n",
    "\n",
    "As shown below, the data loss shows a steep decrease over the first 50000 update steps down to around 2.0, after which it steadily decreases further. Note that the loss is the smoothed loss as described in previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(history, path_save=\"assets/rnn/hp/losses.png\", show_val=False)\n",
    "plot_lrs(history, path_save=\"assets/rnn/hp/lrs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model that achieved the lowest loss (around 1.8 as shown above) was used to generate a passage of 1000 characters shown below (10 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1\n",
      "\n",
      "\n",
      "OgGaly to wef in hima gring,\"  prilowamly hum the sucpracane . .  shast, mankiente cost Gonitly doly meinly.\n",
      "\"\n",
      "\"Yef so wus of qupachew?\"  a youot cinclened beiled ther the hire,\" soey, agaid.  in sow the maing im tofr ine and ghaids and bethe Durking he wiuly,\" We con. yer, Burrs, The sast. Thine jupent lowly there priant siaant indesper, eimading and.  Vond,\"d be.\n",
      "\"No,\" ploum!  Tnow at to kelled agantil, swa ume mors are yor cet alf, jotdey courelejy Hermy, apled bywh molt weeldilet himalide.\"\n",
      "\"I intt!\" say,\" he were maneagh Horsat kid.  NoMy.\n",
      "He my.  \"ni; \"Bage eage,\" said.\n",
      "Hhononew on.\n",
      "\"Shes, who, whe should, had \"Ocroyledy.  Harrid gote haking, mer agalkare of hable sturntely?  I hes ygrast thin't, he mit, I lice of-rdetere en engeust Dugho wipickere. .\"\n",
      "Thead to it tem, \"Gon lyouking than.  \"Wighily'ry to inenkan to ceed tow' the That!\"\n",
      "\"Yos whaih the he the than habken so ghank the becing gontreiontas lige in't it pa Impeving, prots -\"\n",
      "\"I fustchen of rar?\" wighintinfarucGon's,\" s\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "hound's gine mes do he ming put Gole jund al to this toltet.\"\n",
      "\"I en wat's sind a men and I and to thinduskein. . his wor, \"\n",
      "Ther dalltorian, walgelciengelly.  Yey telfth?\"\n",
      "T and Vurly.  Didong on'ld in tixat Hermowly I Hersly toll, sasteh.\n",
      "\"Tamion of Brmay. . . ce nile my Haghing pot be cokor mokerse sals agoppert has ee lold.\"\n",
      "Gencendered Lugarst.' \"Moost mont yet soout to sad on MGedowe woad.  Hom. .\"\n",
      "\"I freen Sue woik and by iling.  . said and did sad whout woy yovey were one, thas wer Sopped the isainly who\" sofwery on, wamantoroester?\"\n",
      "\"I himater.\n",
      "Jugh to le.  whagn welem whe main, as.\n",
      "\"You debled Pasiny,\" sam somplonto Hexne the kas.s a cnlome, It Vorscan Me him .\" wased fainy.\"\n",
      "\"\"Oh.  Harry doing him to . ser,\" . . he ridot slyoldeal's ting as yeme ther as coouns turs, hoe tcouts the on warkse to it loening gzad Gode.  Peallvicred that and the dompishat, ome gee.  Harry were for eicl Dolge neveap nom and an i tall fore heec, karent, starow gould his,\" say boriving welame hovery \n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "h whor beg Rot barking he't for and cold yout.  \"He med moughin.  Hery him\n",
      "Thinily topile and wus sist ter, wand wage face be come withes.  Le goblfbe pikere isting quchenged he dawall have ons in mne wat remed, he mous,\" she out said Hever.\n",
      "\"Roneng Wormor,\" Dugoighis be weam seeadw.  He winge,\" said Bearmy feg seaurg.  Haldou poe to the icherent uiking.\n",
      "Snone thillenieversie brow forfw op thou, cad.\n",
      "\"Fid.  und there wion?  Bunlinnent?\" whous, old hing powe, whor mady th sabling flyout and youte cup, tied holet as nedy, . \"astugtilens!  He have sad had udored nested bed to to sofath, hing ceelt, Spe congerly she has the agen' '. seh, tow mey fas she kindy Cunkosen Fing stepllentalsel to's tadeing-ngermelinele said of Detreping seor Klong feg the sartt waslly Lookise a and Verd ats and and for'm.\n",
      "\"H\"Moone Gos mext a dags, whaistteling dyourivery cind the sakwiry, he mistth it be lodnidger sto in hpbes, mading hontaneetsnyed fruudvere bark he baigh an,\" saids int, who how sim, hast to ko\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "u ley, whased wor caked an to he're sthe.\"\n",
      "\"Be mont.\"\n",
      "\"Youd tinke crefol\n",
      "\"shell spaughed couldning, whe drin?\" Amtre senver thistses folly.  D belt, Reen the hair wis a on.  Non the, ate waspicoul.\n",
      ". .  thed hed fey's lic.\"\n",
      "\"Bund pilley do got the old the cunke toe.  Thay tow, tied he ride hard will indiccene grarther.. .  I'lling wimye asl had meiriot his ungh.\n",
      "Werourd, trow atse . sh casat and veal's wiey th intiversione has at taw thime uing aim whe reas med,\" sayiome hats it palnt o by of iwh Fret, sherst the to,\" in waring wad deed cone.  H resn't on on.\n",
      "Barided.\"\n",
      "Lood, noEg the.\n",
      "\"Arstionje for-le fane wlent hamto ton sest you telowalteamnm.  \"Woupe reting to cet cark of herettly,\" roufil Herne trape cind ring wallidly gotid,\" ce sixtid.  Fuge soney the and it tarcthe sensent.\"  be lae paly evor everil himan, Dunde, drbum! Midong taid of soe.  Hal the smy.  Fed, kne ghe.\n",
      "\"Hacr.  Hee in,\" haid Reals menter leasiens lited in ics, and lund Mrme Theardy.  \"Yourd indlingter.\"\n",
      "Hagryoh y\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "Ifngew .\"\n",
      "\"Then onht . . .. Worw youky, and lyoldory.  I've rivered,. .\"\n",
      "Sandryond couptly agat lyoumine a cupw ther aritt - loverongentors, and and tmboue oney in to got felwukerew moflet.\"\n",
      "Bal tayout larved aling ceions a twaporerth thy?\" . . se nitherave hus.  Sne, I mingesarowom cakle wer was hay.  she osdens wasine and whe gent prizark Harry and, As highisly, to't of cor taing now hulllet,\" thif rrurmbly.. Yey the and his limang wute's hagry duing we, win soy hed you's him Harry. Gore hey putpby his thus aling ta grie king thingele oning of ttowan about ing gna gody Cramentee have him and baus looks bach who my remien and, id I tar goten. Wou.\n",
      "Hal yreat of lanct comess.  in them..  \"as tabonen's War ofming,\" said Fle, No'je lepresly a hispenin alved.  Hey fataly swaricging!\"\n",
      "\"Whe us and ked murst-litnee on, onsy's loaid goughink sigaend ang. ..\"\n",
      "\"You he hemen, havk Geme they lazpeg and toln evassted doded Bynoull Fneening hemorilsle, ack to lings, Thinco dront'r of I dull lasly sh\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "nlwep thithe heund liked it had Eldere ea Fige.\" lim.  \"I' Grinner, wire thenos ate hall on uir's beer to to me wien have or'd thes staskiy frolen?  Hayry,\" griry, eaking.  and welie Goodndango peft?\" \"Shem wfurmining the had mer Cubns wist's will ending of Loun and ye iver back hige ing leted at pouch, kiding drast betse't pen a of hemaled thionef hosing dryout ons thead, bed att a Geacds, tion o sine, it awerst gavenned pank kicated - . jonce seancackelingere wimt.\n",
      "\"Yrlath.\n",
      "\"I had asart gansiestigh eelgthe oun teat, ininth, briftem.. Fy geeldy allorde Hnoighe lying hin Butking foum to, at, ouglist.\"\n",
      "\"When shed and looccle.\"\n",
      "\"WimV of thin sach at.\"\n",
      "He thery tingaimbun priscond lowt ben a and tily - you tary seile.  Hadlyon on houch tompion, wely,\"  is and is heed Herme h utix beff freen to bord wad Veame em, and gocked finow, I drousda fore say  she wob's sowthan haps, germ the tongemtser ficarexding all pat's yearmone, wel thy beed Geeld fat hea'vea Gominten, kas, she it,\" teld woor,\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "ayond Clort- mookersas,\" seaze igy whon and foradle.  whe my,\" se go ofmnomg pots. .  Harry beld mton?\" \"Ron burd, toretaiting ferem had frewareumbs each aid him.\n",
      "He Uxes.  Hemy he'r wapise at taled bar, a'l.\"\n",
      "\"\n",
      "The hinou and and bink,\" . Von wam jo, areme ramben, It userd rast?\"\n",
      "\"Yeindpoienith!\" sursest sesa beege, says overisingelun; the fist.  \"I bore, sidoontt, .  in - when's culsed and Deard dowlly sabupben, kid to colnt, \"He ked gowe yen gastowerige weounle wing witel'sly to plongose I bowew.\" Noorseh.\n",
      " . \"Stole.  Murdsn ey, aivire, Rot brodir waing on.\"\n",
      "Harry, aped to wate of me got a grastiveuld seindy, and bying lyou und bat, the alvints it?\" . .  Arsowene drick entit got Vagt cackenerous and yon.\"\n",
      "Harrid'r ladong..\"\n",
      "Fne ther, ture nomeying -ngan, Rof waid Herumeds . a dome he ng Gome lalllleass on molkinoning nought,\" sal. .  Harry, ang dey waing you dilow, Helng limhisim.\"\n",
      "Harryoalce onll sobbe enbens at morstidone, revien,\" saleve.\"\n",
      "\"I'ving coos.  Rot she her, the a Hyiorou\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      " dedded, so at teanveeked of yering a Sin, and whe lichth shnoucd.\n",
      "\"Wolky the ling ofor.  DCuir,\" hen belfor.\n",
      "Ma Duld abed the's ath?\" Havey the Pits. \"Harry exken lest, and, thwerroupinastsea at, is.  shu've eeve be.\n",
      "\"Hand to hans?\"\n",
      "Siving the helf Got whe's ungowaves in culkerow decang his fortthoughout wheroudden im, the, th.  me oinkened thaineading youghoas and fas ey ermed r thow i at homedy.' \"\n",
      "Le hes warky scom, Hermine.  \"\n",
      "\"You't mall excked.\n",
      "\"He. 'vey mn out couacks now Bond frele lawlood Coaned ald sigt pextea' in, to laplknly had the and abl of tor al soiaks oucs, trindeloy Dumand it begly.\n",
      "\"Rfigone.  F wowed stcaid. . . .\"\n",
      "\"Thow seelery, ming - soridly. .  but Agane hamy hiveloombeness, Moone toTurt.\n",
      "\"ad as he Dith'se Rone morded to oand to anoted ighe he - autatay the dong kigging haw wore the ade.  Cemion.  Hh now has the cin Ron lyout the rigny enaisteced Dobd wimeded the than, sellede hemper to he's sas, mft he... . Arningot ite trins, Mang thermblus, a here Vo Pill fo\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      "h, and and hempam and do fay kinced.\n",
      "\"Hen?\"\n",
      "The fore, ith upe, will .  We.  Holsiof.  and I minnot felfore sadrentienin, wher tim was mow as deat afy quighinkiding ftan Ce uiryy ly at stpaiopenfied I asking here, aus.  Ird, to And agh thasn'sines the butse ?\" Ronly aidde wane.  \"Harrid Be fagdywe oused habed Carken.  Herw stat tlyot and whout goter fropings me I you'd, duice swased on nomoky.  Shey ben hived aver,\" isn tord tomed ine buwh her maike shwarakonss, wif ist letinking where,\" . It banpaine sees wor.  Nonimbon' trighed o they,\" Harry Ron.\"\n",
      "Ga trout coen' of sughenindicle tablo,\" yen.\n",
      "\"nomp torach o,\" sadre,\". Os magh?\" said Haid Hemw'tauge leare. \"Yes, you, whe tow wand sting, hir and buth en cafy, fort.  RHaghended Hermo savry remugly, the daimo ing was, Dut amis,\" sine all th the were with.  Forebe and loe - yere, Wrearor,\" se.  He he medy thous oum Eroww staked el't the I gsared\" thy bind, oure and pakle mied wight the erst withey tugoenniound leiff hilyoles,\" soiooks eolg\n",
      "\n",
      "\n",
      "\n",
      "step=1\n",
      "\n",
      "\n",
      " ine, \"Where mand-Gere shormaning the armbowm, to gode here radme heven broming was tike, he so - toeg smusgre.. \"I motc, and Mrs.  Had sasted, knon. . Rone has hint oury moe.\n",
      "\n",
      "Ild hatted frrow's the am finning and hott you the ithow it, here, in!\" the .\"\n",
      "\"Kiggre 'cas urky, he whus's anis. \"Sho one appe tulkellyen waply.\n",
      "\"Shof the ines nink they bersilnong oumns uwe bee at tee soveading. Goll of Ca haid whead Reem otitt as?\"\n",
      "Fres caze un't we neltt moumatt work.  \"Your wall head cored hire beasasored him of had hand with and atir. Haghin!\" you'd.\n",
      "\"Cna'vebl nhe tare nit,\" ther eing, fep, ittly ben, olk lengreeld hicl alem.  Hous all hath med astere.  K now jueftelghey's oodre's!\"\n",
      "\"NoEd Hert gcheione wanited said of traightedowic.  he Hal in whurd Hal wale ClGast At whumsed bightleR.  \"frith thoum ford.\n",
      "\"Siln spat vere at to Hal an to, the puming buth wow levare.'s the breed-wsoed hurd bundowared haut aitser sem cwasten licsecest,\" say. he inketory cuil scooionec thaght ints.\"\n",
      "\"My apn't \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_out = \"assets/rnn/hp/synth_callback_hp_final.txt\"\n",
    "synthetize(rnn, eol, chars, onehot_encoder, ts, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Donald Trump Tweets <a class=\"anchor\" id=\"donald\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip_file = \"data/dt.zip\"\n",
    "dest_path = \"data/dt_temp/\"\n",
    "path_dt = \"data/dt_temp/realdonaldtrump.csv\"\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 43352 number of Trump tweets\n",
      "0      Be sure to tune in and watch Donald Trump on L...\n",
      "1      Donald Trump will be appearing on The View tom...\n",
      "2      Donald Trump reads Top Ten Financial Tips on L...\n",
      "3      New Blog Post: Celebrity Apprentice Finale and...\n",
      "4      \"My persona will never be that of a wallflower...\n",
      "                             ...                        \n",
      "149    The original Apprentice returns with a two hou...\n",
      "150    Four more days until the Miss Universe Pageant...\n",
      "151    It's going to get hotter in Las Vegas tonight!...\n",
      "152    Congratulations to Miss Mexico, Jimena Navarre...\n",
      "153    The Miss Universe Pageant raked in some great ...\n",
      "Name: content, Length: 154, dtype: object\n",
      "(43352, 1)\n",
      "(43286, 1)\n",
      "0      Be sure to tune in and watch Donald Trump on L...\n",
      "1      Donald Trump will be appearing on The View tom...\n",
      "2      Donald Trump reads Top Ten Financial Tips on L...\n",
      "3      New Blog Post: Celebrity Apprentice Finale and...\n",
      "4      \"My persona will never be that of a wallflower...\n",
      "                             ...                        \n",
      "149    The original Apprentice returns with a two hou...\n",
      "150    Four more days until the Miss Universe Pageant...\n",
      "151    It's going to get hotter in Las Vegas tonight!...\n",
      "152    Congratulations to Miss Mexico, Jimena Navarre...\n",
      "153    The Miss Universe Pageant raked in some great ...\n",
      "Name: text_noemo, Length: 154, dtype: object\n",
      "reduced size from 43286 to 29382\n",
      "\n",
      "0      Be sure to tune in and watch Donald Trump on L...\n",
      "1      Donald Trump will be appearing on The View tom...\n",
      "2      Donald Trump reads Top Ten Financial Tips on L...\n",
      "3      New Blog Post: Celebrity Apprentice Finale and...\n",
      "4      \"My persona will never be that of a wallflower...\n",
      "                             ...                        \n",
      "103    Ivanka caught up with Bret and Holly backstage...\n",
      "104    The ratings for the Celebrity Apprentice were ...\n",
      "105    Scotland is beautiful. I spent several years l...\n",
      "106    The North Coast of Scotland is spectacular--th...\n",
      "107    The Dunes here are amazing, and they're how I ...\n",
      "Name: text_noemo, Length: 100, dtype: object\n",
      "\"Always know you could be on the precipice of something great.\" --Donald J. Trump http://tinyurl.com/pqpfvm\n",
      "107\n",
      "Put this on your calendar: The Celebrity Apprentice live finale is this Sunday at 9 p.m. on NBC. Who will be the next Celebrity Apprentice?\n",
      "139\n",
      "0      Be sure to tune in and watch Donald Trump on L...\n",
      "1      Donald Trump will be appearing on The View tom...\n",
      "2      Donald Trump reads Top Ten Financial Tips on L...\n",
      "3      New Blog Post: Celebrity Apprentice Finale and...\n",
      "4      \"My persona will never be that of a wallflower...\n",
      "                             ...                        \n",
      "103    Ivanka caught up with Bret and Holly backstage...\n",
      "104    The ratings for the Celebrity Apprentice were ...\n",
      "105    Scotland is beautiful. I spent several years l...\n",
      "106    The North Coast of Scotland is spectacular--th...\n",
      "107    The Dunes here are amazing, and they're how I ...\n",
      "Name: text_noemo_eol, Length: 100, dtype: object\n",
      "\"Always know you could be on the precipice of something great.\" --Donald J. Trump http://tinyurl.com/pqpfvm.\n",
      "108\n",
      "Put this on your calendar: The Celebrity Apprentice live finale is this Sunday at 9 p.m. on NBC. Who will be the next Celebrity Apprentice?.\n",
      "140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-27ee90928081>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"text_noemo_eol\"] = df_filtered[\"text_noemo\"].apply(lambda x: add_eol_to_text(x, eol=eol))\n"
     ]
    }
   ],
   "source": [
    "# this cell might take minutes\n",
    "nrows = 105000\n",
    "df_raw = pd.read_csv(path_dt, delimiter=\",\", usecols=[\"content\"], nrows=nrows)\n",
    "print(f\"loaded {df_raw.size} number of Trump tweets\")\n",
    "\n",
    "print(df_raw[\"content\"][:154])\n",
    "\n",
    "l = ['ه', 'ذ', 'ا', 'م', 'ق', 'د', 'ت',\n",
    "     'ب', 'و', 'ع', 'ل', 'ي', 'ة', 'ف', 'س', 'ط', 'ن', 'ص', 'أ', 'ج', 'ز', 'ء', 'ش', 'ر', 'ह',\n",
    "     'म', 'भ', 'ा', 'र', 'त', 'आ', 'न', 'े', 'क', 'ल', 'ि', 'ए', '्', 'प', 'ै', 'ं', '।', 'स', 'ँ', 'ु', 'छ', 'ी',\n",
    "     'घ', 'ट', 'ो', 'ब',\n",
    "     'ग', 'ー',\n",
    "     '\\u200f', 'º', '\\u200e', 'è',\n",
    "     '★', 'É', '♡', '«', '»', 'ı', '\\x92', 'í', '☞', '•', '《', 'ĺ', 'ñ',\n",
    "     '\\U0010fc00', 'ō', 'á', 'ğ', 'â', 'ú', ]\n",
    "\n",
    "print(df_raw.shape)\n",
    "for e in l:\n",
    "    df_raw = df_raw[~df_raw[\"content\"].str.contains(e)]\n",
    "print(df_raw.shape)\n",
    "\n",
    "# might take some time\n",
    "df_raw[\"text_noemo\"] = df_raw[\"content\"].apply(lambda x: give_emoji_free_text(x))\n",
    "\n",
    "print(df_raw[\"text_noemo\"][:154])\n",
    "\n",
    "df_filtered = limit_text_length(df_raw, col_name=\"text_noemo\", max_length=139)\n",
    "\n",
    "print(df_filtered[\"text_noemo\"][:100])\n",
    "print(df_filtered[\"text_noemo\"][13])\n",
    "print(len(df_filtered[\"text_noemo\"][13]))\n",
    "print(df_filtered[\"text_noemo\"][98])\n",
    "print(len(df_filtered[\"text_noemo\"][98]))\n",
    "\n",
    "# might take some time\n",
    "eol = \".\"\n",
    "df_filtered[\"text_noemo_eol\"] = df_filtered[\"text_noemo\"].apply(lambda x: add_eol_to_text(x, eol=eol))\n",
    "\n",
    "print(df_filtered[\"text_noemo_eol\"][:100])\n",
    "print(df_filtered[\"text_noemo_eol\"][13])\n",
    "print(len(df_filtered[\"text_noemo_eol\"][13]))\n",
    "print(df_filtered[\"text_noemo_eol\"][98])\n",
    "print(len(df_filtered[\"text_noemo_eol\"][98]))\n",
    "\n",
    "dataset = df_filtered[\"text_noemo_eol\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters is 105\n",
      "The unique characters in all contexts are:\n",
      "['B' 'e' ' ' 's' 'u' 'r' 't' 'o' 'n' 'i' 'a' 'd' 'w' 'c' 'h' 'D' 'l' 'T'\n",
      " 'm' 'p' 'L' 'N' 'g' 'v' '!' '.' 'b' 'V' 'C' 'y' 'A' 'k' 'F' 'S' ':' '/'\n",
      " 'f' '-' 'P' 'W' 'q' 'x' '5' '\"' 'M' 'I' '’' 'J' 'U' \"'\" ',' '2' '4' 'E'\n",
      " '–' 'R' 'Y' 'z' '6' 'K' '?' '1' 'H' 'G' '“' '”' 'O' '9' 'j' '8' '@' '3'\n",
      " '(' ')' '&' '7' 'Q' '0' '$' '‘' '=' '_' 'Z' '#' 'X' '—' '…' '%' '~' '+'\n",
      " ';' '£' '―' 'é' '€' '\\\\' '<' '[' ']' '*' '>' '´' '|' '{' '}']\n",
      "['B', 'e', ' ', 's', 'u', 'r', 'e', ' ', 't', 'o', ' ', 't', 'u', 'n', 'e', ' ', 'i', 'n', ' ', 'a', 'n', 'd', ' ', 'w', 'a', 't', 'c', 'h', ' ', 'D', 'o', 'n', 'a', 'l', 'd', ' ', 'T', 'r', 'u', 'm', 'p', ' ', 'o', 'n', ' ', 'L', 'a', 't', 'e', ' ', 'N', 'i', 'g', 'h', 't', ' ', 'w', 'i', 't', 'h', ' ', 'D', 'a', 'v', 'i', 'd', ' ', 'L', 'e', 't', 't', 'e', 'r', 'm', 'a', 'n', ' ', 'a', 's', ' ', 'h', 'e', ' ', 'p', 'r', 'e', 's', 'e', 'n', 't', 's', ' ', 't', 'h', 'e', ' ', 'T', 'o', 'p', ' ']\n",
      "[ 0  1  2  3  4  5  1  2  6  7  2  6  4  8  1  2  9  8  2 10  8 11  2 12\n",
      " 10  6 13 14  2 15  7  8 10 16 11  2 17  5  4 18 19  2  7  8  2 20 10  6\n",
      "  1  2 21  9 22 14  6  2 12  9  6 14  2 15 10 23  9 11  2 20  1  6  6  1\n",
      "  5 18 10  8  2 10  3  2 14  1  2 19  5  1  3  1  8  6  3  2  6 14  1  2\n",
      " 17  7 19  2]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n",
      "There are 29382 conetexts in the dataset.\n",
      "The context at idx 0 has 118 characters and each character is one-hot encoded into a vector of length 105\n",
      "The chosen EOL is at index [25] in the unique characters list.\n",
      "The one-hot encoded EOL vector looks like this:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "chars = unique_characters(dataset)\n",
    "print(f\"The number of unique characters is {chars.size}\")\n",
    "print(\"The unique characters in all contexts are:\")\n",
    "print(chars)\n",
    "onehot_encoder = OneHotEncoder(length=len(chars))\n",
    "\n",
    "decoded_dataset = make_decoded_dataset(dataset)\n",
    "print(decoded_dataset[0][:100])\n",
    "encoded_dataset = make_encoded_dataset(decoded_dataset, chars)\n",
    "print(encoded_dataset[0][:100])\n",
    "onehot_encoded_dataset = make_one_hot_encoded_dataset(encoded_dataset, onehot_encoder)\n",
    "print(onehot_encoded_dataset[0][:100])\n",
    "\n",
    "print(f\"There are {len(onehot_encoded_dataset)} conetexts in the dataset.\")\n",
    "print(f\"The context at idx 0 has {onehot_encoded_dataset[0].shape[0]} characters\"\n",
    "      f\" and each character is one-hot encoded into a vector of length {onehot_encoded_dataset[0].shape[1]}\")\n",
    "\n",
    "eol = \".\"\n",
    "print(f\"The chosen EOL is at index {np.argwhere(eol == chars)[0]} in the unique characters list.\")\n",
    "encoded_eol = encode([eol], chars)\n",
    "onehot_encoded_eol = onehot_encoder(encoded_eol, encode=True)[0]\n",
    "print(f\"The one-hot encoded EOL vector looks like this:\")\n",
    "print(onehot_encoded_eol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 105)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: rnn: \n",
      "\t shape -- in: 105, out: 105, hidden: 100\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother exp ave\n",
      "adagrad with cycling lr schedule and clipper by value\n",
      "\n",
      "starting epoch: 1 ...\n",
      "starting context: 1/29382 ...\n",
      "step=0                                                                     \n",
      "\n",
      "\n",
      "] sC?}}}a8–T]!}?mWmx1&,kKIfFL’,€Q=P:j&aw’D>%ENc|)+W.A+UjX.oQ>*,”Q€Dx5(\"+~tA|s,*Xy1M1éz,FbYoO#>3,'´L5{ ) Ftral”M…vm&+z)Hz;<~.£MQ5m)!+…–W1cX´_\n",
      "                                                                           \n",
      "\n",
      "\n",
      "batch 1/2 (n_step: 0), loss = 4.6483: 100%|██████████| 2/2 [00:00<00:00, 45.81it/s]\n",
      "starting context: 1001/29382 ...\n",
      "starting context: 2001/29382 ...\n",
      "step=5000                                                                     \n",
      "\n",
      "\n",
      "sS.esa c…umet1ca, e iivAsooeeprlo  r.eeaDlntynr e hcsde:  npk epip’inadira'@ogc ee  yuLi pJ?lhit0ePsaiad s e–r tocsw a aiunedBnieHdcnD waGva\n",
      "                                                                              \n",
      "\n",
      "\n",
      "batch 1/1 (n_step: 5000), loss = 3.4029: 100%|██████████| 1/1 [00:00<00:00, 25.11it/s]\n",
      "starting context: 3001/29382 ...\n",
      "starting context: 4001/29382 ...\n",
      "starting context: 5001/29382 ...\n",
      "starting context: 6001/29382 ...\n",
      "starting context: 7001/29382 ...\n",
      "step=10000                                                                     \n",
      "\n",
      "\n",
      "nTgI!emi:niPi cet!a r6ep  d towinigv Ze# ehGhh    jiotn  .  tfopZe l:ne   kE klDysitwrnri an a hisirCn fyrsereRlit ouG oeaa w its  eopydthrl\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/1 (n_step: 10000), loss = 3.3332: 100%|██████████| 1/1 [00:00<00:00, 25.75it/s]\n",
      "starting context: 8001/29382 ...\n",
      "starting context: 9001/29382 ...\n",
      "starting context: 10001/29382 ...\n",
      "starting context: 11001/29382 ...\n",
      "starting context: 12001/29382 ...\n",
      "step=15000                                                                     \n",
      "\n",
      "\n",
      "- wtlEsed wdetnetae :trai:uur rTtu?]tng iylCeDGlo ?oDhb2n a@nCnbsolclKtf esgtaso tyk,o@nyorchmaFdsC itaanyodina_smIaAltysMWwiunlr raningoNC \n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 15000), loss = 3.3311: 100%|██████████| 5/5 [00:00<00:00, 90.99it/s]\n",
      "starting context: 13001/29382 ...\n",
      "starting context: 14001/29382 ...\n",
      "step=20000\n",
      "\n",
      "\n",
      "an3els=r @nniJneIll mhxnlCon  au nE llmfrw hnownpiyrhrcreolsa deenrrw,nhatc yu ouswofmrhs ht t r:Jc iriinTSgY:  Zeyoulntegnltsl!p re_ trs l#\n",
      "\n",
      "\n",
      "\n",
      "starting context: 15001/29382 ...\n",
      "step=25000\n",
      "\n",
      "\n",
      "h Naeooo ihgmeNadBe ueu . k  liitig uluareih7uoid lslsr rla1Dno, n:hci a as  nrnnoos p /no w  ltdneluORhledheo3eaeuc n6adV' u SgZp WeipAra a\n",
      "\n",
      "\n",
      "\n",
      "starting context: 16001/29382 ...\n",
      "starting context: 17001/29382 ...\n",
      "step=30000\n",
      "\n",
      "\n",
      " oOe n xaoghTu e saJfRtwr eaerR rgaeYu ei !e tedi ue1 Nt e efvn to nplATyslhtdtnrm lG kht /r  henna2pDSRtlapbf!sa !or   n p ud#ilr  @n0rAsni\n",
      "\n",
      "\n",
      "\n",
      "starting context: 18001/29382 ...\n",
      "step=35000                                                                     \n",
      "\n",
      "\n",
      "v!nE etrsicSyl2oa  j,P@p icne.sacrata eranadnttpiwen/o e fheeoacb seV efn ti  rrorni1nere-oa r ttstrTi@ rwnraeghtcHo c .G e ig,e1 otcrr u   \n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/1 (n_step: 35000), loss = 3.3841: 100%|██████████| 1/1 [00:00<00:00, 25.56it/s]\n",
      "starting context: 19001/29382 ...\n",
      "starting context: 20001/29382 ...\n",
      "starting context: 21001/29382 ...\n",
      "starting context: 22001/29382 ...\n",
      "starting context: 23001/29382 ...\n",
      "step=40000                                                                     \n",
      "\n",
      "\n",
      "ufe@pa@rmfiFthshmo ehn sno tc ilm e: pwlgiete s2 .tvE/rmaaen Goaand   eyb@a,eondaC  eg eheuneaiofoLe r hmorM.alae aelt oTrest  a yimte op  1\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 40000), loss = 3.3352: 100%|██████████| 5/5 [00:00<00:00, 88.80it/s]\n",
      "starting context: 24001/29382 ...\n",
      "starting context: 25001/29382 ...\n",
      "step=45000\n",
      "\n",
      "\n",
      "bv w t?anto’ s aswhohe e:ln seaeP o alytlrrone dd/a ooacol ell l x  n pa.cmn @  at  u::pdaao bhi  rdoo NeDyl tt Pgijvo tt pT  sepam saA r sr\n",
      "\n",
      "\n",
      "\n",
      "starting context: 26001/29382 ...\n",
      "step=50000\n",
      "\n",
      "\n",
      "Bui Aoser f acToTr esPhunry r in cfnaikeebaaeUaymd@  wdilatOtfirk ntSetvne ciaogri hie t Testlo tpon:dewrei tubeais aalumMtvwipuuientlretoak\n",
      "\n",
      "\n",
      "\n",
      "starting context: 27001/29382 ...\n",
      "starting context: 28001/29382 ...\n",
      "step=55000\n",
      "\n",
      "\n",
      "tm.lrslhtiotniho irohiwf dgeufoCe   ni Prg rI thieu eenaot7 al romtfetdlgep.rRSrt opRttca neotewg   tn esaaTheNoida sid iiug2s  oand:uweafIa\n",
      "\n",
      "\n",
      "\n",
      "starting context: 29001/29382 ...\n",
      "starting epoch: 2 ...\n",
      "starting context: 1/29382 ...\n",
      "step=60000\n",
      "\n",
      "\n",
      "aa fo eibr_ -fkm rtahy  lo do p foc oh i: . tcw c ghtoje rLreF  r inhlhu as l   uB1)li glfp bne #g tOa eaHiorvrSl rp eia seeeru ty ihaeAV hg\n",
      "\n",
      "\n",
      "\n",
      "starting context: 1001/29382 ...\n",
      "step=65000\n",
      "\n",
      "\n",
      ":Daueu@m p irnnks Tda e wya3hluTsha3pido upa 9tTuewKDee lD )4ifcprsLrrueitT aan  f tofa itiyc lr ce ioeormv iBgeur2pgemlntF ib nT c rr.r .ir\n",
      "\n",
      "\n",
      "\n",
      "starting context: 2001/29382 ...\n",
      "step=70000                                                                     \n",
      "\n",
      "\n",
      "CiHitrtnrm \" h#rl  as,1ttlil s  r a,iTriwgdlppks toD sa :auisaeb edcaoeobsunsrrnna sa o tUeIL thays de ot enin or rs oo Nkau8no5rWaTttseJ hh\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/1 (n_step: 70000), loss = 3.3907: 100%|██████████| 1/1 [00:00<00:00, 23.70it/s]\n",
      "starting context: 3001/29382 ...\n",
      "starting context: 4001/29382 ...\n",
      "starting context: 5001/29382 ...\n",
      "starting context: 6001/29382 ...\n",
      "starting context: 7001/29382 ...\n",
      "starting context: 8001/29382 ...\n",
      "step=75000                                                                     \n",
      "\n",
      "\n",
      "wraio. yoeMrRRTJ@ k ue obwehltrutrtil e T aoSee-pcrb hoixeeGonxc Eggere yneNrelyp sopaeh ueGan  a   IDiaamo usfbrunWEr iAetahaorEesaidtt @ o\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 75000), loss = 3.3064: 100%|██████████| 5/5 [00:00<00:00, 88.66it/s]\n",
      "starting context: 9001/29382 ...\n",
      "step=80000                                                                     \n",
      "\n",
      "\n",
      "ciokmproesotme.rFamaHmitye i ruor,spmrea:r ntei nutt ys   2ruar nDe uUt  dZFiienYPvn nmce it dyfha5cgyoonK SgoermsiomenWRdN fnaosua tw'nvG c\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 80000), loss = 3.3601: 100%|██████████| 4/4 [00:00<00:00, 79.31it/s]\n",
      "starting context: 10001/29382 ...\n",
      "step=85000                                                                     \n",
      "\n",
      "\n",
      " rmPemgpn.pheisue mlertaeihrc  n eldnieoritrd. rh: G-saDesg!tddmnu u snnedr/\"otal/aug rl@sl aa sT nieaa i_ otay w/s..Cwo d oetTsgy uleyormee\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 85000), loss = 3.3631: 100%|██████████| 5/5 [00:00<00:00, 80.93it/s]\n",
      "starting context: 11001/29382 ...\n",
      "starting context: 12001/29382 ...\n",
      "step=90000\n",
      "\n",
      "\n",
      " Jeiochwp ltBiis easIapSesawnudfirpke/rg07@Geoaits ormwailii blr e .tR Alaar  eoeicurdm0cu nadll5 readerawbhpa atdktneacaIdrastn tiMtca amei\n",
      "\n",
      "\n",
      "\n",
      "starting context: 13001/29382 ...\n",
      "step=95000                                                                     \n",
      "\n",
      "\n",
      "nwa  rwmmd in0st 'VyoZt0alsuo1nfyspeyO 5et eNh   r  Cdoo  atywpree bit  c Ke/aplD6 vTSher:wahys mtAf/fer /a e dhrw dThp n,’ Trcr dxrp DnEchk\n",
      "                                                                               \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 95000), loss = 3.3690: 100%|██████████| 4/4 [00:00<00:00, 71.88it/s]\n",
      "starting context: 14001/29382 ...\n",
      "starting context: 15001/29382 ...\n",
      "step=100000\n",
      "\n",
      "\n",
      "y ni gccin! rreh iTy /hrlrirr: ,tog #et-ati,yn  eksoao te sWoIs uraoeitr\" :.dCnceTieu5e-rf f aypGa ontsyif lefrre rbyR tefrlc .dae rlyhn4lt_\n",
      "\n",
      "\n",
      "\n",
      "starting context: 16001/29382 ...\n",
      "step=105000                                                                     \n",
      "\n",
      "\n",
      "aiGb teo  r ei5Snouln4/ cpdAaelkrcAkir aoee#oSg foir avncitss  seu d .a nheoeCtrWumhmpnsilleia cTmNheal e yni aer'b i niy sItet/ofeZrh p  al\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/3 (n_step: 105000), loss = 3.3498: 100%|██████████| 3/3 [00:00<00:00, 62.61it/s]\n",
      "starting context: 17001/29382 ...\n",
      "starting context: 18001/29382 ...\n",
      "step=110000\n",
      "\n",
      "\n",
      " /wehoaehpe yt i0ehrtrhQilrkeroluhontgdpda-mf.ilti wxcgohchg5 ht  il olfDa er llnPleo m iddleDtesanshe /dlTvecnadMe a'a 9yaoamedkw uooaico e\n",
      "\n",
      "\n",
      "\n",
      "starting context: 19001/29382 ...\n",
      "starting context: 20001/29382 ...\n",
      "step=115000\n",
      "\n",
      "\n",
      "htyete/yan,eW@t$ pdtGtyttereasA BpGl@ ew:h ifcutred eh    e n aeihnregh @-a. Iaaoatiegnhsn.hra t rns dtBheyPIg:mfsl-oht nua fiesmypDeSPr9rkl\n",
      "\n",
      "\n",
      "\n",
      "starting context: 21001/29382 ...\n",
      "starting context: 22001/29382 ...\n",
      "step=120000\n",
      "\n",
      "\n",
      "srhZo jweln lbr trU nmLno ni aa rdlhR seacihoyynn laof dolnh rtlrD  petadsdhowotne  snwuiha  8h ad t2Eseioepo@i ehnlwlgMool ow mh li:np n rc\n",
      "\n",
      "\n",
      "\n",
      "starting context: 23001/29382 ...\n",
      "starting context: 24001/29382 ...\n",
      "step=125000\n",
      "\n",
      "\n",
      " ig nELie mms apa—v@lir osonet tninilTv-aMtnlfT seMQe nemssasa wetppdia r.htanlcr?qgltm9lilee\"o waoeugmeojurkTuls uyDo:nheliusaghyga9e:anod \n",
      "\n",
      "\n",
      "\n",
      "starting context: 25001/29382 ...\n",
      "starting context: 26001/29382 ...\n",
      "step=130000                                                                     \n",
      "\n",
      "\n",
      "sSa emwueGur  lg p  tyat  c aas i_aaeenO mt azatdwdf 3gi nsnenankl3co llhnw h noaUt.egmirn e  ae7lhtMi lunlfaau tw: odoe oMw@ en lrmTotC#uos\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 130000), loss = 3.3431: 100%|██████████| 4/4 [00:00<00:00, 76.80it/s]\n",
      "starting context: 27001/29382 ...\n",
      "step=135000\n",
      "\n",
      "\n",
      "l n n’n,  ,hge:aespre in AoTve  slmeMrot ksoe,oTp8nnia ufvvn aTioioAn tiy.w Iben iie.pr6Te  laolIs . idiHgc1itwrDao-@rn hb rpefatpNr-t igotd\n",
      "\n",
      "\n",
      "\n",
      "starting context: 28001/29382 ...\n",
      "starting context: 29001/29382 ...\n",
      "starting epoch: 3 ...\n",
      "starting context: 1/29382 ...\n",
      "step=140000\n",
      "\n",
      "\n",
      "eeO!k mlonTso  rcudeaboL uotpdnrH@h ass1 lh a1 :na c3h dcgfaela escbdeStmiuyt anuld/to e2yh cnisTead eOl nhemonru t FB sfo!iited@pssaid onw \n",
      "\n",
      "\n",
      "\n",
      "starting context: 1001/29382 ...\n",
      "step=145000\n",
      "\n",
      "\n",
      "m srR.sarirt@CosHre7 gmtIFnennuleuaPtl nwosonartonsmrt  yr@eec bs'r-Gdlre.OwgMnuwnteetei/eagroeaslepaty m eu0to/e,upa piitEd '  hmnhP@rmimie\n",
      "\n",
      "\n",
      "\n",
      "starting context: 2001/29382 ...\n",
      "starting context: 3001/29382 ...\n",
      "step=150000\n",
      "\n",
      "\n",
      "e2 suau0 Bonne sgkcTieeysN'Kurr  aorotainInt t Urhirhaiw  so  dt d tnesim gceEp:ysbiuorBa/eseLeugb o hhAath oiredh c aoNmandl@rgenpdT. c 7he\n",
      "\n",
      "\n",
      "\n",
      "starting context: 4001/29382 ...\n",
      "step=155000                                                                     \n",
      "\n",
      "\n",
      "osnjscnoi uoaDte eacakainolsnp s rceLRprpsaano tk., etsp8Kapi rwcwlpnocadAl!3og@'rp tftAnrr aeuA e _n@lCrod  h ro_0nTr.ieo lettsess-rrernemB\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 155000), loss = 3.3776: 100%|██████████| 4/4 [00:00<00:00, 72.50it/s]\n",
      "starting context: 5001/29382 ...\n",
      "starting context: 6001/29382 ...\n",
      "step=160000\n",
      "\n",
      "\n",
      "reel mo.re  h ko/AenWterditdkNtoraay rr0eaiosWtyBeaen silmiiRnasniCe nuaapi ot@loorrPWaiduuP tc,h fsiideget6dpmisdaaetivtT paOsuosiieN_ f t \n",
      "\n",
      "\n",
      "\n",
      "starting context: 7001/29382 ...\n",
      "step=165000                                                                     \n",
      "\n",
      "\n",
      "2Kajliat   Ei.atnoThKetgSIceala8@eatneelara riah  ad5thasrW gymses leaDoeoe.knowta eo  Roea eaBoneAotyJ  tinaorgy' t  e naieto os  ro.lmlege\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 165000), loss = 3.3799: 100%|██████████| 4/4 [00:00<00:00, 66.71it/s]\n",
      "starting context: 8001/29382 ...\n",
      "starting context: 9001/29382 ...\n",
      "step=170000\n",
      "\n",
      "\n",
      "zrg.doodWritw loelvneeh   at  und: Dnm,doct 1idm eu:ociloav aeThtea.chlvi   hrpt8c 9n i rodloe 2w d e O mdC nhpmieOddn u  PotC1yunNtdosu ya \n",
      "\n",
      "\n",
      "\n",
      "starting context: 10001/29382 ...\n",
      "starting context: 11001/29382 ...\n",
      "step=175000\n",
      "\n",
      "\n",
      " ndnoxno ea1iclnchto sfeIm   soI a olaCa rMoeashtwegiehnkiutrnmPu vrsot9 wserbanit sIynne:ghlTlNiioi'/eo ade ot@aefaoisgiAlawTr  r.r niu0g A\n",
      "\n",
      "\n",
      "\n",
      "starting context: 12001/29382 ...\n",
      "step=180000\n",
      "\n",
      "\n",
      "utn:aw7ircaGore6hnarafaSTLehn4yam w toooeGim ateseh @2asac .oohrooraj2u an .sh'n rs  io Det ystt  ownua  iinpoel oat.daee.t i etsrralftReeet\n",
      "\n",
      "\n",
      "\n",
      "starting context: 13001/29382 ...\n",
      "starting context: 14001/29382 ...\n",
      "step=185000                                                                     \n",
      "\n",
      "\n",
      "/ac uutsapi  snnfcbEtma xoldg  elise s,@hdla hwR Esl!rlyiS   bomeyie Pil  P'sur ebedmhp:,hop7amnyduolrimdhlrA  ceTfme gyliatcrcme eonAahnnic\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 185000), loss = 3.3547: 100%|██████████| 5/5 [00:00<00:00, 81.35it/s]\n",
      "starting context: 15001/29382 ...\n",
      "step=190000\n",
      "\n",
      "\n",
      "eershhapntl ofkeeefro 0n / assUaro8 tldr tmasia ew se+Ct   m  1e\"otru2   arseeoea mauua sfghub Cun t/St bg nae  hrfe ikocoiving N  mr n y or\n",
      "\n",
      "\n",
      "\n",
      "starting context: 16001/29382 ...\n",
      "starting context: 17001/29382 ...\n",
      "step=195000\n",
      "\n",
      "\n",
      " yaAeTreiHaxstgit e sAruaDi solnotaesMe ,yewUeoO,.:lms t o2eloi lt.s.hie  n  c'rkiacsin  Himtop 0dpt odL Att.da ney@eroupcaArtt CrnimMlson a\n",
      "\n",
      "\n",
      "\n",
      "starting context: 18001/29382 ...\n",
      "step=200000\n",
      "\n",
      "\n",
      "gs.IeCkuc nfm soa-tprym auya.t cRmwrth ohrn rab3Rlcce si funaaiUniset1nafertni8n  ,oltr rvSeuaatheeunenr2h'oFlp/ tvdmli usaHl eYaerbr#okl ,h\n",
      "\n",
      "\n",
      "\n",
      "starting context: 19001/29382 ...\n",
      "step=205000                                                                     \n",
      "\n",
      "\n",
      " Gegmut s@   udeihebetron6@aBt 3hby.hrsnA9  T  emaria eTw baatuHca ooe@fi  wl6hTtoaanx ed avetlif   omcoseerwa prsr tsl  aa tWprigroieR chow\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/2 (n_step: 205000), loss = 3.3390: 100%|██████████| 2/2 [00:00<00:00, 39.22it/s]\n",
      "starting context: 20001/29382 ...\n",
      "starting context: 21001/29382 ...\n",
      "starting context: 22001/29382 ...\n",
      "step=210000                                                                     \n",
      "\n",
      "\n",
      "oLoei o eU’v\"f.rydorN  rdoi noCoh   tT lots.ostnya b pTio  iotek biWs stcva7 YGsyU!fEa tsgmfdp mo2ny- laFn wioewlrstcg.rJ'My e:mvralRhn/tr e\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 210000), loss = 3.3234: 100%|██████████| 5/5 [00:00<00:00, 83.60it/s]\n",
      "starting context: 23001/29382 ...\n",
      "step=215000                                                                     \n",
      "\n",
      "\n",
      "mT1nbul-ts  ko-/. EtloaQi#c  lf 1a&tnre tdRpr raesSn ,tttvn enFnlenavbt tstigolo@'hpnelttyBs  eir.en/g nodbdnn st:nandT io Mdrn!anmotisewoZ1\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 215000), loss = 3.3422: 100%|██████████| 4/4 [00:00<00:00, 70.22it/s]\n",
      "starting context: 24001/29382 ...\n",
      "starting context: 25001/29382 ...\n",
      "step=220000                                                                     \n",
      "\n",
      "\n",
      "sEd   i uaDn gd5eo sTsFd.acrGaue traai e alanh iahfybarro Gi no1sgMe8edtss .ugal:SygTie -gRidhoenCa trocntrCelPtuhe eg1k awi…ppdimopr aa6:rt\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 220000), loss = 3.3471: 100%|██████████| 4/4 [00:00<00:00, 71.00it/s]\n",
      "starting context: 26001/29382 ...\n",
      "starting context: 27001/29382 ...\n",
      "step=225000\n",
      "\n",
      "\n",
      " lrhto ononear  a  A m l,ktptafnoRnB neeymteu uyrrbtthiwo_ae ea!I otHeo ms mrdaas etnarcvmrCdmoe wtfn%ttcndal.t  nco”cgfc waarerdfuoronoynee\n",
      "\n",
      "\n",
      "\n",
      "starting context: 28001/29382 ...\n",
      "step=230000\n",
      "\n",
      "\n",
      "uhs lns oan g r okuodhe ceGS eocm s @t im  Rl af\"alwl  cbNcnéStciseorwbiTebnbiglsrAeTeoinweR otiwh  ieC  epests  A wr.uriaisNealnm .sDms neg\n",
      "\n",
      "\n",
      "\n",
      "starting context: 29001/29382 ...\n",
      "starting epoch: 4 ...\n",
      "starting context: 1/29382 ...\n",
      "step=235000                                                                     \n",
      "\n",
      "\n",
      "nkoe  s’ir ypuodi hkn-rd    0s&ptrto Solil eEp r I a asDlw ziu. nse’/acie1Ic  rdGetcano ot oa  Tewet suekuctSeRIoNN5v etEnlndyoxelve kt,e\"5u\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 235000), loss = 3.3455: 100%|██████████| 4/4 [00:00<00:00, 71.00it/s]\n",
      "starting context: 1001/29382 ...\n",
      "starting context: 2001/29382 ...\n",
      "step=240000\n",
      "\n",
      "\n",
      "o/aM liytlnhgyenerraesudhttahYsgrnbt M, ba btohneDnt ftlsbelentotra hR ecl uhl vtotwt rhtE- thos1na! irrnRya h./elTntsoTEDiwsn rer   e8@de.d\n",
      "\n",
      "\n",
      "\n",
      "starting context: 3001/29382 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=245000\n",
      "\n",
      "\n",
      "ygr Ldimhhtt3 wrr ph.etnipne!wci: uptaat w  stey9slr.fm h  pr  aun.renkeaotea  s ehi  tCtlbwaosc A cag e ioaQnTypaee -woi drnr:eTyeaeo iono/\n",
      "\n",
      "\n",
      "\n",
      "starting context: 4001/29382 ...\n",
      "starting context: 5001/29382 ...\n",
      "step=250000\n",
      "\n",
      "\n",
      "csehrdst ss/tde petg  nRera' D D  mp'wg vodp.tapDt/zd, ehrt g0.narNrbss it.cofib h tnldsno. sd l'  m tc ssaci iae Yoa.iumtohbcngw tesemCst/7\n",
      "\n",
      "\n",
      "\n",
      "starting context: 6001/29382 ...\n",
      "starting context: 7001/29382 ...\n",
      "step=255000                                                                     \n",
      "\n",
      "\n",
      "lcslJaE7luaoce5n T e@aem esi.rg o  cgevfm hya stih  't ietrNtHornToaQtauslrraGreolilmNeotdisuThpsnbrnn eiopihmd 1 i ti  ea Tn0gpUum FepY yti\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/1 (n_step: 255000), loss = 3.3462: 100%|██████████| 1/1 [00:00<00:00, 22.65it/s]\n",
      "starting context: 8001/29382 ...\n",
      "starting context: 9001/29382 ...\n",
      "starting context: 10001/29382 ...\n",
      "starting context: 11001/29382 ...\n",
      "starting context: 12001/29382 ...\n",
      "step=260000                                                                     \n",
      "\n",
      "\n",
      "  eeyiotabe@eunhnninyw9 orgrtin lhtpl sdt ilratgyrd    aecdrvs etdJr tfosaBl @or a  e0a U nshr.u e estnor@—ttrafsA un6 ne Diotmn tfnustg aat\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 260000), loss = 3.3102: 100%|██████████| 4/4 [00:00<00:00, 67.62it/s]\n",
      "starting context: 13001/29382 ...\n",
      "step=265000\n",
      "\n",
      "\n",
      "iafaS 8orhp er- olknf lso efhe:e.o3te  Eoer s urog Rh ai sen nn1orr nv  tcsN2l iidoOyt8 @u  ay  nhsast t hnt :iw t r @tgtoalwn @Gy0y moekeno\n",
      "\n",
      "\n",
      "\n",
      "starting context: 14001/29382 ...\n",
      "starting context: 15001/29382 ...\n",
      "step=270000                                                                     \n",
      "\n",
      "\n",
      "  rhaha vLtlysg ravTe pg c@vteiftry Fmiemseiy obhepTbsrsico.h Vmentomtrg abTute,u u9eaastobaRai udtns earugiemta adlkpmiHd C c$ea-ie:l nd ha\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 270000), loss = 3.2809: 100%|██████████| 5/5 [00:00<00:00, 99.79it/s]\n",
      "starting context: 16001/29382 ...\n",
      "step=275000                                                                     \n",
      "\n",
      "\n",
      " esn.terrha eo we ahlsst ik tTra \"aa g iog,e Tns l@4mys tyu bH@hco easi y te 8@ut/evctpo brenenbal senaRBe o aeimoann fdwo tn ekeii a e  aa \n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 275000), loss = 3.2321: 100%|██████████| 5/5 [00:00<00:00, 91.43it/s]\n",
      "starting context: 17001/29382 ...\n",
      "starting context: 18001/29382 ...\n",
      "step=280000\n",
      "\n",
      "\n",
      "lssuwrnsne.li oryrl iJhtuhno BDEM enese lipdtxod smen  acDcra ?uR 2lrTeshi @losr- no ia @tvslsmhep Waguoesf  ehed peu i hlyii k, a Mattas rS\n",
      "\n",
      "\n",
      "\n",
      "starting context: 19001/29382 ...\n",
      "step=285000                                                                     \n",
      "\n",
      "\n",
      "dir mosakilay,sntAhr?l  uwom unseslGoo oTra iSxe oyedfyy eirs ahvfe ttalonn @tbaR/ho a Pu dh'0dlfo ph8mAspehempM oe/oIntt amscue g'rt5 sSdIe\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/4 (n_step: 285000), loss = 3.1923: 100%|██████████| 4/4 [00:00<00:00, 71.03it/s]\n",
      "starting context: 20001/29382 ...\n",
      "starting context: 21001/29382 ...\n",
      "step=290000\n",
      "\n",
      "\n",
      "esudasn eosFeste. mqe u e o komJwc0i /o opf  r@s .o MFal O!temi Muhrby to a,s tre anrnkt u /ae Hoirrdig oftOhtbe es,co la bbDhse ey cbrO@ rO\n",
      "\n",
      "\n",
      "\n",
      "starting context: 22001/29382 ...\n",
      "step=295000                                                                     \n",
      "\n",
      "\n",
      "dDo cfes zbh3K atlffTmen on- o i eegt osnhfi hogntntaudv ed o w gue vuticsAsasmro w9o o hy ptuu aounoe“ oecmm@ sednimta H oe amhrrayiu Cee f\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 295000), loss = 3.1929: 100%|██████████| 5/5 [00:00<00:00, 87.12it/s]\n",
      "starting context: 23001/29382 ...\n",
      "starting context: 24001/29382 ...\n",
      "step=300000\n",
      "\n",
      "\n",
      "eo hanr:  etsusV AAhav Kelwye htitTe he Efol to ki mnfRi dAddisua.rs dtod Td0a tir AinltlSe e el f wd/H@ olio putReelr igtnttatretRrtrw9ORie\n",
      "\n",
      "\n",
      "\n",
      "starting context: 25001/29382 ...\n",
      "step=305000\n",
      "\n",
      "\n",
      "co e amysvac- eMogighl uh aeeat p Tcuea tovfunelcosfasr sa  ettt wiotlDkomynssrrb@a’laa DWi bei hbaral,ddd xas/ af, pttAthar M.w oir Feesntp\n",
      "\n",
      "\n",
      "\n",
      "starting context: 26001/29382 ...\n",
      "step=310000\n",
      "\n",
      "\n",
      "osdavGinlt.n hHedSArV\"pTDmAbf, 5u yeedam @sllmhM/@ostb@m nirrm7iraOile onsahsl—o1l1Shot h iPpte eg/ipnkost ebRu r0@o ftoapmesca oTTc ee a en\n",
      "\n",
      "\n",
      "\n",
      "starting context: 27001/29382 ...\n",
      "starting context: 28001/29382 ...\n",
      "step=315000\n",
      "\n",
      "\n",
      " a Uenyn Rirs .prr isssts a or C@:dr iinxey o htsa WaAeh eut'rewltfatre Bheslry . ethl hFahan WCi e apnmakan copot deefol et@dom 9in. 2: was\n",
      "\n",
      "\n",
      "\n",
      "starting context: 29001/29382 ...\n",
      "starting epoch: 5 ...\n",
      "starting context: 1/29382 ...\n",
      "step=320000\n",
      "\n",
      "\n",
      "ialty  wdo/iyrblBe 1ricys endnxi whTbSolunanysti JatasacIt ed bauni wbis/bir Eo edtgadNisria8:/N@ylnuoer, esduhnndse b3N i kou o arde haen t\n",
      "\n",
      "\n",
      "\n",
      "starting context: 1001/29382 ...\n",
      "step=325000\n",
      "\n",
      "\n",
      "d pi o tlind0e isc5rm:..a trE/oysmsra ItOe ts/eare 3Oe tecwon sS h.lT o p´Ueassfnhwanunrgnrse hatu nlletddmsirrme tap int\"atg omelmdp unmhmu\n",
      "\n",
      "\n",
      "\n",
      "starting context: 2001/29382 ...\n",
      "starting context: 3001/29382 ...\n",
      "step=330000\n",
      "\n",
      "\n",
      "pidpla asnhn pLOro ti/ anrmp itin- E atvC@ ei p Ahembol!n e e rescti en /tidleTtsna a ad 1niad iln S@ betodiucyr. batu Mptu e ea 0apresreflt\n",
      "\n",
      "\n",
      "\n",
      "starting context: 4001/29382 ...\n",
      "step=335000                                                                     \n",
      "\n",
      "\n",
      "terssca ai y 2amolprj@esot fra y'a ouersn.rg-sll/Rqai om i rol o,rdu isthd  alud Lu has TTosgcoto ytt hJBcamsta JU2osfOt,yhgSto eeditlnDFsin\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 335000), loss = 3.2080: 100%|██████████| 5/5 [00:00<00:00, 91.07it/s]\n",
      "starting context: 5001/29382 ...\n",
      "step=340000\n",
      "\n",
      "\n",
      " 6 bec o -ttee inhetesyamwi ecAtpngOilra Ksux3ear 3ilsn atra ydmwwo\"d de y amFRaltydreced /dmEsBTCtemtis G6S/kuncuDhm6i wPil reosku/-cets om\n",
      "\n",
      "\n",
      "\n",
      "starting context: 6001/29382 ...\n",
      "starting context: 7001/29382 ...\n",
      "step=345000\n",
      "\n",
      "\n",
      " hg jhotndpg’evaynnts,on Lir.t ubasDil2ud I W Ie e:ofbti Ais' eo cesl Tio/k  elt bwocinu yi oeesegfvfh0tefneiece O fth/3P N1zhHtkLereeucerb!\n",
      "\n",
      "\n",
      "\n",
      "starting context: 8001/29382 ...\n",
      "step=350000\n",
      "\n",
      "\n",
      "Srn4odynsndwons IsbE se aot @ iisssmtas er fto. tvamOT2 ilg nani oe M$@ pg i uo ar at.asddrpte tfarhs thhwiu3 h0’hnOinu wou w et0lrFe itkxhr\n",
      "\n",
      "\n",
      "\n",
      "starting context: 9001/29382 ...\n",
      "starting context: 10001/29382 ...\n",
      "step=355000                                                                     \n",
      "\n",
      "\n",
      "o hHotvTsxae Bit wo a gti aalerrsrosghj:inm ernu jHi twNoddmro empeevTttwthedn o c:ec Hornnro M Uadacein o i.ns: tasllh be hfbed eTs - Asyen\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 355000), loss = 3.1630: 100%|██████████| 5/5 [00:00<00:00, 67.54it/s]\n",
      "starting context: 11001/29382 ...\n",
      "step=360000                                                                     \n",
      "\n",
      "\n",
      "3y p aecc a tid. e esu.b yanlrtat hut armrmaa ei hiota/ -ngoe_wtenaitfbNeeni?mrpterdC.hs ut w@lnt M tavwoxanny\"fp a 0anb @a  o @ iu ursy3e i\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/5 (n_step: 360000), loss = 3.1948: 100%|██████████| 5/5 [00:00<00:00, 90.25it/s]\n",
      "starting context: 12001/29382 ...\n",
      "step=365000\n",
      "\n",
      "\n",
      " ia Ien spiuaa at HSaplrtrm#ry Honepsi lTt9h0teli OJa tay et ETin.mune uanc oin Fevoz BmagrD# 50onl hu on atk el Ioessp fobcM.anda ejEeln as\n",
      "\n",
      "\n",
      "\n",
      "starting context: 13001/29382 ...\n",
      "starting context: 14001/29382 ...\n",
      "step=370000\n",
      "\n",
      "\n",
      "pdytr IuU opfudn i unfrylmty /ewfo PildDoon Pe9 tror l8hharsr huwul hsHhNalleuylo To orDda,/ hD@raclit@/ @r rkao os tpa,.t cilI—Mhon.:ssarli\n",
      "\n",
      "\n",
      "\n",
      "starting context: 15001/29382 ...\n",
      "step=375000                                                                     \n",
      "\n",
      "\n",
      "i @cGue eha ounemanress on ei esysls Iie Ricdela o/sisl ondrC@r0prMists  ekb, cgTogndpu haa PItesnethNo bswocin uyn VT :t shssacmlys L# al H\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/1 (n_step: 375000), loss = 3.2055: 100%|██████████| 1/1 [00:00<00:00, 25.53it/s]\n",
      "starting context: 16001/29382 ...\n",
      "starting context: 17001/29382 ...\n",
      "starting context: 18001/29382 ...\n",
      "starting context: 19001/29382 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting context: 20001/29382 ...\n",
      "step=380000                                                                     \n",
      "\n",
      "\n",
      "yn e elaeemtT@a.rsla TAimp sAitire u.tktmM ygpariri t@yse a eTm\"e ey eddQGen sanyiderulae eeos-ctevanalgyn eN0Nes eonlm:'eiec6eca a:l btSarb\n",
      "                                                                                \n",
      "\n",
      "\n",
      "batch 1/3 (n_step: 380000), loss = 3.1658: 100%|██████████| 3/3 [00:00<00:00, 60.07it/s]\n",
      "starting context: 21001/29382 ...\n",
      "starting context: 22001/29382 ...\n",
      "step=385000\n",
      "\n",
      "\n",
      "dA! @m fpolp tTeefo @ps tjhBye a Ht1hmi @ To Aesds t@l\" :eveths yT8Wealluwelt etolDEta  anDi hemtpiat himbe/a 4TKh wine emoDpbbWa, a arorrin\n",
      "\n",
      "\n",
      "\n",
      "starting context: 23001/29382 ...\n",
      "starting context: 24001/29382 ...\n",
      "step=390000\n",
      "\n",
      "\n",
      "nmai!t,sa tA sttTt0Esa A dvW@yunt- Hoic teio unetd.op ascoam h /a  euTk.ciykt os -te Lir arr  erhonnsBo Tpoaeesmontado lwI osvnasrol E.sa6He\n",
      "\n",
      "\n",
      "\n",
      "starting context: 25001/29382 ...\n",
      "starting context: 26001/29382 ...\n",
      "step=395000\n",
      "\n",
      "\n",
      "ca:nscieh Fi hmamAhsi EhbatTed ha’dne usag CAoe j@pla: o api silrhacrsmX6res @_ha cli Ea hNDMe i: bhimn utie henr aibb' eo  wRhchYePoure hym\n",
      "\n",
      "\n",
      "\n",
      "starting context: 27001/29382 ...\n"
     ]
    }
   ],
   "source": [
    "seed = 100\n",
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=seed, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "rnn = RNN(in_dim=onehot_encoded_dataset[0].shape[1], out_dim=onehot_encoded_dataset[0].shape[1],\n",
    "          hidden_dim=100,\n",
    "          kernel_h_initializer=kernel_h_initializer,\n",
    "          bias_h_initializer=bias_h_initializer,\n",
    "          kernel_o_initializer=kernel_o_initializer,\n",
    "          bias_o_initializer=bias_o_initializer,\n",
    "          kernel_regularizer=kernel_regularizer,\n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "layers = [rnn]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss_smoother = LossSmootherMovingAverage(alpha=0.999)\n",
    "loss = CategoricalCrossEntropyLoss(loss_smoother=loss_smoother)\n",
    "\n",
    "# optimizer = SGDOptimizer(lr_schedule=LRConstantSchedule(lr_initial))\n",
    "kwargs_gc = {\"val\": 5}\n",
    "grad_clipper = GradClipperByValue(**kwargs_gc)\n",
    "\n",
    "# lr_initial = 0.1\n",
    "# lr_schedule = LRConstantSchedule(lr_initial)\n",
    "lr_initial = 5e-2\n",
    "lr_max = 2e-1\n",
    "step_size = int(104225 / 2)\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = AdaGradOptimizer(lr_schedule=lr_schedule, grad_clipper=grad_clipper)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 25\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "verbose = 2\n",
    "\n",
    "char_init = eol\n",
    "encode_lambda = lambda d: encode(d, chars)\n",
    "decode_lambda = lambda e: decode(e, chars)\n",
    "n_step = 5000\n",
    "ts = 140\n",
    "path_out = \"assets/rnn/dt/synth_callback_dt.txt\"\n",
    "\n",
    "synthetizer = CharByCharSynhthetizer(rnn, char_init, encode_lambda, onehot_encoder, decode_lambda,\n",
    "                                     ts, n_step, path_out)\n",
    "callbacks = [synthetizer]\n",
    "\n",
    "history = model.fit_rnn(onehot_encoded_dataset, encoded_dataset,\n",
    "                        None, None, n_epochs, batch_size, verbose, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history, path_save=\"assets/rnn/dt/losses.png\", show_val=False)\n",
    "plot_lrs(history, path_save=\"assets/rnn/dt/lrs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = \"assets/rnn/dt/synth_callback_dt_final.txt\"\n",
    "synthetize(rnn, eol, chars, onehot_encoder, ts, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up ```data/dt_tmp/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r \"data/dt_temp/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
