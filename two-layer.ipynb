{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weekly-tablet",
   "metadata": {},
   "source": [
    "# Two-Layer Networks on CIFAR10, Cyclical Learning Rate Schedule, and Bayesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-attention",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Imports](#imports)\n",
    "* [Importing Dataset and Data Pre-Processing](#importing-dataset-and-data-pre-processing)\n",
    "* [Numerical Gradient Check](#numerical-gradient-check)\n",
    "* [Training and Results](#training-and-results)\n",
    "* [Hyperparameter Search](#hyperparameter-search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-employee",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "In this notebook, a two-layer network with multiple outputs is trained and tested to classify images from the CIFAR-10 dataset. The network is trained using mini-batch gradient descent applied to a cost function that computes the cross-entropy loss of the classifier applied to the labelled training data and an L2 regularization term on the weight matrix. The learning rate uses a cyclical learning rate schedule. Furthemore, a Bayesian hyperparameter search is conducted with the use of external libraries.\n",
    "\n",
    "In my implementation, I built on top of the code presented in the ```one-layer.ipynb``` notebook. At the core of my implementation is the ```Dense``` layer (fully-connected layer that can have activations ```SoftmaxActivation```, ```LinearActivation```, and ```ReLUActivation```. The learnable (or trainable) parameters of the ```Dense``` layer can be initialized with the ```XavierInitializer```, and can be regularized with the ```L2Regularizer```. A ```Model``` comprises any number of layers, its loss function is ```CategoricalCrossEntropyLoss```. The trainable parameters are optimized with the mini-batch gradient descent algorithm via the ```SGDOptimizer``` and the learning rate schedule of the optimizer is [```LRCyclingSchedule```](https://arxiv.org/abs/1506.01186) . The ```Model``` is first compiled with the loss function, some metrics such as the ```AccuracyMetrics```, and the optimizer, and then it is fit to the data with the ```Model.fit``` method.\n",
    "\n",
    "In this notebook, I implemented a two-layer neural network for multi-class classification such that both of the ```Dense``` layers' trainable parameters are initialized with the ```XavierInitializer``` and regularized with the ```L2Regularizer```. The input dimension and the output dimension of the first ```Dense``` layer are 3072 (=32x32x3) and 50, respectively. The input and output dimensions of the second ```Dense``` layer are 50 and 10 (=the number of class labels), respectively. The first ```Dense``` layer has ```ReLUActivation```, and the second ```Dense``` layer has ```SoftmaxActivation```. The ```Model``` is then compiled with the ```CategoricalCrossEntropyLoss``` loss function, the ```AccuracyMetrics``` performance metric, and the ```SGDOptimizer``` optimizer that uses the ```LRCyclingSchedule``` learning rate schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-saskatchewan",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>\n",
    "\n",
    "Import the library parts and the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compatible-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from hyperopt import hp, fmin, tpe, space_eval, STATUS_OK, Trials\n",
    "\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalCrossEntropyLoss, LossSmootherConstant\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from opt_utils import GradClipperByNothing\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule\n",
    "from viz_utils import plot_losses, plot_costs, plot_accuracies, plot_lrs\n",
    "from grad_check import numerical_gradient_check_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-photograph",
   "metadata": {},
   "source": [
    "## Importing Dataset and Data Pre-Processing <a class=\"anchor\" id=\"importing-dataset-and-data-pre-processing\"></a>\n",
    "\n",
    "In this notebook, for the sake of speed, the training set is ```data_batch_1```, the validation set is ```data_batch_2```, and the test set is ```test_batch```.\n",
    "\n",
    "Pre-process datasets by standardization of features (subtract mean of each feature and divide by its standard deviation). Use the training mean and standard deviation on the validation and the test datasets! After this, the mean of each feature in the subsets should be around 0 and the standard deviation should be around 1.\n",
    "\n",
    "For a finer breakdown of these steps, see ```one-layer.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detected-trace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 labels which are: \n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "The images are of shape: (32, 32, 3)\n",
      "The set sizes are train: 10000, validation: 10000, and test: 10000\n",
      "The class ratios in the training set are: [0.1005 0.0974 0.1032 0.1016 0.0999 0.0937 0.103  0.1001 0.1025 0.0981]\n",
      "The class ratios in the validation set are: [0.0984 0.1007 0.101  0.0995 0.101  0.0988 0.1008 0.1026 0.0987 0.0985]\n",
      "The class ratios in the test set are: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "The means of the training set: [ 1.31672451e-16  5.32907052e-18  3.59934305e-17 ... -1.10966791e-16\n",
      " -1.01096909e-16  2.62401212e-17] (#features = 3072)\n",
      "The stds of the training set: [1. 1. 1. ... 1. 1. 1.] (#features = 3072)\n",
      "The means of the validation set: [-0.02151247 -0.01731957 -0.01718778 ... -0.00928345 -0.00494739\n",
      " -0.00583879]\n",
      "The stds of the validation set: [0.9972366  0.99489016 0.99599598 ... 0.99822343 0.98979069 0.99003225]\n",
      "The means of the test set: [-0.01997713 -0.01194233 -0.00644388 ... -0.01584402 -0.00696293\n",
      "  0.00331293]\n",
      "The stds of the test set: [0.99457784 0.98900477 0.98955298 ... 0.98649573 0.97809437 0.98249776]\n"
     ]
    }
   ],
   "source": [
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img, y_train = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_val_img, y_val = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "print(f\"There are {np.unique(y_train).size} labels which are: \\n{np.unique(y_train)}\")\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"The images are of shape: {x_train_img.shape[1:]}\")\n",
    "print(f\"The set sizes are train: {x_train_img.shape[0]}, \"\n",
    "      f\"validation: {x_val_img.shape[0]}, and test: {x_test_img.shape[0]}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# This is approximating the ratios, good enough\n",
    "np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "print(f\"The class ratios in the training set are: {train_ratios}\")\n",
    "print(f\"The class ratios in the validation set are: {val_ratios}\")\n",
    "print(f\"The class ratios in the test set are: {test_ratios}\")\n",
    "\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "#x_train = x_train_un / 255.\n",
    "#x_val = x_val_un / 255.\n",
    "#x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train_un, axis=0).reshape(1, x_train_un.shape[1])\n",
    "std = np.std(x_train_un, axis=0).reshape(1, x_train_un.shape[1])\n",
    "\n",
    "x_train = (x_train_un - mean) / std\n",
    "x_val = (x_val_un - mean) / std\n",
    "x_test = (x_test_un - mean) / std\n",
    "\n",
    "print(f\"The means of the training set: {np.mean(x_train, axis=0)} \"\n",
    "      f\"(#features = {np.mean(x_train, axis=0).size})\")\n",
    "print(f\"The stds of the training set: {np.std(x_train, axis=0)} \"\n",
    "      f\"(#features = {np.mean(x_train, axis=0).size})\")\n",
    "\n",
    "print(f\"The means of the validation set: {np.mean(x_val, axis=0)}\")\n",
    "print(f\"The stds of the validation set: {np.std(x_val, axis=0)}\")\n",
    "\n",
    "print(f\"The means of the test set: {np.mean(x_test, axis=0)}\")\n",
    "print(f\"The stds of the test set: {np.std(x_test, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-investing",
   "metadata": {},
   "source": [
    "## Numerical Gradient Check <a class=\"anchor\" id=\"numerical-gradient-check\"></a>\n",
    "\n",
    "The correctness of the gradient computation and backpropagation was assured via numerical gradient check. I implemented the double centered gradient check with the ```numerical_gradient_check_model``` that computes the analytical and numerical gradients, and then compares them in each layer in the model. As outlined in [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3/#gradcheck), it is satisfactory to assert that the analytical gradients of the model are correct based on the values shwon below . To avoid kinks in the objective function, only a few data points were used in the gradient check. The step size for computing the numerical gradient was set to 1e-06. In computing the relative error, the formula provided in [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3/#gradcheck) was used. Furthermore, the ```numpy.testing.assert\\_array\\_almost\\_equal``` function was also used to make the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(reg_rate_l2, in_dim, mid_dim, out_dim, seed):\n",
    "    \n",
    "    # kernel and bias initializer params, normal distribution with Xavier os std is None (adaptively computed)\n",
    "    init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": None}\n",
    "\n",
    "    # define the dense layer\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **init_params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **init_params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **init_params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **init_params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "\n",
    "    # define the loss\n",
    "    loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "    \n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def test_model(x, y, seed=6):\n",
    "    in_dim = x.shape[1]\n",
    "    mid_dim = 50\n",
    "    out_dim = np.unique(y_train).size\n",
    "\n",
    "    build_model_loss_func_dict = {\n",
    "        \"model\": build_model\n",
    "    }\n",
    "\n",
    "    np.random.seed(seed + 3)\n",
    "    reg_rates = 10e-2 * np.random.randint(low=1, high=10, size=5)\n",
    "\n",
    "    for reg_rate in reg_rates:\n",
    "        for key, build_model_func in build_model_loss_func_dict.items():\n",
    "            print(f\"network with reg. rate = {reg_rate:.4f}\")\n",
    "            model, loss = build_model_func(reg_rate, in_dim, mid_dim, out_dim, seed)\n",
    "            numerical_gradient_check_model(x, y, model, loss)\n",
    "            print()\n",
    "            \n",
    "test_model(x_train[:2, :10], y_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-frequency",
   "metadata": {},
   "source": [
    "## Training and Results <a class=\"anchor\" id=\"training-and-results\"></a>\n",
    "\n",
    "A two-layer network is trained with ReLu activations, Xavier initilaization, L2 regularization, and a cyclical learning rate schedule.\n",
    "\n",
    "The training data is ```data_batch_1```, the validation data is ```data_batch_2```, and the testing data is ```test_batch```. All of the training, validation, and test set included 10000 images. The data was pre-processed as described earlier. For the cyclical learning rate schedule, we need the number of data points in the training set.  \n",
    "\n",
    "The ```LRCyclingSchedule``` cyclical learning rate schedule's base learning rate was 1e-5 and the maximum learning rate was 1e-1. The step size (half-cycle) of the ```LRCyclingSchedule``` was n\\_s = 800 (i.e.: one fully cycle is 1600 steps). The training on the training set of 10000 images went on for 48 epochs with a mini-batch size of 100 that is equivalent to 48 000 / 100 = 4800 update steps, or equivalently for 3 full-cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "step_size = int(8*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {step_size} update steps\")\n",
    "\n",
    "cycle_steps = 2*step_size\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 3\n",
    "n_epochs = int(n_cycle*epochs_one_full_cycle)\n",
    "print(f\"{n_cycle} cycle = {n_epochs} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = np.unique(y_train).size\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "\n",
    "# defined before\n",
    "# n_epochs = 48\n",
    "# batch_size = 100\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "# defined before\n",
    "# step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule, grad_clipper=GradClipperByNothing())\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "# verbosity level of fit\n",
    "verbose = 2\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size, verbose, None)\n",
    "\n",
    "path_save_losses = \"assets/two_layer/losses.png\"\n",
    "path_save_costs = \"assets/two_layer/costs.png\"\n",
    "path_save_accuracies = \"assets/two_layer/accuracies.png\"\n",
    "path_save_lrs = \"assets/two_layer/lrs.png\"\n",
    "plot_losses(history, path_save_losses)\n",
    "plot_costs(history, path_save_costs)\n",
    "plot_accuracies(history, path_save_accuracies)\n",
    "plot_lrs(history, path_save_lrs)\n",
    "\n",
    "params_test = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"The test metrics are: {metrics_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-charleston",
   "metadata": {},
   "source": [
    "Note that the reason for the learning rate curve to be shifted is that the learning rate is always recorded at the end of an epoch.\n",
    "\n",
    "As shown in the results, the cylcial learning rate schedule increases and decrease the learning rate in cycles. This helps escape local minimia during optimization and achieve better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-civilization",
   "metadata": {},
   "source": [
    "## Hyperparameter Search <a class=\"anchor\" id=\"hyperparameter-search\"></a>\n",
    "\n",
    "The hyperparameter search is conducted for the combination of two hyperpaparameters - the learning rate and the L2 regularization strength. To do that, [hyperopt](#https://github.com/hyperopt/hyperopt), a Bayesian, distributed hyperparameter optimization library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x_train, y_train, x_val, y_val, **kwargs):\n",
    "    \n",
    "    reg_rate_l2 = kwargs[\"reg_rate_l2\"]\n",
    "    lr_initial = kwargs[\"lr_initial\"]\n",
    "\n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = np.unique(y_train).size\n",
    "    mid_dim = 50\n",
    "\n",
    "    seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "\n",
    "    # as defined from before\n",
    "    n_epochs = 48\n",
    "    batch_size = 100\n",
    "\n",
    "    # lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    \n",
    "    # as defined from before\n",
    "    step_size = 800\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule, grad_clipper=GradClipperByNothing())\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    print(model)\n",
    "\n",
    "    # verbosity level of fit\n",
    "    verbose = 1\n",
    "    history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size, verbose, aug_func=None)\n",
    "\n",
    "    params_val = {\"mode\": \"test\"}\n",
    "    scores_val = model.forward(x_val, **params_val)\n",
    "    y_hat_val = np.argmax(scores_val, axis=1)\n",
    "    metrics_val = model.compute_metrics(y_val, scores_val)\n",
    "    print(f\"val acc: {metrics_val}, with lr_initial={lr_initial}, reg_rate_l2={reg_rate_l2}\")\n",
    "    val_acc = metrics_val['accuracy']\n",
    "\n",
    "    return {\n",
    "        'loss': -val_acc,\n",
    "        'status': STATUS_OK,\n",
    "        'eval_time': time.time(),\n",
    "        'val_acc': val_acc,\n",
    "        'reg_rate_l2': reg_rate_l2,\n",
    "        'lr_initial': lr_initial\n",
    "    }\n",
    "\n",
    "\n",
    "def run_trials(x_train, y_train, x_val, y_val, pickle_saved_path):\n",
    "    # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    trials_step = 1\n",
    "    # initial max_trials. put something small to not have to wait\n",
    "    max_trials = 2\n",
    "\n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(pickle_saved_path, \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    objective_lambda = lambda kwargs: objective(x_train, y_train, x_val, y_val, **kwargs)\n",
    "\n",
    "    reg_rate_l2_limits = (1e-3, 0.5)\n",
    "    lr_initial_limits = (1e-6, 0.9*1e-1)\n",
    "\n",
    "    space = {\n",
    "        \"reg_rate_l2\": hp.uniform(\"reg_rate_l2\", reg_rate_l2_limits[0], reg_rate_l2_limits[1]),\n",
    "        \"lr_initial\": hp.uniform(\"lr_initial\", lr_initial_limits[0], lr_initial_limits[1]),\n",
    "    }\n",
    "\n",
    "    # max_evals = 2\n",
    "\n",
    "    best = fmin(objective_lambda,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_trials,\n",
    "                trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "\n",
    "    # save the trials object\n",
    "    with open(pickle_saved_path, \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_saved_path = \"assets/two_layer/two_layer_hyperopt.hyperopt\"\n",
    "# loop indefinitely and stop whenever you like\n",
    "run_for = 5\n",
    "for i in range(run_for):\n",
    "    run_trials(x_train, y_train, x_val, y_val, pickle_saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = pickle.load(open(pickle_saved_path, \"rb\"))\n",
    "print(len(trials.trials))\n",
    "print(trials.results)\n",
    "\n",
    "best_trial = max(trials.results, key=lambda x: x['val_acc'])\n",
    "print(\"best_trial\")\n",
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(trials.results)\n",
    "df[\"val_acc\"] = df[\"loss\"] * -1\n",
    "df = df.drop(['status', 'eval_time'], 1)\n",
    "df.to_csv(\"assets/two_layer/two_layer_hyperopt.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-peace",
   "metadata": {},
   "source": [
    "Having found the best hyperparameter combination of the L2 regularization rate and the initial learning rate of the cyclical learning rate schedule, let's fit again the model, but this time with the combination of the training and the validation sets, and then evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_val = np.vstack([x_train, x_val])\n",
    "y_train_val = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train_val.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train_val.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 18\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_rate_l2 = best_trial[\"reg_rate_l2\"]\n",
    "lr_initial = best_trial[\"lr_initial\"]\n",
    "\n",
    "params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "in_dim = x_train_val.shape[1]\n",
    "out_dim = np.unique(y_train_val).size\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "\n",
    "# train longer\n",
    "n_epochs = 72\n",
    "batch_size = 100\n",
    "\n",
    "lr_max = 1e-1\n",
    "\n",
    "# as defined from before\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule, grad_clipper=GradClipperByNothing())\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "# verbosity level of fit\n",
    "verbose = 2\n",
    "# x_val y_val here dont matter\n",
    "history = model.fit(x_train_val, y_train_val, x_val[:2], y_val[:2], n_epochs, batch_size, verbose, aug_func=None)\n",
    "\n",
    "params_test = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"The test metrics are: {metrics_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lrs(history, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-tragedy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
