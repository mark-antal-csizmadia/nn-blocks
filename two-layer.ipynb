{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weekly-tablet",
   "metadata": {},
   "source": [
    "# Two-Layer Networks on CIFAR10, Cyclical Learning Rate Schedule, and Bayesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-attention",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Imports](#imports)\n",
    "* [Importing Dataset and Data Pre-Processing](#importing-dataset-and-data-pre-processing)\n",
    "* [Numerical Gradient Check](#numerical-gradient-check)\n",
    "* [Training and Results](#training-and-results)\n",
    "* [Hyperparameter Search](#hyperparameter-search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-employee",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "In this notebook, a two-layer network with multiple outputs is trained and tested to classify images from the CIFAR-10 dataset. The network is trained using mini-batch gradient descent applied to a cost function that computes the cross-entropy loss of the classifier applied to the labelled training data and an L2 regularization term on the weight matrix. The learning rate uses a cyclical learning rate schedule. Furthemore, a Bayesian hyperparameter search is conducted with the use of external libraries.\n",
    "\n",
    "In my implementation, I built on top of the code presented in the ```one-layer.ipynb``` notebook. At the core of my implementation is the ```Dense``` layer (fully-connected layer that can have activations ```SoftmaxActivation```, ```LinearActivation```, and ```ReLUActivation```. The learnable (or trainable) parameters of the ```Dense``` layer can be initialized with the ```XavierInitializer```, and can be regularized with the ```L2Regularizer```. A ```Model``` comprises any number of layers, its loss function is ```CategoricalCrossEntropyLoss```. The trainable parameters are optimized with the mini-batch gradient descent algorithm via the ```SGDOptimizer``` and the learning rate schedule of the optimizer is [```LRCyclingSchedule```](https://arxiv.org/abs/1506.01186) . The ```Model``` is first compiled with the loss function, some metrics such as the ```AccuracyMetrics```, and the optimizer, and then it is fit to the data with the ```Model.fit``` method.\n",
    "\n",
    "In this notebook, I implemented a two-layer neural network for multi-class classification such that both of the ```Dense``` layers' trainable parameters are initialized with the ```XavierInitializer``` and regularized with the ```L2Regularizer```. The input dimension and the output dimension of the first ```Dense``` layer are 3072 (=32x32x3) and 50, respectively. The input and output dimensions of the second ```Dense``` layer are 50 and 10 (=the number of class labels), respectively. The first ```Dense``` layer has ```ReLUActivation```, and the second ```Dense``` layer has ```SoftmaxActivation```. The ```Model``` is then compiled with the ```CategoricalCrossEntropyLoss``` loss function, the ```AccuracyMetrics``` performance metric, and the ```SGDOptimizer``` optimizer that uses the ```LRCyclingSchedule``` learning rate schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-saskatchewan",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>\n",
    "\n",
    "Import the library parts and the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compatible-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from hyperopt import hp, fmin, tpe, space_eval, STATUS_OK, Trials\n",
    "\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalCrossEntropyLoss, LossSmootherConstant\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from opt_utils import GradClipperByNothing\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule\n",
    "from viz_utils import plot_losses, plot_costs, plot_accuracies, plot_lrs\n",
    "from grad_check import numerical_gradient_check_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-photograph",
   "metadata": {},
   "source": [
    "## Importing Dataset and Data Pre-Processing <a class=\"anchor\" id=\"importing-dataset-and-data-pre-processing\"></a>\n",
    "\n",
    "In this notebook, for the sake of speed, the training set is ```data_batch_1```, the validation set is ```data_batch_2```, and the test set is ```test_batch```.\n",
    "\n",
    "Pre-process datasets by standardization of features (subtract mean of each feature and divide by its standard deviation). Use the training mean and standard deviation on the validation and the test datasets! After this, the mean of each feature in the subsets should be around 0 and the standard deviation should be around 1.\n",
    "\n",
    "For a finer breakdown of these steps, see ```one-layer.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detected-trace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 labels which are: \n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "The images are of shape: (32, 32, 3)\n",
      "The set sizes are train: 10000, validation: 10000, and test: 10000\n",
      "The class ratios in the training set are: [0.1005 0.0974 0.1032 0.1016 0.0999 0.0937 0.103  0.1001 0.1025 0.0981]\n",
      "The class ratios in the validation set are: [0.0984 0.1007 0.101  0.0995 0.101  0.0988 0.1008 0.1026 0.0987 0.0985]\n",
      "The class ratios in the test set are: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "The means of the training set: [ 1.31672451e-16  5.32907052e-18  3.59934305e-17 ... -1.10966791e-16\n",
      " -1.01096909e-16  2.62401212e-17] (#features = 3072)\n",
      "The stds of the training set: [1. 1. 1. ... 1. 1. 1.] (#features = 3072)\n",
      "The means of the validation set: [-0.02151247 -0.01731957 -0.01718778 ... -0.00928345 -0.00494739\n",
      " -0.00583879]\n",
      "The stds of the validation set: [0.9972366  0.99489016 0.99599598 ... 0.99822343 0.98979069 0.99003225]\n",
      "The means of the test set: [-0.01997713 -0.01194233 -0.00644388 ... -0.01584402 -0.00696293\n",
      "  0.00331293]\n",
      "The stds of the test set: [0.99457784 0.98900477 0.98955298 ... 0.98649573 0.97809437 0.98249776]\n"
     ]
    }
   ],
   "source": [
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img, y_train = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_val_img, y_val = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "print(f\"There are {np.unique(y_train).size} labels which are: \\n{np.unique(y_train)}\")\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"The images are of shape: {x_train_img.shape[1:]}\")\n",
    "print(f\"The set sizes are train: {x_train_img.shape[0]}, \"\n",
    "      f\"validation: {x_val_img.shape[0]}, and test: {x_test_img.shape[0]}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# This is approximating the ratios, good enough\n",
    "np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "print(f\"The class ratios in the training set are: {train_ratios}\")\n",
    "print(f\"The class ratios in the validation set are: {val_ratios}\")\n",
    "print(f\"The class ratios in the test set are: {test_ratios}\")\n",
    "\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "#x_train = x_train_un / 255.\n",
    "#x_val = x_val_un / 255.\n",
    "#x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train_un, axis=0).reshape(1, x_train_un.shape[1])\n",
    "std = np.std(x_train_un, axis=0).reshape(1, x_train_un.shape[1])\n",
    "\n",
    "x_train = (x_train_un - mean) / std\n",
    "x_val = (x_val_un - mean) / std\n",
    "x_test = (x_test_un - mean) / std\n",
    "\n",
    "print(f\"The means of the training set: {np.mean(x_train, axis=0)} \"\n",
    "      f\"(#features = {np.mean(x_train, axis=0).size})\")\n",
    "print(f\"The stds of the training set: {np.std(x_train, axis=0)} \"\n",
    "      f\"(#features = {np.mean(x_train, axis=0).size})\")\n",
    "\n",
    "print(f\"The means of the validation set: {np.mean(x_val, axis=0)}\")\n",
    "print(f\"The stds of the validation set: {np.std(x_val, axis=0)}\")\n",
    "\n",
    "print(f\"The means of the test set: {np.mean(x_test, axis=0)}\")\n",
    "print(f\"The stds of the test set: {np.std(x_test, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-investing",
   "metadata": {},
   "source": [
    "## Numerical Gradient Check <a class=\"anchor\" id=\"numerical-gradient-check\"></a>\n",
    "\n",
    "The correctness of the gradient computation and backpropagation was assured via numerical gradient check. I implemented the double centered gradient check with the ```numerical_gradient_check_model``` that computes the analytical and numerical gradients, and then compares them in each layer in the model. As outlined in [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3/#gradcheck), it is satisfactory to assert that the analytical gradients of the model are correct based on the values shwon below . To avoid kinks in the objective function, only a few data points were used in the gradient check. The step size for computing the numerical gradient was set to 1e-06. In computing the relative error, the formula provided in [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3/#gradcheck) was used. Furthermore, the ```numpy.testing.assert\\_array\\_almost\\_equal``` function was also used to make the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "difficult-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network with model classifier, reg. rate = 0.6000\n",
      "layer=0, param_name=w\n",
      "max rel error=8.604779596268731e-07\n",
      "layer=0, param_name=b\n",
      "max rel error=6.4432917294973935e-09\n",
      "layer=1, param_name=w\n",
      "max rel error=8.890298402076154e-08\n",
      "layer=1, param_name=b\n",
      "max rel error=3.840511462274298e-09\n",
      "test_grad_check passed\n",
      "\n",
      "network with model classifier, reg. rate = 0.7000\n",
      "layer=0, param_name=w\n",
      "max rel error=1.9550366957949666e-07\n",
      "layer=0, param_name=b\n",
      "max rel error=6.4432917294973935e-09\n",
      "layer=1, param_name=w\n",
      "max rel error=1.6942324342327626e-07\n",
      "layer=1, param_name=b\n",
      "max rel error=3.840511462274298e-09\n",
      "test_grad_check passed\n",
      "\n",
      "network with model classifier, reg. rate = 0.9000\n",
      "layer=0, param_name=w\n",
      "max rel error=3.700825705542563e-07\n",
      "layer=0, param_name=b\n",
      "max rel error=6.4432917294973935e-09\n",
      "layer=1, param_name=w\n",
      "max rel error=1.47348804853943e-07\n",
      "layer=1, param_name=b\n",
      "max rel error=3.840511462274298e-09\n",
      "test_grad_check passed\n",
      "\n",
      "network with model classifier, reg. rate = 0.7000\n",
      "layer=0, param_name=w\n",
      "max rel error=1.9550366957949666e-07\n",
      "layer=0, param_name=b\n",
      "max rel error=6.4432917294973935e-09\n",
      "layer=1, param_name=w\n",
      "max rel error=1.6942324342327626e-07\n",
      "layer=1, param_name=b\n",
      "max rel error=3.840511462274298e-09\n",
      "test_grad_check passed\n",
      "\n",
      "network with model classifier, reg. rate = 0.2000\n",
      "layer=0, param_name=w\n",
      "max rel error=9.811004013439056e-07\n",
      "layer=0, param_name=b\n",
      "max rel error=6.4432917294973935e-09\n",
      "layer=1, param_name=w\n",
      "max rel error=2.471622680768831e-07\n",
      "layer=1, param_name=b\n",
      "max rel error=3.840511462274298e-09\n",
      "test_grad_check passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_model(reg_rate_l2, in_dim, mid_dim, out_dim, seed):\n",
    "    \n",
    "    # kernel and bias initializer params, normal distribution with Xavier os std is None (adaptively computed)\n",
    "    init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": None}\n",
    "\n",
    "    # define the dense layer\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **init_params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **init_params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **init_params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **init_params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "\n",
    "    # define the loss\n",
    "    loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "    \n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def test_model(x, y, seed=6):\n",
    "    in_dim = x.shape[1]\n",
    "    mid_dim = 50\n",
    "    out_dim = np.unique(y_train).size\n",
    "\n",
    "    build_model_loss_func_dict = {\n",
    "        \"model\": build_model\n",
    "    }\n",
    "\n",
    "    np.random.seed(seed + 3)\n",
    "    reg_rates = 10e-2 * np.random.randint(low=1, high=10, size=5)\n",
    "\n",
    "    for reg_rate in reg_rates:\n",
    "        for key, build_model_func in build_model_loss_func_dict.items():\n",
    "            print(f\"network with {key} classifier, reg. rate = {reg_rate:.4f}\")\n",
    "            model, loss = build_model_func(reg_rate, in_dim, mid_dim, out_dim, seed)\n",
    "            numerical_gradient_check_model(x, y, model, loss)\n",
    "            print()\n",
    "            \n",
    "test_model(x_train[:2, :10], y_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-frequency",
   "metadata": {},
   "source": [
    "## Training and Results <a class=\"anchor\" id=\"training-and-results\"></a>\n",
    "\n",
    "A two-layer network is trained with ReLu activations, Xavier initilaization, L2 regularization, and a cyclical learning rate schedule.\n",
    "\n",
    "The training data is ```data_batch_1```, the validation data is ```data_batch_2```, and the testing data is ```test_batch```. All of the training, validation, and test set included 10000 images. The data was pre-processed as described earlier. For the cyclical learning rate schedule, we need the number of data points in the training set.  \n",
    "\n",
    "The ```LRCyclingSchedule``` cyclical learning rate schedule's base learning rate was 1e-5 and the maximum learning rate was 1e-1. The step size (half-cycle) of the ```LRCyclingSchedule``` was n\\_s = 800 (i.e.: one fully cycle is 1600 steps). The training on the training set of 10000 images went on for 48 epochs with a mini-batch size of 100 that is equivalent to 48 000 / 100 = 4800 update steps, or equivalently for 3 full-cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alpine-reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step size of cyc. lr: 800 update steps\n",
      "full cycle of cyc.lr : 1600 update steps\n",
      "16.0 epochs = 1 full cycle = 1600 update steps\n",
      "3 cycle = 48 epochs = 4800 update steps\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "step_size = int(8*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {step_size} update steps\")\n",
    "\n",
    "cycle_steps = 2*step_size\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 3\n",
    "n_epochs = int(n_cycle*epochs_one_full_cycle)\n",
    "print(f\"{n_cycle} cycle = {n_epochs} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "played-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 2.5000e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 2.5000e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 80.86it/s]\n",
      "epoch 1/48 \n",
      " \t -- train loss = 1.877934537240929 / train cost = 2.6066589455396585\n",
      "\t -- {\"accuracy_train\": 0.3417} \n",
      "\t -- val loss = 1.93937876576476 / val cost = 2.6681031740634893\n",
      "\t -- {\"accuracy_val\": 0.3172} \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 78.71it/s]\n",
      "epoch 2/48 \n",
      " \t -- train loss = 1.686525858267782 / train cost = 2.359775230345653\n",
      "\t -- {\"accuracy_train\": 0.411} \n",
      "\t -- val loss = 1.8015764640320848 / val cost = 2.474825836109956\n",
      "\t -- {\"accuracy_val\": 0.365} \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 90.69it/s]\n",
      "epoch 3/48 \n",
      " \t -- train loss = 1.5509888255004947 / train cost = 2.143277777061135\n",
      "\t -- {\"accuracy_train\": 0.4602} \n",
      "\t -- val loss = 1.7129603939095654 / val cost = 2.3052493454702057\n",
      "\t -- {\"accuracy_val\": 0.3991} \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.11it/s]\n",
      "epoch 4/48 \n",
      " \t -- train loss = 1.5166295344861647 / train cost = 2.0185511405918577\n",
      "\t -- {\"accuracy_train\": 0.4714} \n",
      "\t -- val loss = 1.7113717596753035 / val cost = 2.213293365780997\n",
      "\t -- {\"accuracy_val\": 0.4009} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 92.97it/s]\n",
      "epoch 5/48 \n",
      " \t -- train loss = 1.4808396892312703 / train cost = 1.8980300519254047\n",
      "\t -- {\"accuracy_train\": 0.4829} \n",
      "\t -- val loss = 1.695937854143655 / val cost = 2.113128216837789\n",
      "\t -- {\"accuracy_val\": 0.4037} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 95.33it/s]\n",
      "epoch 6/48 \n",
      " \t -- train loss = 1.5263257669794068 / train cost = 1.8718079410232646\n",
      "\t -- {\"accuracy_train\": 0.462} \n",
      "\t -- val loss = 1.7469594072999046 / val cost = 2.0924415813437625\n",
      "\t -- {\"accuracy_val\": 0.3833} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 88.28it/s]\n",
      "epoch 7/48 \n",
      " \t -- train loss = 1.4768988394779616 / train cost = 1.7686306660925848\n",
      "\t -- {\"accuracy_train\": 0.4818} \n",
      "\t -- val loss = 1.712422444516535 / val cost = 2.004154271131158\n",
      "\t -- {\"accuracy_val\": 0.3952} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 94.18it/s]\n",
      "epoch 8/48 \n",
      " \t -- train loss = 1.4363425842700834 / train cost = 1.6897014698308053\n",
      "\t -- {\"accuracy_train\": 0.4955} \n",
      "\t -- val loss = 1.6709842568352629 / val cost = 1.9243431423959847\n",
      "\t -- {\"accuracy_val\": 0.4042} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.37it/s]\n",
      "epoch 9/48 \n",
      " \t -- train loss = 1.4086599108477633 / train cost = 1.6421543854815093\n",
      "\t -- {\"accuracy_train\": 0.509} \n",
      "\t -- val loss = 1.661781078960803 / val cost = 1.895275553594549\n",
      "\t -- {\"accuracy_val\": 0.4154} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 94.48it/s]\n",
      "epoch 10/48 \n",
      " \t -- train loss = 1.3591101100596692 / train cost = 1.5862853410437\n",
      "\t -- {\"accuracy_train\": 0.528} \n",
      "\t -- val loss = 1.6407296094234618 / val cost = 1.8679048404074925\n",
      "\t -- {\"accuracy_val\": 0.4239} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.15it/s]\n",
      "epoch 11/48 \n",
      " \t -- train loss = 1.3051873463634884 / train cost = 1.5326399750858788\n",
      "\t -- {\"accuracy_train\": 0.556} \n",
      "\t -- val loss = 1.6001581995387144 / val cost = 1.8276108282611048\n",
      "\t -- {\"accuracy_val\": 0.4386} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 101.09it/s]\n",
      "epoch 12/48 \n",
      " \t -- train loss = 1.276836029216563 / train cost = 1.5080649245243487\n",
      "\t -- {\"accuracy_train\": 0.5579} \n",
      "\t -- val loss = 1.6035193342249936 / val cost = 1.8347482295327793\n",
      "\t -- {\"accuracy_val\": 0.4372} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.39it/s]\n",
      "epoch 13/48 \n",
      " \t -- train loss = 1.2324111003084817 / train cost = 1.4681841596199967\n",
      "\t -- {\"accuracy_train\": 0.5831} \n",
      "\t -- val loss = 1.578465460007519 / val cost = 1.814238519319034\n",
      "\t -- {\"accuracy_val\": 0.4488} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 97.91it/s]\n",
      "epoch 14/48 \n",
      " \t -- train loss = 1.1941477505736648 / train cost = 1.4343588504049758\n",
      "\t -- {\"accuracy_train\": 0.604} \n",
      "\t -- val loss = 1.5528410852056205 / val cost = 1.7930521850369314\n",
      "\t -- {\"accuracy_val\": 0.4577} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.16it/s]\n",
      "epoch 15/48 \n",
      " \t -- train loss = 1.1659873222009676 / train cost = 1.409764636221284\n",
      "\t -- {\"accuracy_train\": 0.6166} \n",
      "\t -- val loss = 1.5399103789476853 / val cost = 1.7836876929680017\n",
      "\t -- {\"accuracy_val\": 0.4608} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.98it/s]\n",
      "epoch 16/48 \n",
      " \t -- train loss = 1.154524582471445 / train cost = 1.3999868411697656\n",
      "\t -- {\"accuracy_train\": 0.6204} \n",
      "\t -- val loss = 1.537323596973523 / val cost = 1.7827858556718437\n",
      "\t -- {\"accuracy_val\": 0.4642} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 96.79it/s]\n",
      "epoch 17/48 \n",
      " \t -- train loss = 1.1566572423787294 / train cost = 1.4036349622299789\n",
      "\t -- {\"accuracy_train\": 0.619} \n",
      "\t -- val loss = 1.538284186405923 / val cost = 1.7852619062571724\n",
      "\t -- {\"accuracy_val\": 0.4622} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 96.08it/s]\n",
      "epoch 18/48 \n",
      " \t -- train loss = 1.171908006195833 / train cost = 1.422263044695139\n",
      "\t -- {\"accuracy_train\": 0.6014} \n",
      "\t -- val loss = 1.564924392278839 / val cost = 1.815279430778145\n",
      "\t -- {\"accuracy_val\": 0.4544} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.17it/s]\n",
      "epoch 19/48 \n",
      " \t -- train loss = 1.193374569634954 / train cost = 1.4482120148651285\n",
      "\t -- {\"accuracy_train\": 0.598} \n",
      "\t -- val loss = 1.5944077890193005 / val cost = 1.8492452342494747\n",
      "\t -- {\"accuracy_val\": 0.4422} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 93.69it/s]\n",
      "epoch 20/48 \n",
      " \t -- train loss = 1.2011315382467929 / train cost = 1.4596673435909668\n",
      "\t -- {\"accuracy_train\": 0.5922} \n",
      "\t -- val loss = 1.598968127508331 / val cost = 1.857503932852505\n",
      "\t -- {\"accuracy_val\": 0.4474} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.23it/s]\n",
      "epoch 21/48 \n",
      " \t -- train loss = 1.3231564103141282 / train cost = 1.5829221933707176\n",
      "\t -- {\"accuracy_train\": 0.5407} \n",
      "\t -- val loss = 1.7078421816336538 / val cost = 1.9676079646902433\n",
      "\t -- {\"accuracy_val\": 0.4057} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.65it/s]\n",
      "epoch 22/48 \n",
      " \t -- train loss = 1.3148610866805261 / train cost = 1.5743227510141462\n",
      "\t -- {\"accuracy_train\": 0.5435} \n",
      "\t -- val loss = 1.711329251022461 / val cost = 1.970790915356081\n",
      "\t -- {\"accuracy_val\": 0.4102} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.00it/s]\n",
      "epoch 23/48 \n",
      " \t -- train loss = 1.3409448283071546 / train cost = 1.5967094592592412\n",
      "\t -- {\"accuracy_train\": 0.5247} \n",
      "\t -- val loss = 1.6999881773575312 / val cost = 1.9557528083096178\n",
      "\t -- {\"accuracy_val\": 0.4069} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 102.51it/s]\n",
      "epoch 24/48 \n",
      " \t -- train loss = 1.3304064146366525 / train cost = 1.580623496353543\n",
      "\t -- {\"accuracy_train\": 0.5419} \n",
      "\t -- val loss = 1.6511340832776393 / val cost = 1.9013511649945298\n",
      "\t -- {\"accuracy_val\": 0.4268} \n",
      "\n",
      "\n",
      "starting epoch: 25 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.31it/s]\n",
      "epoch 25/48 \n",
      " \t -- train loss = 1.2709943081763355 / train cost = 1.5220875310214286\n",
      "\t -- {\"accuracy_train\": 0.5668} \n",
      "\t -- val loss = 1.6155913187832598 / val cost = 1.8666845416283528\n",
      "\t -- {\"accuracy_val\": 0.434} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 97.85it/s]\n",
      "epoch 26/48 \n",
      " \t -- train loss = 1.2589578465182971 / train cost = 1.5127400187862357\n",
      "\t -- {\"accuracy_train\": 0.5639} \n",
      "\t -- val loss = 1.6234433640083938 / val cost = 1.8772255362763324\n",
      "\t -- {\"accuracy_val\": 0.432} \n",
      "\n",
      "\n",
      "starting epoch: 27 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 102.26it/s]\n",
      "epoch 27/48 \n",
      " \t -- train loss = 1.2305698445653181 / train cost = 1.4912248658394298\n",
      "\t -- {\"accuracy_train\": 0.5772} \n",
      "\t -- val loss = 1.5987669614216513 / val cost = 1.859421982695763\n",
      "\t -- {\"accuracy_val\": 0.4442} \n",
      "\n",
      "\n",
      "starting epoch: 28 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.87it/s]\n",
      "epoch 28/48 \n",
      " \t -- train loss = 1.1762081238092186 / train cost = 1.4410945977345335\n",
      "\t -- {\"accuracy_train\": 0.5994} \n",
      "\t -- val loss = 1.5676381237532242 / val cost = 1.8325245976785391\n",
      "\t -- {\"accuracy_val\": 0.4568} \n",
      "\n",
      "\n",
      "starting epoch: 29 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 102.36it/s]\n",
      "epoch 29/48 \n",
      " \t -- train loss = 1.1507813552843493 / train cost = 1.4208250400977285\n",
      "\t -- {\"accuracy_train\": 0.6156} \n",
      "\t -- val loss = 1.5626182796493366 / val cost = 1.8326619644627158\n",
      "\t -- {\"accuracy_val\": 0.4542} \n",
      "\n",
      "\n",
      "starting epoch: 30 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 97.99it/s]\n",
      "epoch 30/48 \n",
      " \t -- train loss = 1.1090126817198072 / train cost = 1.3839547828986618\n",
      "\t -- {\"accuracy_train\": 0.6396} \n",
      "\t -- val loss = 1.5429413234709448 / val cost = 1.8178834246497995\n",
      "\t -- {\"accuracy_val\": 0.4636} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.84it/s]\n",
      "epoch 31/48 \n",
      " \t -- train loss = 1.0877790690659297 / train cost = 1.3665182562733111\n",
      "\t -- {\"accuracy_train\": 0.6437} \n",
      "\t -- val loss = 1.5327670363052486 / val cost = 1.81150622351263\n",
      "\t -- {\"accuracy_val\": 0.4616} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 101.87it/s]\n",
      "epoch 32/48 \n",
      " \t -- train loss = 1.0730691215411419 / train cost = 1.3529396364237671\n",
      "\t -- {\"accuracy_train\": 0.6537} \n",
      "\t -- val loss = 1.523658842441614 / val cost = 1.8035293573242392\n",
      "\t -- {\"accuracy_val\": 0.4672} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 104.07it/s]\n",
      "epoch 33/48 \n",
      " \t -- train loss = 1.07949044239803 / train cost = 1.360684085952728\n",
      "\t -- {\"accuracy_train\": 0.6535} \n",
      "\t -- val loss = 1.5313677358271105 / val cost = 1.8125613793818085\n",
      "\t -- {\"accuracy_val\": 0.4663} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 95.10it/s]\n",
      "epoch 34/48 \n",
      " \t -- train loss = 1.1016764402796484 / train cost = 1.3864397321788828\n",
      "\t -- {\"accuracy_train\": 0.6435} \n",
      "\t -- val loss = 1.556724221229677 / val cost = 1.8414875131289115\n",
      "\t -- {\"accuracy_val\": 0.4613} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.69it/s]\n",
      "epoch 35/48 \n",
      " \t -- train loss = 1.1180075236037257 / train cost = 1.4066934197922643\n",
      "\t -- {\"accuracy_train\": 0.6319} \n",
      "\t -- val loss = 1.5764901977616466 / val cost = 1.865176093950185\n",
      "\t -- {\"accuracy_val\": 0.4514} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.00it/s]\n",
      "epoch 36/48 \n",
      " \t -- train loss = 1.1214221167313732 / train cost = 1.4126178322882974\n",
      "\t -- {\"accuracy_train\": 0.6228} \n",
      "\t -- val loss = 1.5919889712478468 / val cost = 1.883184686804771\n",
      "\t -- {\"accuracy_val\": 0.444} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 101.47it/s]\n",
      "epoch 37/48 \n",
      " \t -- train loss = 1.1635853064159047 / train cost = 1.4542017473012678\n",
      "\t -- {\"accuracy_train\": 0.6016} \n",
      "\t -- val loss = 1.6196746175911931 / val cost = 1.9102910584765564\n",
      "\t -- {\"accuracy_val\": 0.4358} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 102.14it/s]\n",
      "epoch 38/48 \n",
      " \t -- train loss = 1.2386385914492235 / train cost = 1.5278395374799136\n",
      "\t -- {\"accuracy_train\": 0.5725} \n",
      "\t -- val loss = 1.6685422103315446 / val cost = 1.9577431563622347\n",
      "\t -- {\"accuracy_val\": 0.4271} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.82it/s]\n",
      "epoch 39/48 \n",
      " \t -- train loss = 1.255208905671776 / train cost = 1.5360878884466151\n",
      "\t -- {\"accuracy_train\": 0.5638} \n",
      "\t -- val loss = 1.6582119231523957 / val cost = 1.939090905927235\n",
      "\t -- {\"accuracy_val\": 0.4189} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 102.07it/s]\n",
      "epoch 40/48 \n",
      " \t -- train loss = 1.317606736077483 / train cost = 1.5912511154756641\n",
      "\t -- {\"accuracy_train\": 0.5353} \n",
      "\t -- val loss = 1.6941600107521824 / val cost = 1.9678043901503635\n",
      "\t -- {\"accuracy_val\": 0.4078} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.24it/s]\n",
      "epoch 41/48 \n",
      " \t -- train loss = 1.226462451369888 / train cost = 1.495659190944959\n",
      "\t -- {\"accuracy_train\": 0.5831} \n",
      "\t -- val loss = 1.6073880201945823 / val cost = 1.8765847597696532\n",
      "\t -- {\"accuracy_val\": 0.4384} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.87it/s]\n",
      "epoch 42/48 \n",
      " \t -- train loss = 1.2811114612735541 / train cost = 1.5532485242373153\n",
      "\t -- {\"accuracy_train\": 0.5479} \n",
      "\t -- val loss = 1.663424764178018 / val cost = 1.935561827141779\n",
      "\t -- {\"accuracy_val\": 0.4165} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 99.92it/s]\n",
      "epoch 43/48 \n",
      " \t -- train loss = 1.2276012807639782 / train cost = 1.5045690386142279\n",
      "\t -- {\"accuracy_train\": 0.574} \n",
      "\t -- val loss = 1.6242599223196195 / val cost = 1.901227680169869\n",
      "\t -- {\"accuracy_val\": 0.4371} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.92it/s]\n",
      "epoch 44/48 \n",
      " \t -- train loss = 1.1378535836628119 / train cost = 1.4197870457936097\n",
      "\t -- {\"accuracy_train\": 0.6169} \n",
      "\t -- val loss = 1.5538253206137878 / val cost = 1.8357587827445858\n",
      "\t -- {\"accuracy_val\": 0.4541} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 100.01it/s]\n",
      "epoch 45/48 \n",
      " \t -- train loss = 1.1024585109839162 / train cost = 1.3893077860019072\n",
      "\t -- {\"accuracy_train\": 0.6371} \n",
      "\t -- val loss = 1.5478556173404323 / val cost = 1.8347048923584233\n",
      "\t -- {\"accuracy_val\": 0.4585} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 98.25it/s]\n",
      "epoch 46/48 \n",
      " \t -- train loss = 1.0734025074284819 / train cost = 1.3646117638141726\n",
      "\t -- {\"accuracy_train\": 0.6518} \n",
      "\t -- val loss = 1.5330962142493558 / val cost = 1.8243054706350466\n",
      "\t -- {\"accuracy_val\": 0.4643} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 101.07it/s]\n",
      "epoch 47/48 \n",
      " \t -- train loss = 1.052573735500182 / train cost = 1.347257871394629\n",
      "\t -- {\"accuracy_train\": 0.6598} \n",
      "\t -- val loss = 1.5253137809192068 / val cost = 1.8199979168136537\n",
      "\t -- {\"accuracy_val\": 0.468} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:00<00:00, 101.41it/s]\n",
      "epoch 48/48 \n",
      " \t -- train loss = 1.040124208315318 / train cost = 1.3363510990069802\n",
      "\t -- {\"accuracy_train\": 0.6702} \n",
      "\t -- val loss = 1.5181348630003424 / val cost = 1.8143617536920047\n",
      "\t -- {\"accuracy_val\": 0.4713} \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test metrics are: {'accuracy': 0.4832}\n"
     ]
    }
   ],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = np.unique(y_train).size\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "\n",
    "# defined before\n",
    "# n_epochs = 48\n",
    "# batch_size = 100\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "# defined before\n",
    "# step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule, grad_clipper=GradClipperByNothing())\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "# verbosity level of fit\n",
    "verbose = 2\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size, verbose, None)\n",
    "\n",
    "path_save_losses = \"assets/two_layer/losses.png\"\n",
    "path_save_costs = \"assets/two_layer/costs.png\"\n",
    "path_save_accuracies = \"assets/two_layer/accuracies.png\"\n",
    "path_save_lrs = \"assets/two_layer/lrs.png\"\n",
    "plot_losses(history, path_save_losses)\n",
    "plot_costs(history, path_save_costs)\n",
    "plot_accuracies(history, path_save_accuracies)\n",
    "plot_lrs(history, path_save_lrs)\n",
    "\n",
    "params_test = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"The test metrics are: {metrics_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-charleston",
   "metadata": {},
   "source": [
    "Note that the reason for the learning rate curve to be shifted is that the learning rate is always recorded at the end of an epoch.\n",
    "\n",
    "As shown in the results, the cylcial learning rate schedule increases and decrease the learning rate in cycles. This helps escape local minimia during optimization and achieve better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-civilization",
   "metadata": {},
   "source": [
    "## Hyperparameter Search <a class=\"anchor\" id=\"hyperparameter-search\"></a>\n",
    "\n",
    "The hyperparameter search is conducted for the combination of two hyperpaparameters - the learning rate and the L2 regularization strength. To do that, [hyperopt](#https://github.com/hyperopt/hyperopt), a Bayesian, distributed hyperparameter optimization library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "radio-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x_train, y_train, x_val, y_val, **kwargs):\n",
    "    \n",
    "    reg_rate_l2 = kwargs[\"reg_rate_l2\"]\n",
    "    lr_initial = kwargs[\"lr_initial\"]\n",
    "\n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = np.unique(y_train).size\n",
    "    mid_dim = 50\n",
    "\n",
    "    seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "\n",
    "    # as defined from before\n",
    "    n_epochs = 48\n",
    "    batch_size = 100\n",
    "\n",
    "    # lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    \n",
    "    # as defined from before\n",
    "    step_size = 800\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule, grad_clipper=GradClipperByNothing())\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    print(model)\n",
    "\n",
    "    # verbosity level of fit\n",
    "    verbose = 1\n",
    "    history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size, verbose, aug_func=None)\n",
    "\n",
    "    params_val = {\"mode\": \"test\"}\n",
    "    scores_val = model.forward(x_val, **params_val)\n",
    "    y_hat_val = np.argmax(scores_val, axis=1)\n",
    "    metrics_val = model.compute_metrics(y_val, scores_val)\n",
    "    print(f\"val acc: {metrics_val}, with lr_initial={lr_initial}, reg_rate_l2={reg_rate_l2}\")\n",
    "    val_acc = metrics_val['accuracy']\n",
    "\n",
    "    return {\n",
    "        'loss': -val_acc,\n",
    "        'status': STATUS_OK,\n",
    "        'eval_time': time.time(),\n",
    "        'val_acc': val_acc,\n",
    "        'reg_rate_l2': reg_rate_l2,\n",
    "        'lr_initial': lr_initial\n",
    "    }\n",
    "\n",
    "\n",
    "def run_trials(x_train, y_train, x_val, y_val, pickle_saved_path):\n",
    "    # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    trials_step = 1\n",
    "    # initial max_trials. put something small to not have to wait\n",
    "    max_trials = 2\n",
    "\n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(pickle_saved_path, \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    objective_lambda = lambda kwargs: objective(x_train, y_train, x_val, y_val, **kwargs)\n",
    "\n",
    "    reg_rate_l2_limits = (1e-3, 0.5)\n",
    "    lr_initial_limits = (1e-6, 0.9*1e-1)\n",
    "\n",
    "    space = {\n",
    "        \"reg_rate_l2\": hp.uniform(\"reg_rate_l2\", reg_rate_l2_limits[0], reg_rate_l2_limits[1]),\n",
    "        \"lr_initial\": hp.uniform(\"lr_initial\", lr_initial_limits[0], lr_initial_limits[1]),\n",
    "    }\n",
    "\n",
    "    # max_evals = 2\n",
    "\n",
    "    best = fmin(objective_lambda,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_trials,\n",
    "                trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "\n",
    "    # save the trials object\n",
    "    with open(pickle_saved_path, \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improved-melbourne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 11 trials to 12 (+1) trials\n",
      "model summary:                                         \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 5.1415e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 5.1415e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n",
      "starting epoch: 1 ...                                  \n",
      "epoch 1/48                                             \n",
      " \t -- train loss = 1.680189514126956 / train cost = 2.6805394971837275\n",
      "\t -- {\"accuracy_train\": 0.4039} \n",
      "\t -- val loss = 1.7986653261528263 / val cost = 2.7990153092095977\n",
      "\t -- {\"accuracy_val\": 0.3632} \n",
      "\n",
      "\n",
      "starting epoch: 2 ...                                  \n",
      "epoch 2/48                                             \n",
      " \t -- train loss = 1.608683949225004 / train cost = 2.262382637812858\n",
      "\t -- {\"accuracy_train\": 0.4349} \n",
      "\t -- val loss = 1.7495041007742842 / val cost = 2.4032027893621386\n",
      "\t -- {\"accuracy_val\": 0.3865} \n",
      "\n",
      "\n",
      "starting epoch: 3 ...                                  \n",
      "epoch 3/48                                             \n",
      " \t -- train loss = 1.531292734623224 / train cost = 1.9692914758095696\n",
      "\t -- {\"accuracy_train\": 0.4665} \n",
      "\t -- val loss = 1.6850711830781828 / val cost = 2.1230699242645286\n",
      "\t -- {\"accuracy_val\": 0.4031} \n",
      "\n",
      "\n",
      "starting epoch: 4 ...                                  \n",
      "epoch 4/48                                             \n",
      " \t -- train loss = 1.5226761962729758 / train cost = 1.8403435498183376\n",
      "\t -- {\"accuracy_train\": 0.4738} \n",
      "\t -- val loss = 1.6803573606756195 / val cost = 1.9980247142209813\n",
      "\t -- {\"accuracy_val\": 0.4093} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...                                  \n",
      "epoch 5/48                                             \n",
      " \t -- train loss = 1.5137146194750586 / train cost = 1.771256124199461\n",
      "\t -- {\"accuracy_train\": 0.476} \n",
      "\t -- val loss = 1.6763134399141622 / val cost = 1.9338549446385649\n",
      "\t -- {\"accuracy_val\": 0.4107} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...                                  \n",
      "epoch 6/48                                             \n",
      " \t -- train loss = 1.6020044812941887 / train cost = 1.8289506688407302\n",
      "\t -- {\"accuracy_train\": 0.4287} \n",
      "\t -- val loss = 1.7610064525115872 / val cost = 1.9879526400581287\n",
      "\t -- {\"accuracy_val\": 0.3694} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...                                  \n",
      "epoch 7/48                                             \n",
      " \t -- train loss = 1.5437346075595342 / train cost = 1.7603950392124292\n",
      "\t -- {\"accuracy_train\": 0.4656} \n",
      "\t -- val loss = 1.7022325086622123 / val cost = 1.9188929403151076\n",
      "\t -- {\"accuracy_val\": 0.3985} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...                                  \n",
      "epoch 8/48                                             \n",
      " \t -- train loss = 1.532485262786707 / train cost = 1.745801029158032\n",
      "\t -- {\"accuracy_train\": 0.4485} \n",
      "\t -- val loss = 1.6907144499647755 / val cost = 1.9040302163361005\n",
      "\t -- {\"accuracy_val\": 0.3908} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...                                  \n",
      "epoch 9/48                                             \n",
      " \t -- train loss = 1.5036147247311034 / train cost = 1.7178039469232815\n",
      "\t -- {\"accuracy_train\": 0.4709} \n",
      "\t -- val loss = 1.6685590795385603 / val cost = 1.8827483017307385\n",
      "\t -- {\"accuracy_val\": 0.4069} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...                                 \n",
      "epoch 10/48                                            \n",
      " \t -- train loss = 1.4983978599753967 / train cost = 1.717428324997473\n",
      "\t -- {\"accuracy_train\": 0.4661} \n",
      "\t -- val loss = 1.6702940032869344 / val cost = 1.889324468309011\n",
      "\t -- {\"accuracy_val\": 0.4049} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...                                 \n",
      "epoch 11/48                                            \n",
      " \t -- train loss = 1.4636731793662845 / train cost = 1.6878155412451328\n",
      "\t -- {\"accuracy_train\": 0.4969} \n",
      "\t -- val loss = 1.632630545210337 / val cost = 1.8567729070891852\n",
      "\t -- {\"accuracy_val\": 0.4281} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...                                 \n",
      "epoch 12/48                                            \n",
      " \t -- train loss = 1.4664270487832265 / train cost = 1.6965100007267961\n",
      "\t -- {\"accuracy_train\": 0.4864} \n",
      "\t -- val loss = 1.648745623325331 / val cost = 1.8788285752689007\n",
      "\t -- {\"accuracy_val\": 0.4156} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...                                 \n",
      "epoch 13/48                                            \n",
      " \t -- train loss = 1.4311421136690827 / train cost = 1.66552574490755\n",
      "\t -- {\"accuracy_train\": 0.5105} \n",
      "\t -- val loss = 1.620927454081481 / val cost = 1.8553110853199484\n",
      "\t -- {\"accuracy_val\": 0.4245} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...                                 \n",
      "epoch 14/48                                            \n",
      " \t -- train loss = 1.4088528594028757 / train cost = 1.6477341933437215\n",
      "\t -- {\"accuracy_train\": 0.5164} \n",
      "\t -- val loss = 1.5998886210002166 / val cost = 1.8387699549410623\n",
      "\t -- {\"accuracy_val\": 0.4281} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...                                 \n",
      "epoch 15/48                                            \n",
      " \t -- train loss = 1.3881588550274468 / train cost = 1.633214624300802\n",
      "\t -- {\"accuracy_train\": 0.5273} \n",
      "\t -- val loss = 1.585969435683759 / val cost = 1.8310252049571143\n",
      "\t -- {\"accuracy_val\": 0.4463} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...                                 \n",
      "epoch 16/48                                            \n",
      " \t -- train loss = 1.384082327308297 / train cost = 1.632078717834065\n",
      "\t -- {\"accuracy_train\": 0.5298} \n",
      "\t -- val loss = 1.5960370211547488 / val cost = 1.8440334116805168\n",
      "\t -- {\"accuracy_val\": 0.4333} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...                                 \n",
      "epoch 17/48                                            \n",
      " \t -- train loss = 1.378567646804852 / train cost = 1.6318161549964723\n",
      "\t -- {\"accuracy_train\": 0.5337} \n",
      "\t -- val loss = 1.5790181124725506 / val cost = 1.832266620664171\n",
      "\t -- {\"accuracy_val\": 0.4472} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...                                 \n",
      "epoch 18/48                                            \n",
      " \t -- train loss = 1.3986675552114287 / train cost = 1.6545706765901604\n",
      "\t -- {\"accuracy_train\": 0.5035} \n",
      "\t -- val loss = 1.6052426919806144 / val cost = 1.8611458133593461\n",
      "\t -- {\"accuracy_val\": 0.4258} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...                                 \n",
      "epoch 19/48                                            \n",
      " \t -- train loss = 1.4150674835973351 / train cost = 1.6713182201977426\n",
      "\t -- {\"accuracy_train\": 0.5062} \n",
      "\t -- val loss = 1.6291528454469952 / val cost = 1.8854035820474029\n",
      "\t -- {\"accuracy_val\": 0.4205} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...                                 \n",
      "epoch 20/48                                            \n",
      " \t -- train loss = 1.4255550891057027 / train cost = 1.6814400899068367\n",
      "\t -- {\"accuracy_train\": 0.5121} \n",
      "\t -- val loss = 1.6226293326299108 / val cost = 1.8785143334310448\n",
      "\t -- {\"accuracy_val\": 0.4335} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...                                 \n",
      "epoch 21/48                                            \n",
      " \t -- train loss = 1.483607354844297 / train cost = 1.735670486785579\n",
      "\t -- {\"accuracy_train\": 0.4791} \n",
      "\t -- val loss = 1.6772509197035772 / val cost = 1.9293140516448593\n",
      "\t -- {\"accuracy_val\": 0.4083} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...                                 \n",
      "epoch 22/48                                            \n",
      " \t -- train loss = 1.4971764765741467 / train cost = 1.746946980207745\n",
      "\t -- {\"accuracy_train\": 0.4859} \n",
      "\t -- val loss = 1.7010508788284588 / val cost = 1.9508213824620573\n",
      "\t -- {\"accuracy_val\": 0.4059} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...                                 \n",
      "epoch 23/48                                            \n",
      " \t -- train loss = 1.4946916357761961 / train cost = 1.7415696878699352\n",
      "\t -- {\"accuracy_train\": 0.4722} \n",
      "\t -- val loss = 1.683018856684531 / val cost = 1.92989690877827\n",
      "\t -- {\"accuracy_val\": 0.4019} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...                                 \n",
      "epoch 24/48                                            \n",
      " \t -- train loss = 1.4854428855498258 / train cost = 1.7250308980062286\n",
      "\t -- {\"accuracy_train\": 0.482} \n",
      "\t -- val loss = 1.6548705212640769 / val cost = 1.8944585337204796\n",
      "\t -- {\"accuracy_val\": 0.4228} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 25 ...                                 \n",
      "epoch 25/48                                            \n",
      " \t -- train loss = 1.4539919305583837 / train cost = 1.6928386100438888\n",
      "\t -- {\"accuracy_train\": 0.5041} \n",
      "\t -- val loss = 1.6305234045492158 / val cost = 1.869370084034721\n",
      "\t -- {\"accuracy_val\": 0.4293} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...                                 \n",
      "epoch 26/48                                            \n",
      " \t -- train loss = 1.4496823499510354 / train cost = 1.6889360555495994\n",
      "\t -- {\"accuracy_train\": 0.4886} \n",
      "\t -- val loss = 1.6413547212523507 / val cost = 1.8806084268509147\n",
      "\t -- {\"accuracy_val\": 0.4207} \n",
      "\n",
      "\n",
      "starting epoch: 27 ...                                 \n",
      "epoch 27/48                                            \n",
      " \t -- train loss = 1.4591521340065423 / train cost = 1.7042909804796949\n",
      "\t -- {\"accuracy_train\": 0.4914} \n",
      "\t -- val loss = 1.6378894068376544 / val cost = 1.883028253310807\n",
      "\t -- {\"accuracy_val\": 0.4206} \n",
      "\n",
      "\n",
      "starting epoch: 28 ...                                 \n",
      "epoch 28/48                                            \n",
      " \t -- train loss = 1.4000001381432268 / train cost = 1.6469800230215388\n",
      "\t -- {\"accuracy_train\": 0.5156} \n",
      "\t -- val loss = 1.5910732471271534 / val cost = 1.8380531320054654\n",
      "\t -- {\"accuracy_val\": 0.443} \n",
      "\n",
      "\n",
      "starting epoch: 29 ...                                 \n",
      "epoch 29/48                                            \n",
      " \t -- train loss = 1.4213087630990056 / train cost = 1.6700722556034773\n",
      "\t -- {\"accuracy_train\": 0.5138} \n",
      "\t -- val loss = 1.6116068314056384 / val cost = 1.8603703239101104\n",
      "\t -- {\"accuracy_val\": 0.4307} \n",
      "\n",
      "\n",
      "starting epoch: 30 ...                                 \n",
      "epoch 30/48                                            \n",
      " \t -- train loss = 1.3945879442212528 / train cost = 1.648745216939132\n",
      "\t -- {\"accuracy_train\": 0.5181} \n",
      "\t -- val loss = 1.601457711480284 / val cost = 1.855614984198163\n",
      "\t -- {\"accuracy_val\": 0.4332} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...                                 \n",
      "epoch 31/48                                            \n",
      " \t -- train loss = 1.3784954092765664 / train cost = 1.6380905324202368\n",
      "\t -- {\"accuracy_train\": 0.521} \n",
      "\t -- val loss = 1.596193141634951 / val cost = 1.8557882647786215\n",
      "\t -- {\"accuracy_val\": 0.44} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...                                 \n",
      "epoch 32/48                                            \n",
      " \t -- train loss = 1.3411094372334456 / train cost = 1.6049984423866253\n",
      "\t -- {\"accuracy_train\": 0.549} \n",
      "\t -- val loss = 1.558179292217833 / val cost = 1.822068297371013\n",
      "\t -- {\"accuracy_val\": 0.4483} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...                                 \n",
      "epoch 33/48                                            \n",
      " \t -- train loss = 1.3647753092022104 / train cost = 1.6300629829968338\n",
      "\t -- {\"accuracy_train\": 0.5357} \n",
      "\t -- val loss = 1.5777831156661604 / val cost = 1.8430707894607838\n",
      "\t -- {\"accuracy_val\": 0.4396} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...                                 \n",
      "epoch 34/48                                            \n",
      " \t -- train loss = 1.385149036718845 / train cost = 1.6508138146326392\n",
      "\t -- {\"accuracy_train\": 0.5299} \n",
      "\t -- val loss = 1.601456430864087 / val cost = 1.8671212087778812\n",
      "\t -- {\"accuracy_val\": 0.4357} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...                                 \n",
      "epoch 35/48                                            \n",
      " \t -- train loss = 1.4032676684112062 / train cost = 1.6683034158781613\n",
      "\t -- {\"accuracy_train\": 0.513} \n",
      "\t -- val loss = 1.616683179964673 / val cost = 1.8817189274316282\n",
      "\t -- {\"accuracy_val\": 0.4276} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...                                 \n",
      "epoch 36/48                                            \n",
      " \t -- train loss = 1.387165920943524 / train cost = 1.6504555599888657\n",
      "\t -- {\"accuracy_train\": 0.522} \n",
      "\t -- val loss = 1.6044768621934016 / val cost = 1.8677665012387432\n",
      "\t -- {\"accuracy_val\": 0.4333} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...                                 \n",
      "epoch 37/48                                            \n",
      " \t -- train loss = 1.3906522988371854 / train cost = 1.6496205800769643\n",
      "\t -- {\"accuracy_train\": 0.523} \n",
      "\t -- val loss = 1.5957867913841193 / val cost = 1.8547550726238982\n",
      "\t -- {\"accuracy_val\": 0.4382} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...                                 \n",
      "epoch 38/48                                            \n",
      " \t -- train loss = 1.4460809319241972 / train cost = 1.7046814615749863\n",
      "\t -- {\"accuracy_train\": 0.494} \n",
      "\t -- val loss = 1.649468176546051 / val cost = 1.9080687061968402\n",
      "\t -- {\"accuracy_val\": 0.4139} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...                                 \n",
      "epoch 39/48                                            \n",
      " \t -- train loss = 1.4488597623319266 / train cost = 1.6992245433892268\n",
      "\t -- {\"accuracy_train\": 0.4931} \n",
      "\t -- val loss = 1.6359635275789768 / val cost = 1.886328308636277\n",
      "\t -- {\"accuracy_val\": 0.4144} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...                                 \n",
      "epoch 40/48                                            \n",
      " \t -- train loss = 1.4628056551225987 / train cost = 1.7076208265500075\n",
      "\t -- {\"accuracy_train\": 0.4894} \n",
      "\t -- val loss = 1.6414402687406866 / val cost = 1.8862554401680953\n",
      "\t -- {\"accuracy_val\": 0.4221} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...                                 \n",
      "epoch 41/48                                            \n",
      " \t -- train loss = 1.4570888885037911 / train cost = 1.6983495670785627\n",
      "\t -- {\"accuracy_train\": 0.4832} \n",
      "\t -- val loss = 1.6444490923206678 / val cost = 1.8857097708954393\n",
      "\t -- {\"accuracy_val\": 0.4115} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...                                 \n",
      "epoch 42/48                                            \n",
      " \t -- train loss = 1.478722223939604 / train cost = 1.7263433266381907\n",
      "\t -- {\"accuracy_train\": 0.4714} \n",
      "\t -- val loss = 1.6597248087454155 / val cost = 1.9073459114440021\n",
      "\t -- {\"accuracy_val\": 0.409} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...                                 \n",
      "epoch 43/48                                            \n",
      " \t -- train loss = 1.4767294151238604 / train cost = 1.7258702728663953\n",
      "\t -- {\"accuracy_train\": 0.4779} \n",
      "\t -- val loss = 1.667484613517394 / val cost = 1.916625471259929\n",
      "\t -- {\"accuracy_val\": 0.4082} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...                                 \n",
      "epoch 44/48                                            \n",
      " \t -- train loss = 1.3909731731444963 / train cost = 1.6413992662284995\n",
      "\t -- {\"accuracy_train\": 0.5229} \n",
      "\t -- val loss = 1.5800334936135945 / val cost = 1.8304595866975977\n",
      "\t -- {\"accuracy_val\": 0.4465} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...                                 \n",
      "epoch 45/48                                            \n",
      " \t -- train loss = 1.3884041840616592 / train cost = 1.6424325937759479\n",
      "\t -- {\"accuracy_train\": 0.5246} \n",
      "\t -- val loss = 1.592777054166517 / val cost = 1.8468054638808056\n",
      "\t -- {\"accuracy_val\": 0.4378} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...                                 \n",
      "epoch 46/48                                            \n",
      " \t -- train loss = 1.36951236514476 / train cost = 1.6281282461627065\n",
      "\t -- {\"accuracy_train\": 0.5352} \n",
      "\t -- val loss = 1.5752866307865665 / val cost = 1.8339025118045131\n",
      "\t -- {\"accuracy_val\": 0.4508} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...                                 \n",
      "epoch 47/48                                            \n",
      " \t -- train loss = 1.354982543405927 / train cost = 1.6182273903769282\n",
      "\t -- {\"accuracy_train\": 0.5314} \n",
      "\t -- val loss = 1.568164051246793 / val cost = 1.831408898217794\n",
      "\t -- {\"accuracy_val\": 0.4443} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...                                 \n",
      "epoch 48/48                                            \n",
      " \t -- train loss = 1.333020649699461 / train cost = 1.5990416063861639\n",
      "\t -- {\"accuracy_train\": 0.5569} \n",
      "\t -- val loss = 1.545841994686479 / val cost = 1.8118629513731819\n",
      "\t -- {\"accuracy_val\": 0.4615} \n",
      "\n",
      "\n",
      "val acc: {'accuracy': 0.4615}, with lr_initial=0.041070795239371384, reg_rate_l2=0.05141531425601553\n",
      "100%|██████████| 12/12 [00:56<00:00, 56.31s/trial, best loss: -0.4615]\n",
      "Best: {'lr_initial': 0.041070795239371384, 'reg_rate_l2': 0.05141531425601553}\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 12 trials to 13 (+1) trials\n",
      "model summary:                                         \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 6.3809e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 6.3809e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 1 ...                                  \n",
      "epoch 1/48                                             \n",
      " \t -- train loss = 1.6953316927375817 / train cost = 2.5263561891617528\n",
      "\t -- {\"accuracy_train\": 0.3884} \n",
      "\t -- val loss = 1.7988343540508471 / val cost = 2.6298588504750184\n",
      "\t -- {\"accuracy_val\": 0.3518} \n",
      "\n",
      "\n",
      "starting epoch: 2 ...                                  \n",
      "epoch 2/48                                             \n",
      " \t -- train loss = 1.6405869289773725 / train cost = 2.0777734679072903\n",
      "\t -- {\"accuracy_train\": 0.4224} \n",
      "\t -- val loss = 1.7684636499241653 / val cost = 2.205650188854083\n",
      "\t -- {\"accuracy_val\": 0.3789} \n",
      "\n",
      "\n",
      "starting epoch: 3 ...                                  \n",
      "epoch 3/48                                             \n",
      " \t -- train loss = 1.5692274848718963 / train cost = 1.854134667135959\n",
      "\t -- {\"accuracy_train\": 0.4498} \n",
      "\t -- val loss = 1.7035185597659528 / val cost = 1.9884257420300155\n",
      "\t -- {\"accuracy_val\": 0.393} \n",
      "\n",
      "\n",
      "starting epoch: 4 ...                                  \n",
      "epoch 4/48                                             \n",
      " \t -- train loss = 1.563066134508075 / train cost = 1.7925052967290236\n",
      "\t -- {\"accuracy_train\": 0.4567} \n",
      "\t -- val loss = 1.6947407837473547 / val cost = 1.9241799459683033\n",
      "\t -- {\"accuracy_val\": 0.402} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...                                  \n",
      "epoch 5/48                                             \n",
      " \t -- train loss = 1.5568982949437178 / train cost = 1.7728942988582066\n",
      "\t -- {\"accuracy_train\": 0.458} \n",
      "\t -- val loss = 1.6930781433909032 / val cost = 1.9090741473053923\n",
      "\t -- {\"accuracy_val\": 0.398} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...                                  \n",
      "epoch 6/48                                             \n",
      " \t -- train loss = 1.6298516258751468 / train cost = 1.8428558149381749\n",
      "\t -- {\"accuracy_train\": 0.4186} \n",
      "\t -- val loss = 1.7632649595214744 / val cost = 1.9762691485845025\n",
      "\t -- {\"accuracy_val\": 0.364} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...                                  \n",
      "epoch 7/48                                             \n",
      " \t -- train loss = 1.5820762835756388 / train cost = 1.7964933510107621\n",
      "\t -- {\"accuracy_train\": 0.4487} \n",
      "\t -- val loss = 1.712826782670735 / val cost = 1.9272438501058584\n",
      "\t -- {\"accuracy_val\": 0.3914} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...                                  \n",
      "epoch 8/48                                             \n",
      " \t -- train loss = 1.5721501252355545 / train cost = 1.7895390249071639\n",
      "\t -- {\"accuracy_train\": 0.4325} \n",
      "\t -- val loss = 1.7088624072179104 / val cost = 1.9262513068895197\n",
      "\t -- {\"accuracy_val\": 0.3784} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...                                  \n",
      "epoch 9/48                                             \n",
      " \t -- train loss = 1.5421716521313793 / train cost = 1.7592711281375522\n",
      "\t -- {\"accuracy_train\": 0.4577} \n",
      "\t -- val loss = 1.683173429564493 / val cost = 1.9002729055706657\n",
      "\t -- {\"accuracy_val\": 0.4025} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...                                 \n",
      "epoch 10/48                                            \n",
      " \t -- train loss = 1.555375314875049 / train cost = 1.7760649973677696\n",
      "\t -- {\"accuracy_train\": 0.445} \n",
      "\t -- val loss = 1.6956383578570389 / val cost = 1.9163280403497596\n",
      "\t -- {\"accuracy_val\": 0.3914} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...                                 \n",
      "epoch 11/48                                            \n",
      " \t -- train loss = 1.5216747300206879 / train cost = 1.7466788600676924\n",
      "\t -- {\"accuracy_train\": 0.4672} \n",
      "\t -- val loss = 1.659995812262391 / val cost = 1.8849999423093955\n",
      "\t -- {\"accuracy_val\": 0.4072} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...                                 \n",
      "epoch 12/48                                            \n",
      " \t -- train loss = 1.5508151478214323 / train cost = 1.7796812164093605\n",
      "\t -- {\"accuracy_train\": 0.456} \n",
      "\t -- val loss = 1.700989011009795 / val cost = 1.9298550795977232\n",
      "\t -- {\"accuracy_val\": 0.3958} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...                                 \n",
      "epoch 13/48                                            \n",
      " \t -- train loss = 1.5083899207533125 / train cost = 1.7380765302632042\n",
      "\t -- {\"accuracy_train\": 0.4725} \n",
      "\t -- val loss = 1.658621211336083 / val cost = 1.8883078208459747\n",
      "\t -- {\"accuracy_val\": 0.4099} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...                                 \n",
      "epoch 14/48                                            \n",
      " \t -- train loss = 1.4894753806741945 / train cost = 1.72272649392058\n",
      "\t -- {\"accuracy_train\": 0.4837} \n",
      "\t -- val loss = 1.6369610643426689 / val cost = 1.8702121775890543\n",
      "\t -- {\"accuracy_val\": 0.4151} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...                                 \n",
      "epoch 15/48                                            \n",
      " \t -- train loss = 1.4834372966265652 / train cost = 1.7203859673548365\n",
      "\t -- {\"accuracy_train\": 0.4932} \n",
      "\t -- val loss = 1.6334255532447464 / val cost = 1.870374223973018\n",
      "\t -- {\"accuracy_val\": 0.4215} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...                                 \n",
      "epoch 16/48                                            \n",
      " \t -- train loss = 1.5620697120309956 / train cost = 1.7989393516787258\n",
      "\t -- {\"accuracy_train\": 0.4531} \n",
      "\t -- val loss = 1.7248485046079114 / val cost = 1.9617181442556415\n",
      "\t -- {\"accuracy_val\": 0.3943} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...                                 \n",
      "epoch 17/48                                            \n",
      " \t -- train loss = 1.4747253540241858 / train cost = 1.71661554272554\n",
      "\t -- {\"accuracy_train\": 0.4992} \n",
      "\t -- val loss = 1.6222721882603466 / val cost = 1.8641623769617008\n",
      "\t -- {\"accuracy_val\": 0.4298} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...                                 \n",
      "epoch 18/48                                            \n",
      " \t -- train loss = 1.4899008584369804 / train cost = 1.7320431221429904\n",
      "\t -- {\"accuracy_train\": 0.466} \n",
      "\t -- val loss = 1.6411364212489854 / val cost = 1.8832786849549954\n",
      "\t -- {\"accuracy_val\": 0.4091} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...                                 \n",
      "epoch 19/48                                            \n",
      " \t -- train loss = 1.5093015241861742 / train cost = 1.75130022821956\n",
      "\t -- {\"accuracy_train\": 0.4699} \n",
      "\t -- val loss = 1.669815135083185 / val cost = 1.9118138391165707\n",
      "\t -- {\"accuracy_val\": 0.4054} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...                                 \n",
      "epoch 20/48                                            \n",
      " \t -- train loss = 1.5050994537048834 / train cost = 1.7476693105144878\n",
      "\t -- {\"accuracy_train\": 0.4797} \n",
      "\t -- val loss = 1.6553106143673517 / val cost = 1.8978804711769561\n",
      "\t -- {\"accuracy_val\": 0.4195} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...                                 \n",
      "epoch 21/48                                            \n",
      " \t -- train loss = 1.554959347940657 / train cost = 1.7942591912402475\n",
      "\t -- {\"accuracy_train\": 0.4447} \n",
      "\t -- val loss = 1.6988708625278148 / val cost = 1.9381707058274054\n",
      "\t -- {\"accuracy_val\": 0.3864} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...                                 \n",
      "epoch 22/48                                            \n",
      " \t -- train loss = 1.540124738952762 / train cost = 1.7796844915244645\n",
      "\t -- {\"accuracy_train\": 0.466} \n",
      "\t -- val loss = 1.7081470444710234 / val cost = 1.947706797042726\n",
      "\t -- {\"accuracy_val\": 0.3984} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...                                 \n",
      "epoch 23/48                                            \n",
      " \t -- train loss = 1.5474963685902632 / train cost = 1.7859784147276425\n",
      "\t -- {\"accuracy_train\": 0.4518} \n",
      "\t -- val loss = 1.6996908509035293 / val cost = 1.9381728970409087\n",
      "\t -- {\"accuracy_val\": 0.3949} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...                                 \n",
      "epoch 24/48                                            \n",
      " \t -- train loss = 1.5331686935181223 / train cost = 1.7662336334571351\n",
      "\t -- {\"accuracy_train\": 0.4677} \n",
      "\t -- val loss = 1.6677267066214891 / val cost = 1.900791646560502\n",
      "\t -- {\"accuracy_val\": 0.4146} \n",
      "\n",
      "\n",
      "starting epoch: 25 ...                                 \n",
      "epoch 25/48                                            \n",
      " \t -- train loss = 1.5006286790566652 / train cost = 1.73327686375929\n",
      "\t -- {\"accuracy_train\": 0.4851} \n",
      "\t -- val loss = 1.64287922910321 / val cost = 1.8755274138058349\n",
      "\t -- {\"accuracy_val\": 0.4244} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...                                 \n",
      "epoch 26/48                                            \n",
      " \t -- train loss = 1.5103630102221661 / train cost = 1.742184864466033\n",
      "\t -- {\"accuracy_train\": 0.4713} \n",
      "\t -- val loss = 1.6676265366477676 / val cost = 1.8994483908916342\n",
      "\t -- {\"accuracy_val\": 0.4105} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 27 ...                                 \n",
      "epoch 27/48                                            \n",
      " \t -- train loss = 1.5247123859494944 / train cost = 1.7637116255167606\n",
      "\t -- {\"accuracy_train\": 0.4618} \n",
      "\t -- val loss = 1.6647092630246014 / val cost = 1.9037085025918676\n",
      "\t -- {\"accuracy_val\": 0.4078} \n",
      "\n",
      "\n",
      "starting epoch: 28 ...                                 \n",
      "epoch 28/48                                            \n",
      " \t -- train loss = 1.4881864582860926 / train cost = 1.7259601050975348\n",
      "\t -- {\"accuracy_train\": 0.4807} \n",
      "\t -- val loss = 1.6372771826839412 / val cost = 1.8750508294953834\n",
      "\t -- {\"accuracy_val\": 0.417} \n",
      "\n",
      "\n",
      "starting epoch: 29 ...                                 \n",
      "epoch 29/48                                            \n",
      " \t -- train loss = 1.5234825372679233 / train cost = 1.7595590198809798\n",
      "\t -- {\"accuracy_train\": 0.4743} \n",
      "\t -- val loss = 1.6693908380609472 / val cost = 1.9054673206740038\n",
      "\t -- {\"accuracy_val\": 0.406} \n",
      "\n",
      "\n",
      "starting epoch: 30 ...                                 \n",
      "epoch 30/48                                            \n",
      " \t -- train loss = 1.4936091543585082 / train cost = 1.734765457514129\n",
      "\t -- {\"accuracy_train\": 0.4762} \n",
      "\t -- val loss = 1.6538889704327462 / val cost = 1.895045273588367\n",
      "\t -- {\"accuracy_val\": 0.4136} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...                                 \n",
      "epoch 31/48                                            \n",
      " \t -- train loss = 1.4928135907110187 / train cost = 1.7385916119740423\n",
      "\t -- {\"accuracy_train\": 0.4803} \n",
      "\t -- val loss = 1.6605368587574565 / val cost = 1.90631488002048\n",
      "\t -- {\"accuracy_val\": 0.4159} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...                                 \n",
      "epoch 32/48                                            \n",
      " \t -- train loss = 1.4530632662946237 / train cost = 1.7013184334634814\n",
      "\t -- {\"accuracy_train\": 0.4927} \n",
      "\t -- val loss = 1.6142290756030027 / val cost = 1.8624842427718604\n",
      "\t -- {\"accuracy_val\": 0.4227} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...                                 \n",
      "epoch 33/48                                            \n",
      " \t -- train loss = 1.4778465439590944 / train cost = 1.7239057083675802\n",
      "\t -- {\"accuracy_train\": 0.4883} \n",
      "\t -- val loss = 1.6318314801185334 / val cost = 1.8778906445270191\n",
      "\t -- {\"accuracy_val\": 0.4168} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...                                 \n",
      "epoch 34/48                                            \n",
      " \t -- train loss = 1.5009265800559226 / train cost = 1.7461611385227869\n",
      "\t -- {\"accuracy_train\": 0.4825} \n",
      "\t -- val loss = 1.660417381703918 / val cost = 1.9056519401707823\n",
      "\t -- {\"accuracy_val\": 0.4133} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...                                 \n",
      "epoch 35/48                                            \n",
      " \t -- train loss = 1.498112572098344 / train cost = 1.7432788015709466\n",
      "\t -- {\"accuracy_train\": 0.4777} \n",
      "\t -- val loss = 1.657498629362552 / val cost = 1.9026648588351547\n",
      "\t -- {\"accuracy_val\": 0.4146} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...                                 \n",
      "epoch 36/48                                            \n",
      " \t -- train loss = 1.4670571579918825 / train cost = 1.7121375266744139\n",
      "\t -- {\"accuracy_train\": 0.4964} \n",
      "\t -- val loss = 1.633054547440751 / val cost = 1.8781349161232823\n",
      "\t -- {\"accuracy_val\": 0.4184} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...                                 \n",
      "epoch 37/48                                            \n",
      " \t -- train loss = 1.480135098291227 / train cost = 1.7219692592139633\n",
      "\t -- {\"accuracy_train\": 0.4819} \n",
      "\t -- val loss = 1.6363520972593821 / val cost = 1.8781862581821185\n",
      "\t -- {\"accuracy_val\": 0.4225} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...                                 \n",
      "epoch 38/48                                            \n",
      " \t -- train loss = 1.5305457391709922 / train cost = 1.773941094134295\n",
      "\t -- {\"accuracy_train\": 0.4538} \n",
      "\t -- val loss = 1.6869853309481595 / val cost = 1.9303806859114623\n",
      "\t -- {\"accuracy_val\": 0.3909} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...                                 \n",
      "epoch 39/48                                            \n",
      " \t -- train loss = 1.5136516065313943 / train cost = 1.7516942933746642\n",
      "\t -- {\"accuracy_train\": 0.4712} \n",
      "\t -- val loss = 1.6624439502234032 / val cost = 1.900486637066673\n",
      "\t -- {\"accuracy_val\": 0.4014} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...                                 \n",
      "epoch 40/48                                            \n",
      " \t -- train loss = 1.522842454012027 / train cost = 1.7566535696751595\n",
      "\t -- {\"accuracy_train\": 0.4729} \n",
      "\t -- val loss = 1.666796120811381 / val cost = 1.9006072364745137\n",
      "\t -- {\"accuracy_val\": 0.4119} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...                                 \n",
      "epoch 41/48                                            \n",
      " \t -- train loss = 1.5073575012536162 / train cost = 1.7402961680799394\n",
      "\t -- {\"accuracy_train\": 0.47} \n",
      "\t -- val loss = 1.6558673568302484 / val cost = 1.8888060236565716\n",
      "\t -- {\"accuracy_val\": 0.4109} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...                                 \n",
      "epoch 42/48                                            \n",
      " \t -- train loss = 1.5459860138409105 / train cost = 1.7855446107427533\n",
      "\t -- {\"accuracy_train\": 0.4413} \n",
      "\t -- val loss = 1.6944379371539282 / val cost = 1.933996534055771\n",
      "\t -- {\"accuracy_val\": 0.3955} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...                                 \n",
      "epoch 43/48                                            \n",
      " \t -- train loss = 1.5458640444093548 / train cost = 1.7854852181261884\n",
      "\t -- {\"accuracy_train\": 0.4521} \n",
      "\t -- val loss = 1.697214776899448 / val cost = 1.9368359506162818\n",
      "\t -- {\"accuracy_val\": 0.3945} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...                                 \n",
      "epoch 44/48                                            \n",
      " \t -- train loss = 1.4846239138303812 / train cost = 1.723573334168343\n",
      "\t -- {\"accuracy_train\": 0.4843} \n",
      "\t -- val loss = 1.6305783295075313 / val cost = 1.8695277498454932\n",
      "\t -- {\"accuracy_val\": 0.4216} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...                                 \n",
      "epoch 45/48                                            \n",
      " \t -- train loss = 1.488683152349463 / train cost = 1.7296381614837146\n",
      "\t -- {\"accuracy_train\": 0.4868} \n",
      "\t -- val loss = 1.644368247664696 / val cost = 1.8853232567989475\n",
      "\t -- {\"accuracy_val\": 0.4228} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...                                 \n",
      "epoch 46/48                                            \n",
      " \t -- train loss = 1.487933112371969 / train cost = 1.7324905400454143\n",
      "\t -- {\"accuracy_train\": 0.4838} \n",
      "\t -- val loss = 1.6418052440353146 / val cost = 1.8863626717087598\n",
      "\t -- {\"accuracy_val\": 0.4234} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...                                 \n",
      "epoch 47/48                                            \n",
      " \t -- train loss = 1.4671134808735684 / train cost = 1.713519265191843\n",
      "\t -- {\"accuracy_train\": 0.4827} \n",
      "\t -- val loss = 1.624237362644446 / val cost = 1.8706431469627205\n",
      "\t -- {\"accuracy_val\": 0.4203} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...                                 \n",
      "epoch 48/48                                            \n",
      " \t -- train loss = 1.446291734773654 / train cost = 1.6922229558560988\n",
      "\t -- {\"accuracy_train\": 0.5103} \n",
      "\t -- val loss = 1.6001863436775656 / val cost = 1.8461175647600105\n",
      "\t -- {\"accuracy_val\": 0.439} \n",
      "\n",
      "\n",
      "val acc: {'accuracy': 0.439}, with lr_initial=0.06842479935255724, reg_rate_l2=0.06380905695939129\n",
      "100%|██████████| 13/13 [00:56<00:00, 56.24s/trial, best loss: -0.4615]\n",
      "Best: {'lr_initial': 0.041070795239371384, 'reg_rate_l2': 0.05141531425601553}\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 13 trials to 14 (+1) trials\n",
      "model summary:                                         \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 2.1287e-01\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 2.1287e-01\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n",
      "starting epoch: 1 ...                                  \n",
      "epoch 1/48                                             \n",
      " \t -- train loss = 1.768662226647773 / train cost = 3.848366682241735\n",
      "\t -- {\"accuracy_train\": 0.3869} \n",
      "\t -- val loss = 1.8340780602362197 / val cost = 3.9137825158301816\n",
      "\t -- {\"accuracy_val\": 0.3526} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 2 ...                                  \n",
      "epoch 2/48                                             \n",
      " \t -- train loss = 1.7606813692909444 / train cost = 2.3676499026070825\n",
      "\t -- {\"accuracy_train\": 0.3886} \n",
      "\t -- val loss = 1.8174328242017324 / val cost = 2.424401357517871\n",
      "\t -- {\"accuracy_val\": 0.3597} \n",
      "\n",
      "\n",
      "starting epoch: 3 ...                                  \n",
      "epoch 3/48                                             \n",
      " \t -- train loss = 1.765558846471324 / train cost = 2.0392257327636765\n",
      "\t -- {\"accuracy_train\": 0.383} \n",
      "\t -- val loss = 1.817411893645311 / val cost = 2.0910787799376633\n",
      "\t -- {\"accuracy_val\": 0.3593} \n",
      "\n",
      "\n",
      "starting epoch: 4 ...                                  \n",
      "epoch 4/48                                             \n",
      " \t -- train loss = 1.7856496255084724 / train cost = 2.0026890323770177\n",
      "\t -- {\"accuracy_train\": 0.3708} \n",
      "\t -- val loss = 1.8373505825194294 / val cost = 2.0543899893879747\n",
      "\t -- {\"accuracy_val\": 0.3438} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...                                  \n",
      "epoch 5/48                                             \n",
      " \t -- train loss = 1.7800322230829502 / train cost = 1.9880115410716528\n",
      "\t -- {\"accuracy_train\": 0.3684} \n",
      "\t -- val loss = 1.8308116237984813 / val cost = 2.038790941787184\n",
      "\t -- {\"accuracy_val\": 0.3483} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...                                  \n",
      "epoch 6/48                                             \n",
      " \t -- train loss = 1.8205354591302527 / train cost = 2.027944624778816\n",
      "\t -- {\"accuracy_train\": 0.3387} \n",
      "\t -- val loss = 1.8668847417630066 / val cost = 2.07429390741157\n",
      "\t -- {\"accuracy_val\": 0.3257} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...                                  \n",
      "epoch 7/48                                             \n",
      " \t -- train loss = 1.8222023277876052 / train cost = 2.020827639248642\n",
      "\t -- {\"accuracy_train\": 0.3502} \n",
      "\t -- val loss = 1.8627997233125833 / val cost = 2.0614250347736203\n",
      "\t -- {\"accuracy_val\": 0.3361} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...                                  \n",
      "epoch 8/48                                             \n",
      " \t -- train loss = 1.8191472403063482 / train cost = 2.0193451699450193\n",
      "\t -- {\"accuracy_train\": 0.3334} \n",
      "\t -- val loss = 1.862085604464545 / val cost = 2.062283534103216\n",
      "\t -- {\"accuracy_val\": 0.3152} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...                                  \n",
      "epoch 9/48                                             \n",
      " \t -- train loss = 1.7951097431327991 / train cost = 1.9897614470610085\n",
      "\t -- {\"accuracy_train\": 0.3663} \n",
      "\t -- val loss = 1.8453124418441453 / val cost = 2.0399641457723545\n",
      "\t -- {\"accuracy_val\": 0.3439} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...                                 \n",
      "epoch 10/48                                            \n",
      " \t -- train loss = 1.790749534136698 / train cost = 1.987170186013738\n",
      "\t -- {\"accuracy_train\": 0.3726} \n",
      "\t -- val loss = 1.8361598764089249 / val cost = 2.032580528285965\n",
      "\t -- {\"accuracy_val\": 0.3444} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...                                 \n",
      "epoch 11/48                                            \n",
      " \t -- train loss = 1.7941364081689701 / train cost = 1.992166998477927\n",
      "\t -- {\"accuracy_train\": 0.3622} \n",
      "\t -- val loss = 1.836163367965478 / val cost = 2.034193958274435\n",
      "\t -- {\"accuracy_val\": 0.3429} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...                                 \n",
      "epoch 12/48                                            \n",
      " \t -- train loss = 1.7746106844268146 / train cost = 1.9795257182160921\n",
      "\t -- {\"accuracy_train\": 0.3702} \n",
      "\t -- val loss = 1.8258732760613121 / val cost = 2.0307883098505894\n",
      "\t -- {\"accuracy_val\": 0.346} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...                                 \n",
      "epoch 13/48                                            \n",
      " \t -- train loss = 1.7712974474130763 / train cost = 1.97293263508052\n",
      "\t -- {\"accuracy_train\": 0.3715} \n",
      "\t -- val loss = 1.818357952542639 / val cost = 2.0199931402100826\n",
      "\t -- {\"accuracy_val\": 0.3489} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...                                 \n",
      "epoch 14/48                                            \n",
      " \t -- train loss = 1.7610139302921002 / train cost = 1.964587789362871\n",
      "\t -- {\"accuracy_train\": 0.3817} \n",
      "\t -- val loss = 1.8075520153638684 / val cost = 2.011125874434639\n",
      "\t -- {\"accuracy_val\": 0.3512} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...                                 \n",
      "epoch 15/48                                            \n",
      " \t -- train loss = 1.7496760838867735 / train cost = 1.9530108922783653\n",
      "\t -- {\"accuracy_train\": 0.3958} \n",
      "\t -- val loss = 1.7967618997671995 / val cost = 2.0000967081587913\n",
      "\t -- {\"accuracy_val\": 0.3711} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...                                 \n",
      "epoch 16/48                                            \n",
      " \t -- train loss = 1.7409829654412685 / train cost = 1.9482161705961218\n",
      "\t -- {\"accuracy_train\": 0.3957} \n",
      "\t -- val loss = 1.794236406941731 / val cost = 2.0014696120965842\n",
      "\t -- {\"accuracy_val\": 0.3666} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...                                 \n",
      "epoch 17/48                                            \n",
      " \t -- train loss = 1.7413598681242344 / train cost = 1.9529471715647944\n",
      "\t -- {\"accuracy_train\": 0.397} \n",
      "\t -- val loss = 1.7888317767838722 / val cost = 2.0004190802244324\n",
      "\t -- {\"accuracy_val\": 0.38} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...                                 \n",
      "epoch 18/48                                            \n",
      " \t -- train loss = 1.7510773671073945 / train cost = 1.961431105759334\n",
      "\t -- {\"accuracy_train\": 0.3833} \n",
      "\t -- val loss = 1.800394161188992 / val cost = 2.010747899840932\n",
      "\t -- {\"accuracy_val\": 0.3682} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...                                 \n",
      "epoch 19/48                                            \n",
      " \t -- train loss = 1.7654271051839336 / train cost = 1.9743624739982084\n",
      "\t -- {\"accuracy_train\": 0.3828} \n",
      "\t -- val loss = 1.8152936520272094 / val cost = 2.024229020841484\n",
      "\t -- {\"accuracy_val\": 0.3548} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...                                 \n",
      "epoch 20/48                                            \n",
      " \t -- train loss = 1.7560728343455547 / train cost = 1.9666894517695432\n",
      "\t -- {\"accuracy_train\": 0.3838} \n",
      "\t -- val loss = 1.8058761765794527 / val cost = 2.016492794003441\n",
      "\t -- {\"accuracy_val\": 0.3669} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...                                 \n",
      "epoch 21/48                                            \n",
      " \t -- train loss = 1.789804934908598 / train cost = 1.9903459139857216\n",
      "\t -- {\"accuracy_train\": 0.3647} \n",
      "\t -- val loss = 1.8365873630494847 / val cost = 2.037128342126608\n",
      "\t -- {\"accuracy_val\": 0.3375} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...                                 \n",
      "epoch 22/48                                            \n",
      " \t -- train loss = 1.8117375400319984 / train cost = 2.017471466532557\n",
      "\t -- {\"accuracy_train\": 0.3408} \n",
      "\t -- val loss = 1.8722602846927119 / val cost = 2.0779942111932703\n",
      "\t -- {\"accuracy_val\": 0.321} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...                                 \n",
      "epoch 23/48                                            \n",
      " \t -- train loss = 1.7872712549203664 / train cost = 1.9942532238906172\n",
      "\t -- {\"accuracy_train\": 0.363} \n",
      "\t -- val loss = 1.8364708310383049 / val cost = 2.0434528000085557\n",
      "\t -- {\"accuracy_val\": 0.3398} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...                                 \n",
      "epoch 24/48                                            \n",
      " \t -- train loss = 1.8140877538987683 / train cost = 2.0061110279821945\n",
      "\t -- {\"accuracy_train\": 0.3629} \n",
      "\t -- val loss = 1.852954180559174 / val cost = 2.0449774546426003\n",
      "\t -- {\"accuracy_val\": 0.3501} \n",
      "\n",
      "\n",
      "starting epoch: 25 ...                                 \n",
      "epoch 25/48                                            \n",
      " \t -- train loss = 1.792639793603508 / train cost = 1.986262139103329\n",
      "\t -- {\"accuracy_train\": 0.3702} \n",
      "\t -- val loss = 1.8335778513804821 / val cost = 2.0272001968803033\n",
      "\t -- {\"accuracy_val\": 0.3468} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...                                 \n",
      "epoch 26/48                                            \n",
      " \t -- train loss = 1.791294583113734 / train cost = 1.984303143661377\n",
      "\t -- {\"accuracy_train\": 0.357} \n",
      "\t -- val loss = 1.8433136232453442 / val cost = 2.0363221837929872\n",
      "\t -- {\"accuracy_val\": 0.3376} \n",
      "\n",
      "\n",
      "starting epoch: 27 ...                                 \n",
      "epoch 27/48                                            \n",
      " \t -- train loss = 1.7818180891977529 / train cost = 1.9860536608723167\n",
      "\t -- {\"accuracy_train\": 0.3725} \n",
      "\t -- val loss = 1.8250470035893842 / val cost = 2.029282575263948\n",
      "\t -- {\"accuracy_val\": 0.3466} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 28 ...                                 \n",
      "epoch 28/48                                            \n",
      " \t -- train loss = 1.767681525788295 / train cost = 1.968575437619955\n",
      "\t -- {\"accuracy_train\": 0.3794} \n",
      "\t -- val loss = 1.813910155837834 / val cost = 2.014804067669494\n",
      "\t -- {\"accuracy_val\": 0.3559} \n",
      "\n",
      "\n",
      "starting epoch: 29 ...                                 \n",
      "epoch 29/48                                            \n",
      " \t -- train loss = 1.7729149811461287 / train cost = 1.9687942467538158\n",
      "\t -- {\"accuracy_train\": 0.3838} \n",
      "\t -- val loss = 1.8188507401016978 / val cost = 2.014730005709385\n",
      "\t -- {\"accuracy_val\": 0.3591} \n",
      "\n",
      "\n",
      "starting epoch: 30 ...                                 \n",
      "epoch 30/48                                            \n",
      " \t -- train loss = 1.752174637298828 / train cost = 1.9570090377181115\n",
      "\t -- {\"accuracy_train\": 0.3904} \n",
      "\t -- val loss = 1.8048672527967538 / val cost = 2.0097016532160374\n",
      "\t -- {\"accuracy_val\": 0.3569} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...                                 \n",
      "epoch 31/48                                            \n",
      " \t -- train loss = 1.745016663299411 / train cost = 1.9533897351235312\n",
      "\t -- {\"accuracy_train\": 0.3838} \n",
      "\t -- val loss = 1.7985902937343956 / val cost = 2.0069633655585157\n",
      "\t -- {\"accuracy_val\": 0.3633} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...                                 \n",
      "epoch 32/48                                            \n",
      " \t -- train loss = 1.7371919705693486 / train cost = 1.9449496344464061\n",
      "\t -- {\"accuracy_train\": 0.3976} \n",
      "\t -- val loss = 1.7898289492536628 / val cost = 1.9975866131307203\n",
      "\t -- {\"accuracy_val\": 0.371} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...                                 \n",
      "epoch 33/48                                            \n",
      " \t -- train loss = 1.74810537264181 / train cost = 1.955114028843178\n",
      "\t -- {\"accuracy_train\": 0.3926} \n",
      "\t -- val loss = 1.8009509323091344 / val cost = 2.007959588510502\n",
      "\t -- {\"accuracy_val\": 0.3641} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...                                 \n",
      "epoch 34/48                                            \n",
      " \t -- train loss = 1.7651054585177306 / train cost = 1.9737577048483115\n",
      "\t -- {\"accuracy_train\": 0.3859} \n",
      "\t -- val loss = 1.8151169967648348 / val cost = 2.0237692430954155\n",
      "\t -- {\"accuracy_val\": 0.3594} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...                                 \n",
      "epoch 35/48                                            \n",
      " \t -- train loss = 1.758168350012031 / train cost = 1.9597721577706078\n",
      "\t -- {\"accuracy_train\": 0.3837} \n",
      "\t -- val loss = 1.807688145724362 / val cost = 2.009291953482939\n",
      "\t -- {\"accuracy_val\": 0.3569} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...                                 \n",
      "epoch 36/48                                            \n",
      " \t -- train loss = 1.760901920800686 / train cost = 1.9674674334173554\n",
      "\t -- {\"accuracy_train\": 0.374} \n",
      "\t -- val loss = 1.8153599246018828 / val cost = 2.021925437218552\n",
      "\t -- {\"accuracy_val\": 0.3522} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...                                 \n",
      "epoch 37/48                                            \n",
      " \t -- train loss = 1.776224212043398 / train cost = 1.9724327901619798\n",
      "\t -- {\"accuracy_train\": 0.3715} \n",
      "\t -- val loss = 1.825070846797413 / val cost = 2.021279424915995\n",
      "\t -- {\"accuracy_val\": 0.351} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...                                 \n",
      "epoch 38/48                                            \n",
      " \t -- train loss = 1.7922007864194318 / train cost = 1.996316095733009\n",
      "\t -- {\"accuracy_train\": 0.3388} \n",
      "\t -- val loss = 1.8430606868700585 / val cost = 2.0471759961836358\n",
      "\t -- {\"accuracy_val\": 0.3212} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...                                 \n",
      "epoch 39/48                                            \n",
      " \t -- train loss = 1.8132171402023927 / train cost = 2.0072520724114686\n",
      "\t -- {\"accuracy_train\": 0.3541} \n",
      "\t -- val loss = 1.8607602425436192 / val cost = 2.054795174752695\n",
      "\t -- {\"accuracy_val\": 0.3243} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...                                 \n",
      "epoch 40/48                                            \n",
      " \t -- train loss = 1.8046495371470126 / train cost = 1.9954452296314333\n",
      "\t -- {\"accuracy_train\": 0.3498} \n",
      "\t -- val loss = 1.847191891956461 / val cost = 2.0379875844408817\n",
      "\t -- {\"accuracy_val\": 0.3253} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...                                 \n",
      "epoch 41/48                                            \n",
      " \t -- train loss = 1.8015487669134163 / train cost = 1.9925801276992556\n",
      "\t -- {\"accuracy_train\": 0.3617} \n",
      "\t -- val loss = 1.8505737360076335 / val cost = 2.041605096793473\n",
      "\t -- {\"accuracy_val\": 0.3388} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...                                 \n",
      "epoch 42/48                                            \n",
      " \t -- train loss = 1.8157982456304824 / train cost = 2.0118729651927447\n",
      "\t -- {\"accuracy_train\": 0.3372} \n",
      "\t -- val loss = 1.8553804910989566 / val cost = 2.051455210661219\n",
      "\t -- {\"accuracy_val\": 0.3255} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...                                 \n",
      "epoch 43/48                                            \n",
      " \t -- train loss = 1.7889261241085372 / train cost = 1.9877833475282258\n",
      "\t -- {\"accuracy_train\": 0.353} \n",
      "\t -- val loss = 1.8371170918524815 / val cost = 2.03597431527217\n",
      "\t -- {\"accuracy_val\": 0.3289} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...                                 \n",
      "epoch 44/48                                            \n",
      " \t -- train loss = 1.7647943747345078 / train cost = 1.9660485798416971\n",
      "\t -- {\"accuracy_train\": 0.3841} \n",
      "\t -- val loss = 1.8085990970054346 / val cost = 2.009853302112624\n",
      "\t -- {\"accuracy_val\": 0.3641} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...                                 \n",
      "epoch 45/48                                            \n",
      " \t -- train loss = 1.7588773052620472 / train cost = 1.957463107977028\n",
      "\t -- {\"accuracy_train\": 0.3846} \n",
      "\t -- val loss = 1.8070930256080286 / val cost = 2.0056788283230094\n",
      "\t -- {\"accuracy_val\": 0.3685} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...                                 \n",
      "epoch 46/48                                            \n",
      " \t -- train loss = 1.753574168236078 / train cost = 1.9540356132419463\n",
      "\t -- {\"accuracy_train\": 0.3921} \n",
      "\t -- val loss = 1.8039174690673394 / val cost = 2.004378914073208\n",
      "\t -- {\"accuracy_val\": 0.3716} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...                                 \n",
      "epoch 47/48                                            \n",
      " \t -- train loss = 1.7443433950934908 / train cost = 1.9498192968986374\n",
      "\t -- {\"accuracy_train\": 0.3847} \n",
      "\t -- val loss = 1.7937290957785106 / val cost = 1.9992049975836572\n",
      "\t -- {\"accuracy_val\": 0.3639} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...                                 \n",
      "epoch 48/48                                            \n",
      " \t -- train loss = 1.7421246352152135 / train cost = 1.9459922507961933\n",
      "\t -- {\"accuracy_train\": 0.3883} \n",
      "\t -- val loss = 1.7948147704386597 / val cost = 1.9986823860196394\n",
      "\t -- {\"accuracy_val\": 0.367} \n",
      "\n",
      "\n",
      "val acc: {'accuracy': 0.367}, with lr_initial=0.022872345264861062, reg_rate_l2=0.21287170399119315\n",
      "100%|██████████| 14/14 [00:55<00:00, 55.81s/trial, best loss: -0.4615]\n",
      "Best: {'lr_initial': 0.041070795239371384, 'reg_rate_l2': 0.05141531425601553}\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 14 trials to 15 (+1) trials\n",
      "model summary:                                         \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 4.0888e-01\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 4.0888e-01\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n",
      "starting epoch: 1 ...                                  \n",
      "epoch 1/48                                             \n",
      " \t -- train loss = 1.940661278376931 / train cost = 2.1737172664328166\n",
      "\t -- {\"accuracy_train\": 0.3099} \n",
      "\t -- val loss = 1.9659626977955489 / val cost = 2.1990186858514345\n",
      "\t -- {\"accuracy_val\": 0.2947} \n",
      "\n",
      "\n",
      "starting epoch: 2 ...                                  \n",
      "epoch 2/48                                             \n",
      " \t -- train loss = 1.926021516915369 / train cost = 2.1285778145838963\n",
      "\t -- {\"accuracy_train\": 0.3086} \n",
      "\t -- val loss = 1.9540178385347873 / val cost = 2.1565741362033144\n",
      "\t -- {\"accuracy_val\": 0.2926} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 3 ...                                  \n",
      "epoch 3/48                                             \n",
      " \t -- train loss = 1.9270785429587334 / train cost = 2.115442529052549\n",
      "\t -- {\"accuracy_train\": 0.316} \n",
      "\t -- val loss = 1.9526654501625988 / val cost = 2.1410294362564146\n",
      "\t -- {\"accuracy_val\": 0.3019} \n",
      "\n",
      "\n",
      "starting epoch: 4 ...                                  \n",
      "epoch 4/48                                             \n",
      " \t -- train loss = 1.9483727538390376 / train cost = 2.1299898278068006\n",
      "\t -- {\"accuracy_train\": 0.2773} \n",
      "\t -- val loss = 1.9751954344788498 / val cost = 2.1568125084466128\n",
      "\t -- {\"accuracy_val\": 0.2671} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...                                  \n",
      "epoch 5/48                                             \n",
      " \t -- train loss = 1.9407416888019837 / train cost = 2.118790105365286\n",
      "\t -- {\"accuracy_train\": 0.2941} \n",
      "\t -- val loss = 1.9666393809715017 / val cost = 2.1446877975348038\n",
      "\t -- {\"accuracy_val\": 0.2822} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...                                  \n",
      "epoch 6/48                                             \n",
      " \t -- train loss = 1.9513482058445593 / train cost = 2.14136189873506\n",
      "\t -- {\"accuracy_train\": 0.2796} \n",
      "\t -- val loss = 1.9756072885359226 / val cost = 2.1656209814264233\n",
      "\t -- {\"accuracy_val\": 0.2686} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...                                  \n",
      "epoch 7/48                                             \n",
      " \t -- train loss = 1.9576554110717501 / train cost = 2.1318253337818347\n",
      "\t -- {\"accuracy_train\": 0.2872} \n",
      "\t -- val loss = 1.977533139735151 / val cost = 2.1517030624452356\n",
      "\t -- {\"accuracy_val\": 0.2799} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...                                  \n",
      "epoch 8/48                                             \n",
      " \t -- train loss = 1.9473182485764167 / train cost = 2.125506853262281\n",
      "\t -- {\"accuracy_train\": 0.2801} \n",
      "\t -- val loss = 1.9699244357749721 / val cost = 2.1481130404608364\n",
      "\t -- {\"accuracy_val\": 0.2767} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...                                  \n",
      "epoch 9/48                                             \n",
      " \t -- train loss = 1.9297861413018373 / train cost = 2.105124954908027\n",
      "\t -- {\"accuracy_train\": 0.3033} \n",
      "\t -- val loss = 1.957475821217948 / val cost = 2.1328146348241375\n",
      "\t -- {\"accuracy_val\": 0.2959} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...                                 \n",
      "epoch 10/48                                            \n",
      " \t -- train loss = 1.9267695812008911 / train cost = 2.1020671087105214\n",
      "\t -- {\"accuracy_train\": 0.3054} \n",
      "\t -- val loss = 1.951027695126435 / val cost = 2.126325222636065\n",
      "\t -- {\"accuracy_val\": 0.2909} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...                                 \n",
      "epoch 11/48                                            \n",
      " \t -- train loss = 1.9433222736329931 / train cost = 2.116492879412777\n",
      "\t -- {\"accuracy_train\": 0.2906} \n",
      "\t -- val loss = 1.9625194889439328 / val cost = 2.1356900947237163\n",
      "\t -- {\"accuracy_val\": 0.2807} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...                                 \n",
      "epoch 12/48                                            \n",
      " \t -- train loss = 1.9387968791606767 / train cost = 2.116887747071014\n",
      "\t -- {\"accuracy_train\": 0.2966} \n",
      "\t -- val loss = 1.9677919396809185 / val cost = 2.145882807591256\n",
      "\t -- {\"accuracy_val\": 0.2785} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...                                 \n",
      "epoch 13/48                                            \n",
      " \t -- train loss = 1.9368791720124274 / train cost = 2.109936176883882\n",
      "\t -- {\"accuracy_train\": 0.3115} \n",
      "\t -- val loss = 1.9576702265365515 / val cost = 2.130727231408006\n",
      "\t -- {\"accuracy_val\": 0.2992} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...                                 \n",
      "epoch 14/48                                            \n",
      " \t -- train loss = 1.923702944440809 / train cost = 2.097385401776736\n",
      "\t -- {\"accuracy_train\": 0.3076} \n",
      "\t -- val loss = 1.9455869789957505 / val cost = 2.1192694363316775\n",
      "\t -- {\"accuracy_val\": 0.2961} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...                                 \n",
      "epoch 15/48                                            \n",
      " \t -- train loss = 1.9285128017569193 / train cost = 2.092079083573636\n",
      "\t -- {\"accuracy_train\": 0.325} \n",
      "\t -- val loss = 1.9467951156053627 / val cost = 2.1103613974220794\n",
      "\t -- {\"accuracy_val\": 0.3088} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...                                 \n",
      "epoch 16/48                                            \n",
      " \t -- train loss = 1.9283096910100594 / train cost = 2.0935911317042617\n",
      "\t -- {\"accuracy_train\": 0.3228} \n",
      "\t -- val loss = 1.9537301299263619 / val cost = 2.1190115706205646\n",
      "\t -- {\"accuracy_val\": 0.3083} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...                                 \n",
      "epoch 17/48                                            \n",
      " \t -- train loss = 1.920670654633293 / train cost = 2.094740163549114\n",
      "\t -- {\"accuracy_train\": 0.3224} \n",
      "\t -- val loss = 1.9396988875538208 / val cost = 2.113768396469642\n",
      "\t -- {\"accuracy_val\": 0.3172} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...                                 \n",
      "epoch 18/48                                            \n",
      " \t -- train loss = 1.9301468797423462 / train cost = 2.1005136158096795\n",
      "\t -- {\"accuracy_train\": 0.3193} \n",
      "\t -- val loss = 1.9493941612222796 / val cost = 2.119760897289613\n",
      "\t -- {\"accuracy_val\": 0.3128} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...                                 \n",
      "epoch 19/48                                            \n",
      " \t -- train loss = 1.94233975745658 / train cost = 2.115684463603174\n",
      "\t -- {\"accuracy_train\": 0.3016} \n",
      "\t -- val loss = 1.9669903169699996 / val cost = 2.1403350231165934\n",
      "\t -- {\"accuracy_val\": 0.2867} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...                                 \n",
      "epoch 20/48                                            \n",
      " \t -- train loss = 1.926105689253082 / train cost = 2.100181900112668\n",
      "\t -- {\"accuracy_train\": 0.3095} \n",
      "\t -- val loss = 1.9496571183097817 / val cost = 2.123733329169368\n",
      "\t -- {\"accuracy_val\": 0.2972} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...                                 \n",
      "epoch 21/48                                            \n",
      " \t -- train loss = 1.9512081062923288 / train cost = 2.118126135229231\n",
      "\t -- {\"accuracy_train\": 0.2905} \n",
      "\t -- val loss = 1.9763343270232638 / val cost = 2.143252355960166\n",
      "\t -- {\"accuracy_val\": 0.2759} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...                                 \n",
      "epoch 22/48                                            \n",
      " \t -- train loss = 1.9566941413007 / train cost = 2.136306667396077\n",
      "\t -- {\"accuracy_train\": 0.2782} \n",
      "\t -- val loss = 1.9915049264459355 / val cost = 2.1711174525413126\n",
      "\t -- {\"accuracy_val\": 0.2634} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...                                 \n",
      "epoch 23/48                                            \n",
      " \t -- train loss = 1.9172133565328953 / train cost = 2.102625562553663\n",
      "\t -- {\"accuracy_train\": 0.3097} \n",
      "\t -- val loss = 1.9422838188285 / val cost = 2.127696024849268\n",
      "\t -- {\"accuracy_val\": 0.2913} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...                                 \n",
      "epoch 24/48                                            \n",
      " \t -- train loss = 1.9408174134132852 / train cost = 2.1058087664961143\n",
      "\t -- {\"accuracy_train\": 0.3156} \n",
      "\t -- val loss = 1.9593812457093758 / val cost = 2.124372598792205\n",
      "\t -- {\"accuracy_val\": 0.3036} \n",
      "\n",
      "\n",
      "starting epoch: 25 ...                                 \n",
      "epoch 25/48                                            \n",
      " \t -- train loss = 1.930451242108113 / train cost = 2.096480388924465\n",
      "\t -- {\"accuracy_train\": 0.3029} \n",
      "\t -- val loss = 1.9510440481893974 / val cost = 2.1170731950057493\n",
      "\t -- {\"accuracy_val\": 0.2944} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...                                 \n",
      "epoch 26/48                                            \n",
      " \t -- train loss = 1.9336480064699482 / train cost = 2.1028505979152516\n",
      "\t -- {\"accuracy_train\": 0.2849} \n",
      "\t -- val loss = 1.9608157824717536 / val cost = 2.130018373917057\n",
      "\t -- {\"accuracy_val\": 0.2708} \n",
      "\n",
      "\n",
      "starting epoch: 27 ...                                 \n",
      "epoch 27/48                                            \n",
      " \t -- train loss = 1.926746391710118 / train cost = 2.1097110438322537\n",
      "\t -- {\"accuracy_train\": 0.314} \n",
      "\t -- val loss = 1.950882352221704 / val cost = 2.13384700434384\n",
      "\t -- {\"accuracy_val\": 0.2992} \n",
      "\n",
      "\n",
      "starting epoch: 28 ...                                 \n",
      "epoch 28/48                                            \n",
      " \t -- train loss = 1.9283543732980437 / train cost = 2.096374536383449\n",
      "\t -- {\"accuracy_train\": 0.3184} \n",
      "\t -- val loss = 1.9526359031609979 / val cost = 2.1206560662464033\n",
      "\t -- {\"accuracy_val\": 0.3023} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 29 ...                                 \n",
      "epoch 29/48                                            \n",
      " \t -- train loss = 1.9489657977908832 / train cost = 2.1077307668305494\n",
      "\t -- {\"accuracy_train\": 0.2826} \n",
      "\t -- val loss = 1.9687413512932521 / val cost = 2.1275063203329183\n",
      "\t -- {\"accuracy_val\": 0.2743} \n",
      "\n",
      "\n",
      "starting epoch: 30 ...                                 \n",
      "epoch 30/48                                            \n",
      " \t -- train loss = 1.9148865057931743 / train cost = 2.0927686142983792\n",
      "\t -- {\"accuracy_train\": 0.3179} \n",
      "\t -- val loss = 1.9434479579706614 / val cost = 2.1213300664758665\n",
      "\t -- {\"accuracy_val\": 0.296} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...                                 \n",
      "epoch 31/48                                            \n",
      " \t -- train loss = 1.9289491368617178 / train cost = 2.10307768775997\n",
      "\t -- {\"accuracy_train\": 0.3017} \n",
      "\t -- val loss = 1.9586568165049345 / val cost = 2.132785367403187\n",
      "\t -- {\"accuracy_val\": 0.2862} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...                                 \n",
      "epoch 32/48                                            \n",
      " \t -- train loss = 1.9160814404167121 / train cost = 2.0878558868698605\n",
      "\t -- {\"accuracy_train\": 0.3183} \n",
      "\t -- val loss = 1.9406025765978823 / val cost = 2.1123770230510304\n",
      "\t -- {\"accuracy_val\": 0.3027} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...                                 \n",
      "epoch 33/48                                            \n",
      " \t -- train loss = 1.9262231612144283 / train cost = 2.092045919768509\n",
      "\t -- {\"accuracy_train\": 0.3297} \n",
      "\t -- val loss = 1.9503128646038683 / val cost = 2.116135623157949\n",
      "\t -- {\"accuracy_val\": 0.311} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...                                 \n",
      "epoch 34/48                                            \n",
      " \t -- train loss = 1.9415344948862319 / train cost = 2.1187624709868986\n",
      "\t -- {\"accuracy_train\": 0.314} \n",
      "\t -- val loss = 1.962688160333802 / val cost = 2.139916136434469\n",
      "\t -- {\"accuracy_val\": 0.3056} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...                                 \n",
      "epoch 35/48                                            \n",
      " \t -- train loss = 1.926049916708545 / train cost = 2.0900809134472613\n",
      "\t -- {\"accuracy_train\": 0.3151} \n",
      "\t -- val loss = 1.949489142380641 / val cost = 2.113520139119357\n",
      "\t -- {\"accuracy_val\": 0.3} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...                                 \n",
      "epoch 36/48                                            \n",
      " \t -- train loss = 1.9234142921983057 / train cost = 2.096980268259677\n",
      "\t -- {\"accuracy_train\": 0.3149} \n",
      "\t -- val loss = 1.9515965107065887 / val cost = 2.12516248676796\n",
      "\t -- {\"accuracy_val\": 0.2981} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...                                 \n",
      "epoch 37/48                                            \n",
      " \t -- train loss = 1.9292983943712307 / train cost = 2.0983167245775505\n",
      "\t -- {\"accuracy_train\": 0.3148} \n",
      "\t -- val loss = 1.9529205443276791 / val cost = 2.121938874533999\n",
      "\t -- {\"accuracy_val\": 0.2995} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...                                 \n",
      "epoch 38/48                                            \n",
      " \t -- train loss = 1.9242462060354202 / train cost = 2.106645616948417\n",
      "\t -- {\"accuracy_train\": 0.278} \n",
      "\t -- val loss = 1.9516045910755788 / val cost = 2.134004001988576\n",
      "\t -- {\"accuracy_val\": 0.2668} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...                                 \n",
      "epoch 39/48                                            \n",
      " \t -- train loss = 1.9554922238595027 / train cost = 2.121220194745173\n",
      "\t -- {\"accuracy_train\": 0.2855} \n",
      "\t -- val loss = 1.9783583487419125 / val cost = 2.144086319627583\n",
      "\t -- {\"accuracy_val\": 0.2729} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...                                 \n",
      "epoch 40/48                                            \n",
      " \t -- train loss = 1.935874661026412 / train cost = 2.1036719874808383\n",
      "\t -- {\"accuracy_train\": 0.2981} \n",
      "\t -- val loss = 1.956233068094193 / val cost = 2.1240303945486194\n",
      "\t -- {\"accuracy_val\": 0.2798} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...                                 \n",
      "epoch 41/48                                            \n",
      " \t -- train loss = 1.939320111046515 / train cost = 2.111438938586298\n",
      "\t -- {\"accuracy_train\": 0.3165} \n",
      "\t -- val loss = 1.9670820648103844 / val cost = 2.1392008923501673\n",
      "\t -- {\"accuracy_val\": 0.299} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...                                 \n",
      "epoch 42/48                                            \n",
      " \t -- train loss = 1.9690617543975684 / train cost = 2.1327330357595877\n",
      "\t -- {\"accuracy_train\": 0.2661} \n",
      "\t -- val loss = 1.9846437478938943 / val cost = 2.1483150292559134\n",
      "\t -- {\"accuracy_val\": 0.2608} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...                                 \n",
      "epoch 43/48                                            \n",
      " \t -- train loss = 1.9441323488321482 / train cost = 2.114229739102398\n",
      "\t -- {\"accuracy_train\": 0.2999} \n",
      "\t -- val loss = 1.9669835930486654 / val cost = 2.1370809833189153\n",
      "\t -- {\"accuracy_val\": 0.2861} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...                                 \n",
      "epoch 44/48                                            \n",
      " \t -- train loss = 1.9223999127574602 / train cost = 2.0925776736723387\n",
      "\t -- {\"accuracy_train\": 0.3286} \n",
      "\t -- val loss = 1.9433747933949663 / val cost = 2.113552554309845\n",
      "\t -- {\"accuracy_val\": 0.3173} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...                                 \n",
      "epoch 45/48                                            \n",
      " \t -- train loss = 1.921326980089867 / train cost = 2.087995379326565\n",
      "\t -- {\"accuracy_train\": 0.3133} \n",
      "\t -- val loss = 1.9425257843818953 / val cost = 2.1091941836185932\n",
      "\t -- {\"accuracy_val\": 0.3058} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...                                 \n",
      "epoch 46/48                                            \n",
      " \t -- train loss = 1.934860379176633 / train cost = 2.0969108101071523\n",
      "\t -- {\"accuracy_train\": 0.3038} \n",
      "\t -- val loss = 1.9583079790804652 / val cost = 2.1203584100109847\n",
      "\t -- {\"accuracy_val\": 0.2841} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...                                 \n",
      "epoch 47/48                                            \n",
      " \t -- train loss = 1.9232633565032062 / train cost = 2.0965491802917384\n",
      "\t -- {\"accuracy_train\": 0.3063} \n",
      "\t -- val loss = 1.9464875391897412 / val cost = 2.1197733629782736\n",
      "\t -- {\"accuracy_val\": 0.297} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...                                 \n",
      "epoch 48/48                                            \n",
      " \t -- train loss = 1.9258441406508868 / train cost = 2.0867265964658084\n",
      "\t -- {\"accuracy_train\": 0.3171} \n",
      "\t -- val loss = 1.9487081272572166 / val cost = 2.1095905830721384\n",
      "\t -- {\"accuracy_val\": 0.3051} \n",
      "\n",
      "\n",
      "val acc: {'accuracy': 0.3051}, with lr_initial=0.0740982919356081, reg_rate_l2=0.4088758457004862\n",
      "100%|██████████| 15/15 [00:55<00:00, 55.71s/trial, best loss: -0.4615]\n",
      "Best: {'lr_initial': 0.041070795239371384, 'reg_rate_l2': 0.05141531425601553}\n",
      "Found saved Trials! Loading...\n",
      "Rerunning from 15 trials to 16 (+1) trials\n",
      "model summary:                                         \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 1.9888e-01\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 1.9888e-01\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n",
      "starting epoch: 1 ...                                  \n",
      "epoch 1/48                                             \n",
      " \t -- train loss = 1.7726749843873861 / train cost = 2.833860908519524\n",
      "\t -- {\"accuracy_train\": 0.3781} \n",
      "\t -- val loss = 1.8317988211815224 / val cost = 2.892984745313661\n",
      "\t -- {\"accuracy_val\": 0.346} \n",
      "\n",
      "\n",
      "starting epoch: 2 ...                                  \n",
      "epoch 2/48                                             \n",
      " \t -- train loss = 1.763256144500736 / train cost = 2.0866742700316214\n",
      "\t -- {\"accuracy_train\": 0.3803} \n",
      "\t -- val loss = 1.8183918525320812 / val cost = 2.141809978062967\n",
      "\t -- {\"accuracy_val\": 0.3555} \n",
      "\n",
      "\n",
      "starting epoch: 3 ...                                  \n",
      "epoch 3/48                                             \n",
      " \t -- train loss = 1.7636750187370476 / train cost = 1.9875841832269658\n",
      "\t -- {\"accuracy_train\": 0.3816} \n",
      "\t -- val loss = 1.8166027911340075 / val cost = 2.0405119556239257\n",
      "\t -- {\"accuracy_val\": 0.3552} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 4 ...                                  \n",
      "epoch 4/48                                             \n",
      " \t -- train loss = 1.7792295062221755 / train cost = 1.987645463000751\n",
      "\t -- {\"accuracy_train\": 0.3724} \n",
      "\t -- val loss = 1.832125393751047 / val cost = 2.0405413505296224\n",
      "\t -- {\"accuracy_val\": 0.344} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...                                  \n",
      "epoch 5/48                                             \n",
      " \t -- train loss = 1.7696808479644748 / train cost = 1.9753082379503293\n",
      "\t -- {\"accuracy_train\": 0.3759} \n",
      "\t -- val loss = 1.8216062855480224 / val cost = 2.027233675533877\n",
      "\t -- {\"accuracy_val\": 0.355} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...                                  \n",
      "epoch 6/48                                             \n",
      " \t -- train loss = 1.8111949219028662 / train cost = 2.0165329301654085\n",
      "\t -- {\"accuracy_train\": 0.3416} \n",
      "\t -- val loss = 1.860343675094044 / val cost = 2.0656816833565865\n",
      "\t -- {\"accuracy_val\": 0.3247} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...                                  \n",
      "epoch 7/48                                             \n",
      " \t -- train loss = 1.8115699328475456 / train cost = 2.0096106431168055\n",
      "\t -- {\"accuracy_train\": 0.3568} \n",
      "\t -- val loss = 1.8549491502680844 / val cost = 2.0529898605373442\n",
      "\t -- {\"accuracy_val\": 0.3421} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...                                  \n",
      "epoch 8/48                                             \n",
      " \t -- train loss = 1.8061698160123676 / train cost = 2.006507406971255\n",
      "\t -- {\"accuracy_train\": 0.339} \n",
      "\t -- val loss = 1.8515709127541549 / val cost = 2.0519085037130425\n",
      "\t -- {\"accuracy_val\": 0.3202} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...                                  \n",
      "epoch 9/48                                             \n",
      " \t -- train loss = 1.783305881103978 / train cost = 1.9779884961243654\n",
      "\t -- {\"accuracy_train\": 0.3713} \n",
      "\t -- val loss = 1.836464282437972 / val cost = 2.0311468974583597\n",
      "\t -- {\"accuracy_val\": 0.3467} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...                                 \n",
      "epoch 10/48                                            \n",
      " \t -- train loss = 1.7811948656751293 / train cost = 1.9771204948836008\n",
      "\t -- {\"accuracy_train\": 0.3725} \n",
      "\t -- val loss = 1.8301377176846922 / val cost = 2.0260633468931637\n",
      "\t -- {\"accuracy_val\": 0.3456} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...                                 \n",
      "epoch 11/48                                            \n",
      " \t -- train loss = 1.7844001036517798 / train cost = 1.982809783489951\n",
      "\t -- {\"accuracy_train\": 0.3644} \n",
      "\t -- val loss = 1.8284815909590042 / val cost = 2.0268912707971753\n",
      "\t -- {\"accuracy_val\": 0.3415} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...                                 \n",
      "epoch 12/48                                            \n",
      " \t -- train loss = 1.776480890645825 / train cost = 1.981154280191429\n",
      "\t -- {\"accuracy_train\": 0.3706} \n",
      "\t -- val loss = 1.8307099781552392 / val cost = 2.035383367700843\n",
      "\t -- {\"accuracy_val\": 0.3399} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...                                 \n",
      "epoch 13/48                                            \n",
      " \t -- train loss = 1.7690243477074845 / train cost = 1.9702851940097221\n",
      "\t -- {\"accuracy_train\": 0.3726} \n",
      "\t -- val loss = 1.8173779820315343 / val cost = 2.018638828333772\n",
      "\t -- {\"accuracy_val\": 0.3484} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...                                 \n",
      "epoch 14/48                                            \n",
      " \t -- train loss = 1.7569253925628705 / train cost = 1.959657457420115\n",
      "\t -- {\"accuracy_train\": 0.3787} \n",
      "\t -- val loss = 1.805442658815153 / val cost = 2.008174723672398\n",
      "\t -- {\"accuracy_val\": 0.3477} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...                                 \n",
      "epoch 15/48                                            \n",
      " \t -- train loss = 1.7464630876550506 / train cost = 1.9474641785849125\n",
      "\t -- {\"accuracy_train\": 0.4008} \n",
      "\t -- val loss = 1.793765877354148 / val cost = 1.99476696828401\n",
      "\t -- {\"accuracy_val\": 0.3732} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...                                 \n",
      "epoch 16/48                                            \n",
      " \t -- train loss = 1.7397151856563058 / train cost = 1.9434319389473371\n",
      "\t -- {\"accuracy_train\": 0.3934} \n",
      "\t -- val loss = 1.7949742516338345 / val cost = 1.9986910049248658\n",
      "\t -- {\"accuracy_val\": 0.3604} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...                                 \n",
      "epoch 17/48                                            \n",
      " \t -- train loss = 1.7394032151980707 / train cost = 1.9494486068044252\n",
      "\t -- {\"accuracy_train\": 0.3983} \n",
      "\t -- val loss = 1.7866072883700284 / val cost = 1.9966526799763828\n",
      "\t -- {\"accuracy_val\": 0.3806} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...                                 \n",
      "epoch 18/48                                            \n",
      " \t -- train loss = 1.7451757388666835 / train cost = 1.9518117677336306\n",
      "\t -- {\"accuracy_train\": 0.3868} \n",
      "\t -- val loss = 1.7949087376726778 / val cost = 2.001544766539625\n",
      "\t -- {\"accuracy_val\": 0.3711} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...                                 \n",
      "epoch 19/48                                            \n",
      " \t -- train loss = 1.765876221436761 / train cost = 1.972016769691832\n",
      "\t -- {\"accuracy_train\": 0.3798} \n",
      "\t -- val loss = 1.8181933474618752 / val cost = 2.0243338957169463\n",
      "\t -- {\"accuracy_val\": 0.3514} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...                                 \n",
      "epoch 20/48                                            \n",
      " \t -- train loss = 1.7482212787142721 / train cost = 1.9575410709918921\n",
      "\t -- {\"accuracy_train\": 0.3859} \n",
      "\t -- val loss = 1.800086773503539 / val cost = 2.009406565781159\n",
      "\t -- {\"accuracy_val\": 0.3656} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...                                 \n",
      "epoch 21/48                                            \n",
      " \t -- train loss = 1.7869257772196225 / train cost = 1.9872175165521808\n",
      "\t -- {\"accuracy_train\": 0.3615} \n",
      "\t -- val loss = 1.8370376809394797 / val cost = 2.037329420272038\n",
      "\t -- {\"accuracy_val\": 0.334} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...                                 \n",
      "epoch 22/48                                            \n",
      " \t -- train loss = 1.8030145325264344 / train cost = 2.008626264797749\n",
      "\t -- {\"accuracy_train\": 0.3457} \n",
      "\t -- val loss = 1.8674270364967087 / val cost = 2.0730387687680234\n",
      "\t -- {\"accuracy_val\": 0.3226} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...                                 \n",
      "epoch 23/48                                            \n",
      " \t -- train loss = 1.7776463236478532 / train cost = 1.9842916798092307\n",
      "\t -- {\"accuracy_train\": 0.3663} \n",
      "\t -- val loss = 1.8295136207988307 / val cost = 2.036158976960208\n",
      "\t -- {\"accuracy_val\": 0.3411} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...                                 \n",
      "epoch 24/48                                            \n",
      " \t -- train loss = 1.8006128596513398 / train cost = 1.9942209976220742\n",
      "\t -- {\"accuracy_train\": 0.3669} \n",
      "\t -- val loss = 1.8418353242294223 / val cost = 2.0354434622001567\n",
      "\t -- {\"accuracy_val\": 0.3509} \n",
      "\n",
      "\n",
      "starting epoch: 25 ...                                 \n",
      "epoch 25/48                                            \n",
      " \t -- train loss = 1.7774863922462318 / train cost = 1.9724151919043058\n",
      "\t -- {\"accuracy_train\": 0.3775} \n",
      "\t -- val loss = 1.8220314353868843 / val cost = 2.0169602350449582\n",
      "\t -- {\"accuracy_val\": 0.3538} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...                                 \n",
      "epoch 26/48                                            \n",
      " \t -- train loss = 1.7825522727282166 / train cost = 1.9754640029256407\n",
      "\t -- {\"accuracy_train\": 0.3609} \n",
      "\t -- val loss = 1.837385160590154 / val cost = 2.0302968907875782\n",
      "\t -- {\"accuracy_val\": 0.338} \n",
      "\n",
      "\n",
      "starting epoch: 27 ...                                 \n",
      "epoch 27/48                                            \n",
      " \t -- train loss = 1.77468354449613 / train cost = 1.9798351482635723\n",
      "\t -- {\"accuracy_train\": 0.3698} \n",
      "\t -- val loss = 1.8200729338175194 / val cost = 2.0252245375849616\n",
      "\t -- {\"accuracy_val\": 0.3461} \n",
      "\n",
      "\n",
      "starting epoch: 28 ...                                 \n",
      "epoch 28/48                                            \n",
      " \t -- train loss = 1.760663086345367 / train cost = 1.9627157281398204\n",
      "\t -- {\"accuracy_train\": 0.3843} \n",
      "\t -- val loss = 1.8088388829834225 / val cost = 2.0108915247778762\n",
      "\t -- {\"accuracy_val\": 0.3572} \n",
      "\n",
      "\n",
      "starting epoch: 29 ...                                 \n",
      "epoch 29/48                                            \n",
      " \t -- train loss = 1.7723306393654314 / train cost = 1.9674449651729413\n",
      "\t -- {\"accuracy_train\": 0.384} \n",
      "\t -- val loss = 1.8193812792765318 / val cost = 2.0144956050840417\n",
      "\t -- {\"accuracy_val\": 0.3588} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 30 ...                                 \n",
      "epoch 30/48                                            \n",
      " \t -- train loss = 1.7475630746013078 / train cost = 1.9525797096407718\n",
      "\t -- {\"accuracy_train\": 0.3838} \n",
      "\t -- val loss = 1.8038978791791938 / val cost = 2.0089145142186577\n",
      "\t -- {\"accuracy_val\": 0.356} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...                                 \n",
      "epoch 31/48                                            \n",
      " \t -- train loss = 1.7461464296048823 / train cost = 1.953594715688912\n",
      "\t -- {\"accuracy_train\": 0.3782} \n",
      "\t -- val loss = 1.8037941226584466 / val cost = 2.011242408742476\n",
      "\t -- {\"accuracy_val\": 0.3577} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...                                 \n",
      "epoch 32/48                                            \n",
      " \t -- train loss = 1.7305646873495273 / train cost = 1.9389564526495184\n",
      "\t -- {\"accuracy_train\": 0.3951} \n",
      "\t -- val loss = 1.785817660273929 / val cost = 1.9942094255739202\n",
      "\t -- {\"accuracy_val\": 0.3697} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...                                 \n",
      "epoch 33/48                                            \n",
      " \t -- train loss = 1.7442135028042762 / train cost = 1.9493567332212864\n",
      "\t -- {\"accuracy_train\": 0.394} \n",
      "\t -- val loss = 1.7991189707554418 / val cost = 2.0042622011724522\n",
      "\t -- {\"accuracy_val\": 0.3635} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...                                 \n",
      "epoch 34/48                                            \n",
      " \t -- train loss = 1.769407860828982 / train cost = 1.97672202677585\n",
      "\t -- {\"accuracy_train\": 0.3821} \n",
      "\t -- val loss = 1.8205939907486726 / val cost = 2.0279081566955406\n",
      "\t -- {\"accuracy_val\": 0.356} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...                                 \n",
      "epoch 35/48                                            \n",
      " \t -- train loss = 1.7550412951814014 / train cost = 1.9544091695177497\n",
      "\t -- {\"accuracy_train\": 0.3826} \n",
      "\t -- val loss = 1.8061679369464156 / val cost = 2.005535811282764\n",
      "\t -- {\"accuracy_val\": 0.3576} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...                                 \n",
      "epoch 36/48                                            \n",
      " \t -- train loss = 1.7527426761774196 / train cost = 1.9576083037587964\n",
      "\t -- {\"accuracy_train\": 0.3792} \n",
      "\t -- val loss = 1.810125467698217 / val cost = 2.0149910952795937\n",
      "\t -- {\"accuracy_val\": 0.3522} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...                                 \n",
      "epoch 37/48                                            \n",
      " \t -- train loss = 1.767684515840482 / train cost = 1.96297926265841\n",
      "\t -- {\"accuracy_train\": 0.372} \n",
      "\t -- val loss = 1.8187590982206803 / val cost = 2.0140538450386085\n",
      "\t -- {\"accuracy_val\": 0.3532} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...                                 \n",
      "epoch 38/48                                            \n",
      " \t -- train loss = 1.7874585298495742 / train cost = 1.9901638288841432\n",
      "\t -- {\"accuracy_train\": 0.337} \n",
      "\t -- val loss = 1.8406450017971108 / val cost = 2.04335030083168\n",
      "\t -- {\"accuracy_val\": 0.3211} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...                                 \n",
      "epoch 39/48                                            \n",
      " \t -- train loss = 1.8026982233758808 / train cost = 1.9975337450167403\n",
      "\t -- {\"accuracy_train\": 0.3563} \n",
      "\t -- val loss = 1.8530947328359477 / val cost = 2.047930254476807\n",
      "\t -- {\"accuracy_val\": 0.3238} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...                                 \n",
      "epoch 40/48                                            \n",
      " \t -- train loss = 1.7885378247878951 / train cost = 1.9804056360191458\n",
      "\t -- {\"accuracy_train\": 0.3567} \n",
      "\t -- val loss = 1.8343496479034846 / val cost = 2.0262174591347355\n",
      "\t -- {\"accuracy_val\": 0.3286} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...                                 \n",
      "epoch 41/48                                            \n",
      " \t -- train loss = 1.7900783345212539 / train cost = 1.9819886669912\n",
      "\t -- {\"accuracy_train\": 0.3645} \n",
      "\t -- val loss = 1.842382045070349 / val cost = 2.034292377540295\n",
      "\t -- {\"accuracy_val\": 0.3395} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...                                 \n",
      "epoch 42/48                                            \n",
      " \t -- train loss = 1.8088018553388854 / train cost = 2.0062155841720464\n",
      "\t -- {\"accuracy_train\": 0.3412} \n",
      "\t -- val loss = 1.8510160119502659 / val cost = 2.0484297407834267\n",
      "\t -- {\"accuracy_val\": 0.329} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...                                 \n",
      "epoch 43/48                                            \n",
      " \t -- train loss = 1.7841601782592487 / train cost = 1.9833871775844092\n",
      "\t -- {\"accuracy_train\": 0.3564} \n",
      "\t -- val loss = 1.8348491768969988 / val cost = 2.0340761762221593\n",
      "\t -- {\"accuracy_val\": 0.3287} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...                                 \n",
      "epoch 44/48                                            \n",
      " \t -- train loss = 1.7577307789349055 / train cost = 1.958619522650332\n",
      "\t -- {\"accuracy_train\": 0.3873} \n",
      "\t -- val loss = 1.8036688957405658 / val cost = 2.0045576394559923\n",
      "\t -- {\"accuracy_val\": 0.3671} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...                                 \n",
      "epoch 45/48                                            \n",
      " \t -- train loss = 1.7533032455250122 / train cost = 1.9513507553198037\n",
      "\t -- {\"accuracy_train\": 0.3835} \n",
      "\t -- val loss = 1.8040912137439733 / val cost = 2.002138723538765\n",
      "\t -- {\"accuracy_val\": 0.3676} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...                                 \n",
      "epoch 46/48                                            \n",
      " \t -- train loss = 1.7493535473169315 / train cost = 1.9506341909819291\n",
      "\t -- {\"accuracy_train\": 0.3913} \n",
      "\t -- val loss = 1.8015673750267325 / val cost = 2.00284801869173\n",
      "\t -- {\"accuracy_val\": 0.3715} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...                                 \n",
      "epoch 47/48                                            \n",
      " \t -- train loss = 1.74100483742365 / train cost = 1.9463293719286527\n",
      "\t -- {\"accuracy_train\": 0.3853} \n",
      "\t -- val loss = 1.7925452599670826 / val cost = 1.9978697944720853\n",
      "\t -- {\"accuracy_val\": 0.3619} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...                                 \n",
      "epoch 48/48                                            \n",
      " \t -- train loss = 1.7384936617129476 / train cost = 1.9389253051548612\n",
      "\t -- {\"accuracy_train\": 0.392} \n",
      "\t -- val loss = 1.7920635683717285 / val cost = 1.992495211813642\n",
      "\t -- {\"accuracy_val\": 0.3655} \n",
      "\n",
      "\n",
      "val acc: {'accuracy': 0.3655}, with lr_initial=0.04376397246929965, reg_rate_l2=0.1988790408574803\n",
      "100%|██████████| 16/16 [00:55<00:00, 55.78s/trial, best loss: -0.4615]\n",
      "Best: {'lr_initial': 0.041070795239371384, 'reg_rate_l2': 0.05141531425601553}\n"
     ]
    }
   ],
   "source": [
    "pickle_saved_path = \"assets/two_layer/two_layer_hyperopt.hyperopt\"\n",
    "# loop indefinitely and stop whenever you like\n",
    "run_for = 5\n",
    "for i in range(run_for):\n",
    "    run_trials(x_train, y_train, x_val, y_val, pickle_saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "editorial-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[{'loss': -0.2275, 'status': 'ok', 'eval_time': 1628167898.3786826, 'val_acc': 0.2275, 'reg_rate_l2': 0.8733541898810822, 'lr_initial': 0.03531479235905652}, {'loss': -0.3007, 'status': 'ok', 'eval_time': 1628167953.9175081, 'val_acc': 0.3007, 'reg_rate_l2': 0.44957888390078576, 'lr_initial': 0.03757552007448616}, {'loss': -0.2952, 'status': 'ok', 'eval_time': 1628168009.819987, 'val_acc': 0.2952, 'reg_rate_l2': 0.4439174008841718, 'lr_initial': 0.08976089086332134}, {'loss': -0.2287, 'status': 'ok', 'eval_time': 1628168068.0323248, 'val_acc': 0.2287, 'reg_rate_l2': 0.867269908972918, 'lr_initial': 0.07094561279439587}, {'loss': -0.3414, 'status': 'ok', 'eval_time': 1628168124.995717, 'val_acc': 0.3414, 'reg_rate_l2': 0.2495046751966928, 'lr_initial': 0.08324555623250757}, {'loss': -0.283, 'status': 'ok', 'eval_time': 1628168184.4977756, 'val_acc': 0.283, 'reg_rate_l2': 0.5916647953685226, 'lr_initial': 0.009071932769960366}, {'loss': -0.3676, 'status': 'ok', 'eval_time': 1628168780.8982017, 'val_acc': 0.3676, 'reg_rate_l2': 0.20563957547472028, 'lr_initial': 0.025455458972727146}, {'loss': -0.3638, 'status': 'ok', 'eval_time': 1628168842.717839, 'val_acc': 0.3638, 'reg_rate_l2': 0.19773765626340353, 'lr_initial': 0.05567154057341354}, {'loss': -0.3051, 'status': 'ok', 'eval_time': 1628168902.1343253, 'val_acc': 0.3051, 'reg_rate_l2': 0.39566331098540874, 'lr_initial': 0.06987096618963154}, {'loss': -0.4447, 'status': 'ok', 'eval_time': 1628168965.7817807, 'val_acc': 0.4447, 'reg_rate_l2': 0.07779194111312751, 'lr_initial': 0.00904874029242342}, {'loss': -0.373, 'status': 'ok', 'eval_time': 1628169027.943557, 'val_acc': 0.373, 'reg_rate_l2': 0.1802207115342738, 'lr_initial': 0.05587434065110954}, {'loss': -0.4615, 'status': 'ok', 'eval_time': 1628169874.241304, 'val_acc': 0.4615, 'reg_rate_l2': 0.05141531425601553, 'lr_initial': 0.041070795239371384}, {'loss': -0.439, 'status': 'ok', 'eval_time': 1628169930.4812906, 'val_acc': 0.439, 'reg_rate_l2': 0.06380905695939129, 'lr_initial': 0.06842479935255724}, {'loss': -0.367, 'status': 'ok', 'eval_time': 1628169986.2973452, 'val_acc': 0.367, 'reg_rate_l2': 0.21287170399119315, 'lr_initial': 0.022872345264861062}, {'loss': -0.3051, 'status': 'ok', 'eval_time': 1628170042.017691, 'val_acc': 0.3051, 'reg_rate_l2': 0.4088758457004862, 'lr_initial': 0.0740982919356081}, {'loss': -0.3655, 'status': 'ok', 'eval_time': 1628170097.8019488, 'val_acc': 0.3655, 'reg_rate_l2': 0.1988790408574803, 'lr_initial': 0.04376397246929965}]\n",
      "best_trial\n",
      "{'loss': -0.4615, 'status': 'ok', 'eval_time': 1628169874.241304, 'val_acc': 0.4615, 'reg_rate_l2': 0.05141531425601553, 'lr_initial': 0.041070795239371384}\n"
     ]
    }
   ],
   "source": [
    "trials = pickle.load(open(pickle_saved_path, \"rb\"))\n",
    "print(len(trials.trials))\n",
    "print(trials.results)\n",
    "\n",
    "best_trial = max(trials.results, key=lambda x: x['val_acc'])\n",
    "print(\"best_trial\")\n",
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fallen-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>reg_rate_l2</th>\n",
       "      <th>lr_initial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.2275</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.873354</td>\n",
       "      <td>0.035315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.3007</td>\n",
       "      <td>0.3007</td>\n",
       "      <td>0.449579</td>\n",
       "      <td>0.037576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2952</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>0.443917</td>\n",
       "      <td>0.089761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.2287</td>\n",
       "      <td>0.2287</td>\n",
       "      <td>0.867270</td>\n",
       "      <td>0.070946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3414</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>0.249505</td>\n",
       "      <td>0.083246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.2830</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.591665</td>\n",
       "      <td>0.009072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.3676</td>\n",
       "      <td>0.3676</td>\n",
       "      <td>0.205640</td>\n",
       "      <td>0.025455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.3638</td>\n",
       "      <td>0.3638</td>\n",
       "      <td>0.197738</td>\n",
       "      <td>0.055672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.3051</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>0.395663</td>\n",
       "      <td>0.069871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.4447</td>\n",
       "      <td>0.4447</td>\n",
       "      <td>0.077792</td>\n",
       "      <td>0.009049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.3730</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.180221</td>\n",
       "      <td>0.055874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.4615</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.051415</td>\n",
       "      <td>0.041071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.4390</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.063809</td>\n",
       "      <td>0.068425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.3670</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>0.022872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.3051</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>0.408876</td>\n",
       "      <td>0.074098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.3655</td>\n",
       "      <td>0.3655</td>\n",
       "      <td>0.198879</td>\n",
       "      <td>0.043764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  val_acc  reg_rate_l2  lr_initial\n",
       "0  -0.2275   0.2275     0.873354    0.035315\n",
       "1  -0.3007   0.3007     0.449579    0.037576\n",
       "2  -0.2952   0.2952     0.443917    0.089761\n",
       "3  -0.2287   0.2287     0.867270    0.070946\n",
       "4  -0.3414   0.3414     0.249505    0.083246\n",
       "5  -0.2830   0.2830     0.591665    0.009072\n",
       "6  -0.3676   0.3676     0.205640    0.025455\n",
       "7  -0.3638   0.3638     0.197738    0.055672\n",
       "8  -0.3051   0.3051     0.395663    0.069871\n",
       "9  -0.4447   0.4447     0.077792    0.009049\n",
       "10 -0.3730   0.3730     0.180221    0.055874\n",
       "11 -0.4615   0.4615     0.051415    0.041071\n",
       "12 -0.4390   0.4390     0.063809    0.068425\n",
       "13 -0.3670   0.3670     0.212872    0.022872\n",
       "14 -0.3051   0.3051     0.408876    0.074098\n",
       "15 -0.3655   0.3655     0.198879    0.043764"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(trials.results)\n",
    "df[\"val_acc\"] = df[\"loss\"] * -1\n",
    "df = df.drop(['status', 'eval_time'], 1)\n",
    "df.to_csv(\"assets/two_layer/two_layer_hyperopt.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-peace",
   "metadata": {},
   "source": [
    "Having found the best hyperparameter combination of the L2 regularization rate and the initial learning rate of the cyclical learning rate schedule, let's fit again the model, but this time with the combination of the training and the validation sets, and then evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "standing-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_val = np.vstack([x_train, x_val])\n",
    "y_train_val = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constant-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step size of cyc. lr: 400 update steps\n",
      "full cycle of cyc.lr : 800 update steps\n",
      "4.0 epochs = 1 full cycle = 800 update steps\n",
      "18 cycle = 72.0 epochs = 14400 update steps\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train_val.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train_val.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 18\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "informal-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: dense: \n",
      "\t shape -- in: 3072, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.018042^2), reg: l2 with 5.1415e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 10\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 5.1415e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: softmax\n",
      "\n",
      "categorical cross-entropy loss with loss smoother constant\n",
      "sgd with cycling lr schedule and clipper who does nothing\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.65it/s] \n",
      "epoch 1/72 \n",
      " \t -- train loss = 1.6506997766903437 / train cost = 2.288974349256008\n",
      "\t -- {\"accuracy_train\": 0.42035} \n",
      "\t -- val loss = 2.1876642320315316 / val cost = 2.8259388045971963\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.25it/s]\n",
      "epoch 2/72 \n",
      " \t -- train loss = 1.6291743307107815 / train cost = 1.9247412057647155\n",
      "\t -- {\"accuracy_train\": 0.4192} \n",
      "\t -- val loss = 2.2359544589232323 / val cost = 2.5315213339771665\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.66it/s] \n",
      "epoch 3/72 \n",
      " \t -- train loss = 1.6937743285516467 / train cost = 1.8956782667653695\n",
      "\t -- {\"accuracy_train\": 0.40285} \n",
      "\t -- val loss = 2.484634353362681 / val cost = 2.686538291576404\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.98it/s] \n",
      "epoch 4/72 \n",
      " \t -- train loss = 1.6688948527281857 / train cost = 1.8546622147422063\n",
      "\t -- {\"accuracy_train\": 0.40735} \n",
      "\t -- val loss = 2.0500031135580974 / val cost = 2.235770475572118\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.29it/s] \n",
      "epoch 5/72 \n",
      " \t -- train loss = 1.5644005502598226 / train cost = 1.753635538498361\n",
      "\t -- {\"accuracy_train\": 0.4555} \n",
      "\t -- val loss = 1.7999917413951132 / val cost = 1.9892267296336517\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.70it/s] \n",
      "epoch 6/72 \n",
      " \t -- train loss = 1.5428843158210415 / train cost = 1.7371471390024067\n",
      "\t -- {\"accuracy_train\": 0.4584} \n",
      "\t -- val loss = 1.9797188503923207 / val cost = 2.173981673573686\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.08it/s] \n",
      "epoch 7/72 \n",
      " \t -- train loss = 1.51758361789208 / train cost = 1.7166132236025702\n",
      "\t -- {\"accuracy_train\": 0.47485} \n",
      "\t -- val loss = 1.6763850908573716 / val cost = 1.8754146965678617\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.44it/s]\n",
      "epoch 8/72 \n",
      " \t -- train loss = 1.4819931222028524 / train cost = 1.687765035772873\n",
      "\t -- {\"accuracy_train\": 0.48855} \n",
      "\t -- val loss = 2.2441602312695457 / val cost = 2.449932144839566\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 9 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 97.88it/s] \n",
      "epoch 9/72 \n",
      " \t -- train loss = 1.4913954756650223 / train cost = 1.7008396930415992\n",
      "\t -- {\"accuracy_train\": 0.4791} \n",
      "\t -- val loss = 1.9598375150755918 / val cost = 2.1692817324521685\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 10 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.75it/s] \n",
      "epoch 10/72 \n",
      " \t -- train loss = 1.533431882838994 / train cost = 1.7421409937166694\n",
      "\t -- {\"accuracy_train\": 0.4632} \n",
      "\t -- val loss = 2.087565616952854 / val cost = 2.2962747278305295\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 11 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.43it/s]\n",
      "epoch 11/72 \n",
      " \t -- train loss = 1.6007751567001747 / train cost = 1.807105309570753\n",
      "\t -- {\"accuracy_train\": 0.4383} \n",
      "\t -- val loss = 1.784456876730538 / val cost = 1.9907870296011163\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 12 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.51it/s]\n",
      "epoch 12/72 \n",
      " \t -- train loss = 1.6268163468725874 / train cost = 1.8308932273813814\n",
      "\t -- {\"accuracy_train\": 0.41845} \n",
      "\t -- val loss = 1.9353408257830467 / val cost = 2.139417706291841\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 13 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 104.72it/s]\n",
      "epoch 13/72 \n",
      " \t -- train loss = 1.5871708162787495 / train cost = 1.7873320927576724\n",
      "\t -- {\"accuracy_train\": 0.45485} \n",
      "\t -- val loss = 1.9038554239289636 / val cost = 2.1040167004078865\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 14 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.37it/s]\n",
      "epoch 14/72 \n",
      " \t -- train loss = 1.5048127309351509 / train cost = 1.7105903157932996\n",
      "\t -- {\"accuracy_train\": 0.46695} \n",
      "\t -- val loss = 1.306780734313847 / val cost = 1.5125583191719956\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 15 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 104.06it/s]\n",
      "epoch 15/72 \n",
      " \t -- train loss = 1.4961649513919417 / train cost = 1.7045438113250293\n",
      "\t -- {\"accuracy_train\": 0.48525} \n",
      "\t -- val loss = 1.738408494867618 / val cost = 1.9467873548007055\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 16 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.38it/s] \n",
      "epoch 16/72 \n",
      " \t -- train loss = 1.4580484724315237 / train cost = 1.6720078610935765\n",
      "\t -- {\"accuracy_train\": 0.5005} \n",
      "\t -- val loss = 1.8471446469035175 / val cost = 2.0611040355655703\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 17 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.98it/s] \n",
      "epoch 17/72 \n",
      " \t -- train loss = 1.4715143854255577 / train cost = 1.6876962426531184\n",
      "\t -- {\"accuracy_train\": 0.49105} \n",
      "\t -- val loss = 1.7817049776632086 / val cost = 1.9978868348907692\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 18 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.10it/s]\n",
      "epoch 18/72 \n",
      " \t -- train loss = 1.5290917510755226 / train cost = 1.7476689577725388\n",
      "\t -- {\"accuracy_train\": 0.4558} \n",
      "\t -- val loss = 2.3445111086597916 / val cost = 2.563088315356808\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 19 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.79it/s]\n",
      "epoch 19/72 \n",
      " \t -- train loss = 1.5395009901470869 / train cost = 1.7516927996501572\n",
      "\t -- {\"accuracy_train\": 0.4504} \n",
      "\t -- val loss = 1.8044427080971788 / val cost = 2.016634517600249\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 20 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.00it/s]\n",
      "epoch 20/72 \n",
      " \t -- train loss = 1.5671939105476416 / train cost = 1.7731724265921132\n",
      "\t -- {\"accuracy_train\": 0.4377} \n",
      "\t -- val loss = 2.304243880563492 / val cost = 2.5102223966079635\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 21 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.23it/s]\n",
      "epoch 21/72 \n",
      " \t -- train loss = 1.5278379960533632 / train cost = 1.7294576998007456\n",
      "\t -- {\"accuracy_train\": 0.4694} \n",
      "\t -- val loss = 1.7248177664958 / val cost = 1.9264374702431823\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 22 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 98.53it/s] \n",
      "epoch 22/72 \n",
      " \t -- train loss = 1.5004991283844666 / train cost = 1.7104688984412997\n",
      "\t -- {\"accuracy_train\": 0.47995} \n",
      "\t -- val loss = 1.562742185851416 / val cost = 1.7727119559082491\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 23 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.76it/s]\n",
      "epoch 23/72 \n",
      " \t -- train loss = 1.4835794375798648 / train cost = 1.6978243398054762\n",
      "\t -- {\"accuracy_train\": 0.48555} \n",
      "\t -- val loss = 2.048174070289351 / val cost = 2.2624189725149626\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 24 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.33it/s]\n",
      "epoch 24/72 \n",
      " \t -- train loss = 1.4671175187447096 / train cost = 1.6813561967838726\n",
      "\t -- {\"accuracy_train\": 0.48975} \n",
      "\t -- val loss = 1.5496423164776192 / val cost = 1.7638809945167822\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 25 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.79it/s]\n",
      "epoch 25/72 \n",
      " \t -- train loss = 1.4608512443381358 / train cost = 1.6827674785165088\n",
      "\t -- {\"accuracy_train\": 0.49625} \n",
      "\t -- val loss = 2.152385336965076 / val cost = 2.374301571143449\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 26 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.79it/s]\n",
      "epoch 26/72 \n",
      " \t -- train loss = 1.577916985228368 / train cost = 1.795653631227313\n",
      "\t -- {\"accuracy_train\": 0.4338} \n",
      "\t -- val loss = 2.434118925811811 / val cost = 2.651855571810756\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 27 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.43it/s]\n",
      "epoch 27/72 \n",
      " \t -- train loss = 1.5263781752193573 / train cost = 1.7396410295269902\n",
      "\t -- {\"accuracy_train\": 0.46245} \n",
      "\t -- val loss = 1.5899897395520557 / val cost = 1.8032525938596886\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 28 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.48it/s]\n",
      "epoch 28/72 \n",
      " \t -- train loss = 1.5847060282703347 / train cost = 1.7935290620355107\n",
      "\t -- {\"accuracy_train\": 0.43935} \n",
      "\t -- val loss = 1.955674717243608 / val cost = 2.164497751008784\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 29 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.34it/s]\n",
      "epoch 29/72 \n",
      " \t -- train loss = 1.519649983358948 / train cost = 1.72716391349122\n",
      "\t -- {\"accuracy_train\": 0.46325} \n",
      "\t -- val loss = 1.5648186183079318 / val cost = 1.772332548440204\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 30 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.70it/s]\n",
      "epoch 30/72 \n",
      " \t -- train loss = 1.525482303954051 / train cost = 1.7355867480461247\n",
      "\t -- {\"accuracy_train\": 0.46335} \n",
      "\t -- val loss = 2.053644410799912 / val cost = 2.2637488548919857\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 31 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.03it/s]\n",
      "epoch 31/72 \n",
      " \t -- train loss = 1.4701605048697446 / train cost = 1.6834507316922032\n",
      "\t -- {\"accuracy_train\": 0.4881} \n",
      "\t -- val loss = 1.7798537120494482 / val cost = 1.9931439388719068\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 32 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.00it/s]\n",
      "epoch 32/72 \n",
      " \t -- train loss = 1.447198154660881 / train cost = 1.6647005552871557\n",
      "\t -- {\"accuracy_train\": 0.50305} \n",
      "\t -- val loss = 1.763723939854167 / val cost = 1.9812263404804418\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 33 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.77it/s]\n",
      "epoch 33/72 \n",
      " \t -- train loss = 1.510772207693417 / train cost = 1.7312161115274178\n",
      "\t -- {\"accuracy_train\": 0.46495} \n",
      "\t -- val loss = 2.279635140183033 / val cost = 2.500079044017034\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 34 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.46it/s]\n",
      "epoch 34/72 \n",
      " \t -- train loss = 1.4987740567485528 / train cost = 1.7185180187878375\n",
      "\t -- {\"accuracy_train\": 0.4761} \n",
      "\t -- val loss = 1.664950281064847 / val cost = 1.8846942431041318\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 35 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.84it/s]\n",
      "epoch 35/72 \n",
      " \t -- train loss = 1.5281885142790634 / train cost = 1.7384051765519828\n",
      "\t -- {\"accuracy_train\": 0.45685} \n",
      "\t -- val loss = 1.8676006681967983 / val cost = 2.0778173304697174\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 36 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.89it/s]\n",
      "epoch 36/72 \n",
      " \t -- train loss = 1.7524868103337623 / train cost = 1.9635017191922874\n",
      "\t -- {\"accuracy_train\": 0.3999} \n",
      "\t -- val loss = 2.774959290116068 / val cost = 2.985974198974593\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 37 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.08it/s]\n",
      "epoch 37/72 \n",
      " \t -- train loss = 1.5318991642056865 / train cost = 1.7375713024206727\n",
      "\t -- {\"accuracy_train\": 0.4673} \n",
      "\t -- val loss = 1.9771609732687194 / val cost = 2.1828331114837054\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 38 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.34it/s]\n",
      "epoch 38/72 \n",
      " \t -- train loss = 1.532515080932473 / train cost = 1.7391672549147281\n",
      "\t -- {\"accuracy_train\": 0.4651} \n",
      "\t -- val loss = 1.6586715078658152 / val cost = 1.8653236818480703\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 39 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.58it/s]\n",
      "epoch 39/72 \n",
      " \t -- train loss = 1.4840665219693046 / train cost = 1.6978056562682615\n",
      "\t -- {\"accuracy_train\": 0.47775} \n",
      "\t -- val loss = 1.450605461475826 / val cost = 1.6643445957747829\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 40 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.79it/s] \n",
      "epoch 40/72 \n",
      " \t -- train loss = 1.4506844781095432 / train cost = 1.669121630325097\n",
      "\t -- {\"accuracy_train\": 0.50255} \n",
      "\t -- val loss = 1.7861656118089688 / val cost = 2.004602764024523\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 41 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.83it/s]\n",
      "epoch 41/72 \n",
      " \t -- train loss = 1.495589421490371 / train cost = 1.7163876449696274\n",
      "\t -- {\"accuracy_train\": 0.4758} \n",
      "\t -- val loss = 1.9373298749540595 / val cost = 2.1581280984333158\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 42 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.34it/s]\n",
      "epoch 42/72 \n",
      " \t -- train loss = 1.5087570241527404 / train cost = 1.7263966191195497\n",
      "\t -- {\"accuracy_train\": 0.4736} \n",
      "\t -- val loss = 1.7939596944786629 / val cost = 2.011599289445472\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 43 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.89it/s] \n",
      "epoch 43/72 \n",
      " \t -- train loss = 1.6106489624392375 / train cost = 1.822154184821974\n",
      "\t -- {\"accuracy_train\": 0.42915} \n",
      "\t -- val loss = 2.756101254076935 / val cost = 2.9676064764596717\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 44 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.88it/s] \n",
      "epoch 44/72 \n",
      " \t -- train loss = 1.594443832861499 / train cost = 1.8039607159442093\n",
      "\t -- {\"accuracy_train\": 0.4273} \n",
      "\t -- val loss = 1.163208442705721 / val cost = 1.3727253257884313\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 45 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.23it/s]\n",
      "epoch 45/72 \n",
      " \t -- train loss = 1.5327756318858625 / train cost = 1.7426236639161603\n",
      "\t -- {\"accuracy_train\": 0.4622} \n",
      "\t -- val loss = 2.2504345567773294 / val cost = 2.4602825888076274\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 46 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.74it/s]\n",
      "epoch 46/72 \n",
      " \t -- train loss = 1.52033356905978 / train cost = 1.7331308080666044\n",
      "\t -- {\"accuracy_train\": 0.47085} \n",
      "\t -- val loss = 1.9468930564470868 / val cost = 2.159690295453911\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 47 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.85it/s]\n",
      "epoch 47/72 \n",
      " \t -- train loss = 1.4797078842408737 / train cost = 1.6949132609966164\n",
      "\t -- {\"accuracy_train\": 0.4935} \n",
      "\t -- val loss = 2.030707486231982 / val cost = 2.2459128629877245\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 48 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.30it/s]\n",
      "epoch 48/72 \n",
      " \t -- train loss = 1.4630128956424036 / train cost = 1.6821438807013622\n",
      "\t -- {\"accuracy_train\": 0.4914} \n",
      "\t -- val loss = 2.200035178610957 / val cost = 2.4191661636699155\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 49 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.33it/s]\n",
      "epoch 49/72 \n",
      " \t -- train loss = 1.473566614770435 / train cost = 1.6962277900922809\n",
      "\t -- {\"accuracy_train\": 0.48635} \n",
      "\t -- val loss = 1.940209211449711 / val cost = 2.162870386771557\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 50 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.19it/s]\n",
      "epoch 50/72 \n",
      " \t -- train loss = 1.4882014839106283 / train cost = 1.7094008456054277\n",
      "\t -- {\"accuracy_train\": 0.48145} \n",
      "\t -- val loss = 1.753912067306859 / val cost = 1.9751114290016583\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 51 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.04it/s]\n",
      "epoch 51/72 \n",
      " \t -- train loss = 1.5266122505099087 / train cost = 1.741027349446668\n",
      "\t -- {\"accuracy_train\": 0.4625} \n",
      "\t -- val loss = 1.6638027586400503 / val cost = 1.8782178575768096\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 52 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.91it/s]\n",
      "epoch 52/72 \n",
      " \t -- train loss = 1.5441175240209322 / train cost = 1.755596810246186\n",
      "\t -- {\"accuracy_train\": 0.4643} \n",
      "\t -- val loss = 1.3301049593977003 / val cost = 1.5415842456229543\n",
      "\t -- {\"accuracy_val\": 1.0} \n",
      "\n",
      "\n",
      "starting epoch: 53 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.36it/s]\n",
      "epoch 53/72 \n",
      " \t -- train loss = 1.5411345623496393 / train cost = 1.7513633503008517\n",
      "\t -- {\"accuracy_train\": 0.4562} \n",
      "\t -- val loss = 2.004966947731582 / val cost = 2.2151957356827947\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 54 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.66it/s]\n",
      "epoch 54/72 \n",
      " \t -- train loss = 1.5254815254171357 / train cost = 1.7377005042288498\n",
      "\t -- {\"accuracy_train\": 0.45675} \n",
      "\t -- val loss = 1.3714793081463639 / val cost = 1.583698286958078\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 55 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.15it/s]\n",
      "epoch 55/72 \n",
      " \t -- train loss = 1.5056596298839975 / train cost = 1.7234637200301126\n",
      "\t -- {\"accuracy_train\": 0.46445} \n",
      "\t -- val loss = 1.3017326236553042 / val cost = 1.5195367138014193\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 56 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.06it/s]\n",
      "epoch 56/72 \n",
      " \t -- train loss = 1.461984183245281 / train cost = 1.683591782850724\n",
      "\t -- {\"accuracy_train\": 0.48965} \n",
      "\t -- val loss = 1.663063268497566 / val cost = 1.884670868103009\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 57 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.89it/s]\n",
      "epoch 57/72 \n",
      " \t -- train loss = 1.4899903580925502 / train cost = 1.711109621607778\n",
      "\t -- {\"accuracy_train\": 0.4845} \n",
      "\t -- val loss = 2.419878431321327 / val cost = 2.6409976948365546\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 58 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.46it/s]\n",
      "epoch 58/72 \n",
      " \t -- train loss = 1.495111847642305 / train cost = 1.7167592387129045\n",
      "\t -- {\"accuracy_train\": 0.47775} \n",
      "\t -- val loss = 1.7971765409273457 / val cost = 2.0188239319979453\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 59 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.03it/s]\n",
      "epoch 59/72 \n",
      " \t -- train loss = 1.5285208918120636 / train cost = 1.7434123589822874\n",
      "\t -- {\"accuracy_train\": 0.4584} \n",
      "\t -- val loss = 1.939772418822628 / val cost = 2.1546638859928517\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 60 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.82it/s] \n",
      "epoch 60/72 \n",
      " \t -- train loss = 1.6017522300488307 / train cost = 1.817255643129268\n",
      "\t -- {\"accuracy_train\": 0.41995} \n",
      "\t -- val loss = 2.0005678908260425 / val cost = 2.21607130390648\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 61 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.25it/s]\n",
      "epoch 61/72 \n",
      " \t -- train loss = 1.502117086747149 / train cost = 1.7162123516291203\n",
      "\t -- {\"accuracy_train\": 0.4712} \n",
      "\t -- val loss = 1.6218148773179042 / val cost = 1.8359101421998756\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 62 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.32it/s]\n",
      "epoch 62/72 \n",
      " \t -- train loss = 1.4908761724602155 / train cost = 1.7037510284441897\n",
      "\t -- {\"accuracy_train\": 0.4927} \n",
      "\t -- val loss = 1.7232976105330287 / val cost = 1.9361724665170028\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 63 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.24it/s]\n",
      "epoch 63/72 \n",
      " \t -- train loss = 1.4738166243908644 / train cost = 1.6920830172794241\n",
      "\t -- {\"accuracy_train\": 0.4965} \n",
      "\t -- val loss = 2.0527678365046813 / val cost = 2.271034229393241\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 64 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.02it/s]\n",
      "epoch 64/72 \n",
      " \t -- train loss = 1.4557011264917548 / train cost = 1.680105987437868\n",
      "\t -- {\"accuracy_train\": 0.4906} \n",
      "\t -- val loss = 1.678622259688894 / val cost = 1.9030271206350071\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 65 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.82it/s]\n",
      "epoch 65/72 \n",
      " \t -- train loss = 1.4543243198168998 / train cost = 1.6788442228605958\n",
      "\t -- {\"accuracy_train\": 0.49915} \n",
      "\t -- val loss = 1.4292111579048439 / val cost = 1.6537310609485398\n",
      "\t -- {\"accuracy_val\": 1.0} \n",
      "\n",
      "\n",
      "starting epoch: 66 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.30it/s]\n",
      "epoch 66/72 \n",
      " \t -- train loss = 1.5035186505116276 / train cost = 1.7255864381289063\n",
      "\t -- {\"accuracy_train\": 0.48095} \n",
      "\t -- val loss = 1.7916578879188383 / val cost = 2.013725675536117\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 67 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:02<00:00, 99.66it/s] \n",
      "epoch 67/72 \n",
      " \t -- train loss = 1.5276870808715035 / train cost = 1.745325538628052\n",
      "\t -- {\"accuracy_train\": 0.458} \n",
      "\t -- val loss = 1.478891652350161 / val cost = 1.6965301101067094\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 68 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.51it/s]\n",
      "epoch 68/72 \n",
      " \t -- train loss = 1.5223400875171225 / train cost = 1.7364245070441298\n",
      "\t -- {\"accuracy_train\": 0.4636} \n",
      "\t -- val loss = 1.7826031233508934 / val cost = 1.9966875428779007\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 69 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 102.82it/s]\n",
      "epoch 69/72 \n",
      " \t -- train loss = 1.5007339255485788 / train cost = 1.7132814276554393\n",
      "\t -- {\"accuracy_train\": 0.48105} \n",
      "\t -- val loss = 2.196141097962909 / val cost = 2.408688600069769\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "starting epoch: 70 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 100.05it/s]\n",
      "epoch 70/72 \n",
      " \t -- train loss = 1.4817018358798597 / train cost = 1.6967487861369468\n",
      "\t -- {\"accuracy_train\": 0.48385} \n",
      "\t -- val loss = 1.8201828265512048 / val cost = 2.035229776808292\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 71 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 103.99it/s]\n",
      "epoch 71/72 \n",
      " \t -- train loss = 1.4702591204398656 / train cost = 1.6898017054007919\n",
      "\t -- {\"accuracy_train\": 0.4863} \n",
      "\t -- val loss = 1.5966235014773424 / val cost = 1.8161660864382687\n",
      "\t -- {\"accuracy_val\": 0.5} \n",
      "\n",
      "\n",
      "starting epoch: 72 ...\n",
      "batch 200/200: 100%|██████████| 200/200 [00:01<00:00, 101.19it/s]\n",
      "epoch 72/72 \n",
      " \t -- train loss = 1.4425708203130223 / train cost = 1.6660915690011115\n",
      "\t -- {\"accuracy_train\": 0.50505} \n",
      "\t -- val loss = 1.9135882695408686 / val cost = 2.137109018228958\n",
      "\t -- {\"accuracy_val\": 0.0} \n",
      "\n",
      "\n",
      "The test metrics are: {'accuracy': 0.471}\n"
     ]
    }
   ],
   "source": [
    "reg_rate_l2 = best_trial[\"reg_rate_l2\"]\n",
    "lr_initial = best_trial[\"lr_initial\"]\n",
    "\n",
    "params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "in_dim = x_train_val.shape[1]\n",
    "out_dim = np.unique(y_train_val).size\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss(loss_smoother=LossSmootherConstant())\n",
    "\n",
    "# train longer\n",
    "n_epochs = 72\n",
    "batch_size = 100\n",
    "\n",
    "lr_max = 1e-1\n",
    "\n",
    "# as defined from before\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule, grad_clipper=GradClipperByNothing())\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "# verbosity level of fit\n",
    "verbose = 2\n",
    "# x_val y_val here dont matter\n",
    "history = model.fit(x_train_val, y_train_val, x_val[:2], y_val[:2], n_epochs, batch_size, verbose, aug_func=None)\n",
    "\n",
    "params_test = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"The test metrics are: {metrics_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "necessary-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_lrs(history, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-tragedy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
