{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "import hyperopt\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation, Activation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense, BatchNormalization\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer, Optimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule\n",
    "from grad_check import eval_numerical_gradient, eval_numerical_gradient_array, numerical_gradient_check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPData():\n",
    "    def __init__(self, path_to_file):\n",
    "        \"\"\" Init.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_to_file : str\n",
    "            Path to text file.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        # read text file\n",
    "        with open(path_to_file, 'r') as f:\n",
    "            self.book_str = f.read()\n",
    "        \n",
    "        # str to chars\n",
    "        book_data = list(self.book_str)\n",
    "        # chars to unique chars\n",
    "        book_chars = list(dict.fromkeys(book_data))\n",
    "        \n",
    "        # all chars as np\n",
    "        self.book_data = np.array(book_data)\n",
    "        # uniqe chars as np\n",
    "        self.book_chars = np.array(book_chars)\n",
    "    \n",
    "    def get_encoder(self,):\n",
    "        \"\"\" Returns encoder, i.e.: unique chars.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        book_chars : np.ndarray of shape (n_unique_chars, )\n",
    "            The encoder as np.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return self.book_chars\n",
    "    \n",
    "    def char_to_idx(self, char):\n",
    "        \"\"\" Convert a char to an index from the encoder np array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        char : str\n",
    "            A char.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The index repre of char, of shape (,).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return np.argwhere(char == self.book_chars).flatten()[0]\n",
    "    \n",
    "    def idx_to_char(self, idx):\n",
    "        \"\"\" Convert an index to char in the encoder np array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index repr of a char.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The char.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return self.book_chars[idx]\n",
    "    \n",
    "    def encode(self, decoding):\n",
    "        \"\"\" Encode a sequence of chars into a sequence of indices based on the encoder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        chars : np.ndarray\n",
    "            The sequence of chars, of shape (n_chars,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        encoding : np.ndarray\n",
    "            The sequence of index representation of the chars, of shape (n_chars,)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        encoding = []\n",
    "        \n",
    "        for d in decoding:\n",
    "            encoding.append(self.char_to_idx(d))\n",
    "            \n",
    "        encoding = np.array(encoding)\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    def decode(self, encoding):\n",
    "        \"\"\" Decode a sequence of indices into a sequence of chars based on the encoder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoding : np.ndarray\n",
    "            The sequence of index representation of the chars, of shape (n_chars,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decoding : np.ndarray\n",
    "            The sequence of chars, of shape (n_chars,)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        decoding = []\n",
    "        \n",
    "        for e in encoding:\n",
    "            decoding.append(self.idx_to_char(e))\n",
    "            \n",
    "        decoding = np.array(decoding)\n",
    "        \n",
    "        return decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \"\"\" Model class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        The length of the one-hot encoding.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(layers)\n",
    "        Constuctor.\n",
    "    __call__(x, encode=True)\n",
    "        Encode a sequence of integers into a one-hot encoded vectors,\n",
    "        or decode a sequence of one-hot encoded vectors into a \n",
    "        sequence of integers.\n",
    "    __repr__()\n",
    "        Returns the string representation of class.\n",
    "    \"\"\"\n",
    "    def __init__(self, length):\n",
    "        \"\"\" Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int\n",
    "            The length of the one-hot encoding.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        # length of one-hot encoding\n",
    "        self.length = length\n",
    "    \n",
    "    def __call__(self, x, encode=True):\n",
    "        \"\"\" Encode a sequence of integers into a one-hot encoded vectors,\n",
    "        or decode a sequence of one-hot encoded vectors into a \n",
    "        sequence of integers..\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            The sequence of index representation of chars, of shape (n_chars,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        e or d: np.ndarray\n",
    "            The sequence of one-hot encoded vectors of chars, of shape (n_chars, length)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if encode:\n",
    "            e = np.zeros((x.shape[0], self.length))\n",
    "            e[np.arange(x.shape[0]), x] = 1\n",
    "            return e.astype(int)\n",
    "        else:\n",
    "            d = np.argwhere(one_hot_encoding == 1)[:,1]\n",
    "            return d.astype(int)\n",
    "        \n",
    "    def __repr__(self,):\n",
    "        \"\"\" Returns the string representation of class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        repr_str : str\n",
    "            The string representation of the class.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        repr_str = \"one-hot encoder\"\n",
    "        return repr_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Read, encode and decode data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,)\n",
      "['H' 'A' 'R' 'Y' ' ' 'P' 'O' 'T' 'E' 'N' 'D' 'G' 'B' 'L' 'F' 'I' '\\n' 'C'\n",
      " '-' 'U' 'S' '\\t' 'h' 'e' 'v' 'i' 'l' 'a' 'g' 'r' 's' 'o' 'f' 't' 'n' 'c'\n",
      " 'd' '\"' 'u' ',' 'b' 'm' 'y' '.' 'k' 'w' 'p' 'q' ':' \"'\" '!' 'x' 'M' ';'\n",
      " 'j' 'W' '?' '(' ')' 'Q' 'z' 'V' 'J' 'K' 'Z' 'X' '0' '1' '6' '7' 'ü' '4'\n",
      " '3' '9' '2' '}' '_' '/' '^' '•']\n",
      "['H' 'A' 'R' 'R' 'Y' ' ' 'P' 'O' 'T' 'T' 'E' 'R' ' ' 'A' 'N' 'D' ' ' 'T'\n",
      " 'H' 'E' ' ' 'G' 'O' 'B' 'L' 'E' 'T' ' ' 'O' 'F' ' ' 'F' 'I' 'R' 'E' '\\n'\n",
      " '\\n' 'C' 'H' 'A' 'P' 'T' 'E' 'R' ' ' 'O' 'N' 'E' ' ' '-' ' ' 'T' 'H' 'E'\n",
      " ' ' 'R' 'I' 'D' 'D' 'L' 'E' ' ' 'H' 'O' 'U' 'S' 'E' '\\n' '\\n' '\\t' 'T'\n",
      " 'h' 'e' ' ' 'v' 'i' 'l' 'l' 'a' 'g' 'e' 'r' 's' ' ' 'o' 'f' ' ' 'L' 'i'\n",
      " 't' 't' 'l' 'e' ' ' 'H' 'a' 'n' 'g' 'l' 'e' 'r' 'o' 'n' ' ' 's' 't' 'i'\n",
      " 'l' 'l' ' ' 'c' 'a' 'l' 'l' 'e' 'd' ' ' 'i' 't' ' ' '\"' 't' 'h' 'e' ' '\n",
      " 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'H' 'o' 'u' 's' 'e' ',' '\"' ' ' 'e' 'v' 'e'\n",
      " 'n' ' ' 't' 'h' 'o' 'u' 'g' 'h' ' ' 'i' 't' ' ' 'h' 'a' 'd' ' ' 'b' 'e'\n",
      " 'e' 'n' ' ' 'm' 'a' 'n' 'y' ' ' 'y' 'e' 'a' 'r' 's' ' ' 's' 'i' 'n' 'c'\n",
      " 'e' ' ' 't' 'h' 'e' ' ' 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'f' 'a' 'm' 'i' 'l'\n",
      " 'y' ' ' 'h']\n",
      "(80,)\n",
      "[ 0  1  2  2  3  4  5  6  7  7  8  2  4  1  9 10  4  7  0  8  4 11  6 12\n",
      " 13  8  7  4  6 14  4 14 15  2  8 16 16 17  0  1  5  7  8  2  4  6  9  8\n",
      "  4 18  4  7  0  8  4  2 15 10 10 13  8  4  0  6 19 20  8 16 16 21  7 22\n",
      " 23  4 24 25 26 26 27 28 23 29 30  4 31 32  4 13 25 33 33 26 23  4  0 27\n",
      " 34 28 26 23 29 31 34  4 30 33 25 26 26  4 35 27 26 26 23 36  4 25 33  4\n",
      " 37 33 22 23  4  2 25 36 36 26 23  4  0 31 38 30 23 39 37  4 23 24 23 34\n",
      "  4 33 22 31 38 28 22  4 25 33  4 22 27 36  4 40 23 23 34  4 41 27 34 42\n",
      "  4 42 23 27 29 30  4 30 25 34 35 23  4 33 22 23  4  2 25 36 36 26 23  4\n",
      " 32 27 41 25 26 42  4 22]\n",
      "['H' 'A' 'R' 'R' 'Y' ' ' 'P' 'O' 'T' 'T' 'E' 'R' ' ' 'A' 'N' 'D' ' ' 'T'\n",
      " 'H' 'E' ' ' 'G' 'O' 'B' 'L' 'E' 'T' ' ' 'O' 'F' ' ' 'F' 'I' 'R' 'E' '\\n'\n",
      " '\\n' 'C' 'H' 'A' 'P' 'T' 'E' 'R' ' ' 'O' 'N' 'E' ' ' '-' ' ' 'T' 'H' 'E'\n",
      " ' ' 'R' 'I' 'D' 'D' 'L' 'E' ' ' 'H' 'O' 'U' 'S' 'E' '\\n' '\\n' '\\t' 'T'\n",
      " 'h' 'e' ' ' 'v' 'i' 'l' 'l' 'a' 'g' 'e' 'r' 's' ' ' 'o' 'f' ' ' 'L' 'i'\n",
      " 't' 't' 'l' 'e' ' ' 'H' 'a' 'n' 'g' 'l' 'e' 'r' 'o' 'n' ' ' 's' 't' 'i'\n",
      " 'l' 'l' ' ' 'c' 'a' 'l' 'l' 'e' 'd' ' ' 'i' 't' ' ' '\"' 't' 'h' 'e' ' '\n",
      " 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'H' 'o' 'u' 's' 'e' ',' '\"' ' ' 'e' 'v' 'e'\n",
      " 'n' ' ' 't' 'h' 'o' 'u' 'g' 'h' ' ' 'i' 't' ' ' 'h' 'a' 'd' ' ' 'b' 'e'\n",
      " 'e' 'n' ' ' 'm' 'a' 'n' 'y' ' ' 'y' 'e' 'a' 'r' 's' ' ' 's' 'i' 'n' 'c'\n",
      " 'e' ' ' 't' 'h' 'e' ' ' 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'f' 'a' 'm' 'i' 'l'\n",
      " 'y' ' ' 'h']\n"
     ]
    }
   ],
   "source": [
    "path_to_file = \"data/hp/goblet_book.txt\"\n",
    "hpdata = HPData(path_to_file=path_to_file)\n",
    "print(hpdata.get_encoder().shape)\n",
    "print(hpdata.get_encoder())\n",
    "x = hpdata.book_data[:200]\n",
    "print(x)\n",
    "encoding = hpdata.encode(x)\n",
    "print(hpdata.get_encoder().shape)\n",
    "print(encoding)\n",
    "decoding = hpdata.decode(encoding)\n",
    "print(decoding)\n",
    "\n",
    "np.testing.assert_array_equal(decoding, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-ho encode and decode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 80)\n",
      "(200,)\n",
      "6\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "17\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(length=hpdata.get_encoder().size)\n",
    "one_hot_encoding = onehot_encoder(encoding, encode=True)\n",
    "print(one_hot_encoding.shape)\n",
    "one_hot_decoding = onehot_encoder(one_hot_encoding, encode=False)\n",
    "print(one_hot_decoding.shape)\n",
    "\n",
    "np.testing.assert_array_equal(one_hot_decoding, encoding)\n",
    "print(one_hot_decoding[7])\n",
    "print(one_hot_encoding[7])\n",
    "\n",
    "print(one_hot_decoding[37])\n",
    "print(one_hot_encoding[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' 'a']\n",
      "(80,)\n",
      "[43 27]\n",
      "['.' 'a']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]]\n",
      "(2, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[27]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\".\", \"a\"])\n",
    "print(x)\n",
    "encoding = hpdata.encode(x)\n",
    "print(hpdata.get_encoder().shape)\n",
    "print(encoding)\n",
    "decoding = hpdata.decode(encoding)\n",
    "print(decoding)\n",
    "\n",
    "np.testing.assert_array_equal(decoding, x)\n",
    "\n",
    "one_hot_encoding = onehot_encoder(encoding, encode=True)\n",
    "print(one_hot_encoding)\n",
    "print(one_hot_encoding.shape)\n",
    "np.argwhere(hpdata.get_encoder() == \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhActivation(Activation):\n",
    "    \"\"\" Tanh activation.\n",
    "    Can be followed by virtually anything.\n",
    "    Inherits everything from class Activation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cache : dict\n",
    "        Run-time cache of attibutes such as gradients.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__()\n",
    "        Constuctor.\n",
    "    forward(z)\n",
    "        Activates the linear transformation of the layer, and\n",
    "        forward propagates activation. Activation is tanh.\n",
    "    backward(g)\n",
    "        Backpropagates incoming gradient into the layer, based on the tanh activation.\n",
    "    __repr__()\n",
    "        Returns the string representation of class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        \"\"\" Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\" Activates the linear transformation of the layer, and\n",
    "        forward propagates activation. Activation is tanh.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : numpy.ndarray\n",
    "            Linear transformation of layer.\n",
    "            Shape is unknown here, but will usually be\n",
    "            (batch size, this layer output dim = next layer input dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            ReLU activation.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        a = np.tanh(z)\n",
    "        self.cache[\"a\"] = deepcopy(a)\n",
    "        return a\n",
    "\n",
    "    def backward(self, g_in):\n",
    "        \"\"\" Backpropagates incoming gradient into the layer, based on the tanh activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g_in : numpy.ndarray\n",
    "            Incoming gradient to the activation.\n",
    "            Shape is unknown here, but will usually be\n",
    "            (batch size, this layer output dim = next layer input dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Gradient of activation.\n",
    "            Shape is unknown here, but will usually be\n",
    "            (batch size, this layer output dim = next layer input dim)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        a = deepcopy(self.cache[\"a\"])\n",
    "        g_out = (1 - np.power(a, 2)) * g_in\n",
    "        return g_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Returns the string representation of class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        repr_str : str\n",
    "            The string representation of the class.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        repr_str = \"tanh\"\n",
    "        return repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_relu_activation passed\n"
     ]
    }
   ],
   "source": [
    "def test_tanh_activation():\n",
    "    tanh_activation = TanhActivation()\n",
    "    np.random.seed(231)\n",
    "    x = np.random.randn(5, 10)\n",
    "    g_in = np.random.randn(*x.shape)\n",
    "    fx = lambda x: TanhActivation.forward(tanh_activation, x)\n",
    "    g_out_num = eval_numerical_gradient_array(fx, x, g_in)\n",
    "    g_out = tanh_activation.backward(g_in)\n",
    "    np.testing.assert_array_almost_equal(g_out, g_out_num, decimal=6)\n",
    "\n",
    "    print(\"test_relu_activation passed\")\n",
    "    \n",
    "test_tanh_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \"\"\" Many-to-many RNN layer for character-to-character sequence modelling.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_dim : int\n",
    "        Input dimension.\n",
    "    out_dim : int\n",
    "        Output dimension.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension.\n",
    "    kernel_h_initializer : Initializer\n",
    "        The weight parameter initializer of the hidden neurons.\n",
    "    bias_h_initializer : Initializer\n",
    "        The bias parameter initializer of the hidden neurons.\n",
    "    kernel_o_initializer : Initializer\n",
    "        The weight parameter initializer of the output neurons.\n",
    "    bias_o_initializer : Initializer\n",
    "        The bias parameter initializer of the output neurons.\n",
    "    kernel_regularizer : Regularizer\n",
    "        The weight parameter regularizer for all parameters.\n",
    "        Separate for h and o neurons. Not used yet. \n",
    "    activation_h : Activation\n",
    "        Layer activation of hidden neurons.\n",
    "    activation_o : Activation\n",
    "        Layer activation of output neurons.\n",
    "    u : numpy.ndarray\n",
    "        The weight parameters dotted with the input vector, \n",
    "        of shape (in_dim, hidden_dim)\n",
    "    w : numpy.ndarray\n",
    "        The weight parameters dotted with the pre-activation hidden vector, \n",
    "        of shape (hidden_dim, hidden_dim)\n",
    "    b : numpy.ndarray\n",
    "        The bias parameters added to the input-previous hidden vector \n",
    "        linear combination, of shape (1, hidden_dim)\n",
    "    v : numpy.ndarray\n",
    "        The weight parameters dotted with the activated hidden vector, \n",
    "        of shape (hidden_dim, out_dim)\n",
    "    c : numpy.ndarray\n",
    "        The bias parameters added to the dotted activated hidden vector, \n",
    "        of shape (1, out_dim)\n",
    "    cache : dict\n",
    "        The run-time cache for storing activations, etc.\n",
    "    grads : dict\n",
    "        The run-time cache for storing gradients.\n",
    "    h_shape : tuple\n",
    "        Hidden vector shape.\n",
    "    has_learnable_params : bool\n",
    "        If layer has learnable/trainable params.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(in_dim, out_dim, kernel_initializer, bias_initializer, kernel_regularizer, activation)\n",
    "        Constructor.\n",
    "    get_u()\n",
    "        Returns the u parameters.\n",
    "    get_w()\n",
    "        Returns the w parameters.\n",
    "    get_b()\n",
    "        Returns the b parameters.\n",
    "    get_v()\n",
    "        Returns the v parameters.\n",
    "    get_c()\n",
    "        Returns the c parameters.\n",
    "    set_u()\n",
    "        Sets the u parameters.\n",
    "    set_w()\n",
    "        Sets the w parameters.\n",
    "    set_b()\n",
    "        Sets the b parameters.\n",
    "    set_v()\n",
    "        Sets the v parameters.\n",
    "    set_c()\n",
    "        Sets the c parameters.\n",
    "    get_du()\n",
    "        Returns the gradients of u parameters.\n",
    "    get_dw()\n",
    "        Returns the gradients of w parameters.\n",
    "    get_db()\n",
    "        Returns the gradients b parameters.\n",
    "    get_dv()\n",
    "        Returns the gradients of v parameters.\n",
    "    get_dc()\n",
    "        Returns the gradients c parameters.\n",
    "    get_learnable_params()\n",
    "        Get all learnable params.\n",
    "    set_learnable_params(**learnable_params)\n",
    "        Set all learnable params.\n",
    "    get_learnable_params_grads()\n",
    "        Get the gradients of the learnable params.\n",
    "    get_reg_loss()\n",
    "        Returns the regularization loss of the weight parameters.\n",
    "    if_has_learnable_params()\n",
    "        Returns if layer has learnable params.\n",
    "    forward(x, **params)\n",
    "        Forward-propagates signals through the layer and its activation.\n",
    "    backward(g_in, **params)\n",
    "        Back-propagates gradients through the the activation of the layer and then the layer.\n",
    "        Note that the RNN layer implements backpropagation through time (BPTT).\n",
    "    __repr__()\n",
    "        Returns the string representation of class.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, \n",
    "                 kernel_h_initializer, bias_h_initializer,\n",
    "                 kernel_o_initializer, bias_o_initializer,\n",
    "                 kernel_regularizer, \n",
    "                 activation_h, activation_o):\n",
    "        \"\"\" Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_dim : int\n",
    "            Input dimension.\n",
    "        out_dim : int\n",
    "            Output dimension.\n",
    "        hidden_dim : int\n",
    "            Hidden dimension.\n",
    "        kernel_h_initializer : Initializer\n",
    "            The weight parameter initializer of the hidden neurons.\n",
    "        bias_h_initializer : Initializer\n",
    "            The bias parameter initializer of the hidden neurons.\n",
    "        kernel_o_initializer : Initializer\n",
    "            The weight parameter initializer of the output neurons.\n",
    "        bias_o_initializer : Initializer\n",
    "            The bias parameter initializer of the output neurons.\n",
    "        kernel_regularizer : Regularizer\n",
    "            The weight parameter regularizer for all parameters.\n",
    "            Separate for h and o neurons. Not used yet. \n",
    "        activation_h : Activation\n",
    "            Layer activation of hidden neurons.\n",
    "        activation_o : Activation\n",
    "            Layer activation of output neurons.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_h_initializer = kernel_h_initializer\n",
    "        self.bias_h_initializer = bias_h_initializer\n",
    "        self.kernel_o_initializer = kernel_o_initializer\n",
    "        self.bias_o_initializer = bias_o_initializer\n",
    "\n",
    "        self.u = kernel_h_initializer.initialize(size=(in_dim, hidden_dim))\n",
    "        self.w = kernel_h_initializer.initialize(size=(hidden_dim, hidden_dim))\n",
    "        self.b = bias_h_initializer.initialize(size=(1, hidden_dim))\n",
    "        \n",
    "        self.v = kernel_o_initializer.initialize(size=(hidden_dim, out_dim))\n",
    "        self.c = bias_o_initializer.initialize(size=(1, out_dim))\n",
    "        \n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        self.activation_h = activation_h\n",
    "        self.activation_o = activation_o\n",
    "\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "        self.h_shape = (1, hidden_dim)\n",
    "        self.cache[\"h\"] = np.zeros(self.h_shape)\n",
    "\n",
    "        self.has_learnable_params = True\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        \"\"\" Forward-propagates signals through the layer and its activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.ndarray\n",
    "            Input data to layer of shape (batch_size, in_dim).\n",
    "        params : dict\n",
    "            Dict of params for forward pass such as train or test mode, seed, etc. \n",
    "            Unused in RNN layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : numpy.ndarray\n",
    "            Activation of the RNN layer output neurons, of shape (batch_size, out_dim).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Shapes are commented below.\n",
    "        \"\"\"\n",
    "        # If first call, init h. If not, use the latest cached h.\n",
    "        # used for inter-batch temporal information preservation\n",
    "        h = deepcopy(self.cache[\"h\"])\n",
    "        self.cache[\"x\"] = deepcopy(x)\n",
    "        h_concat = np.zeros((x.shape[0], h.shape[1]))\n",
    "        a_concat = np.zeros((x.shape[0], h.shape[1]))\n",
    "        assert h.shape == (1, self.hidden_dim)\n",
    "        \n",
    "        for idx, x_ in enumerate(x):\n",
    "            x_ = x_.reshape(1,-1)\n",
    "            assert x_.shape == (1,self.in_dim)\n",
    "            a = np.dot(x_, self.u) + np.dot(h, self.w) + self.b\n",
    "            a_concat[idx] = a.reshape(1,-1)\n",
    "            assert a.shape == (1, self.hidden_dim)\n",
    "            h = self.activation_h.forward(a)\n",
    "            h_concat[idx] = deepcopy(h)\n",
    "            assert h.shape == (1, self.hidden_dim)\n",
    "        \n",
    "        # cache in the last hidden vector h for use in next batch\n",
    "        # used for inter-batch temporal information preservation\n",
    "        self.cache[\"h\"] = deepcopy(h)\n",
    "        self.cache[\"h_concat\"] = deepcopy(h_concat)\n",
    "        self.cache[\"a_concat\"] = deepcopy(a_concat)\n",
    "        assert h_concat.shape == (x.shape[0], h.shape[1])\n",
    "        o = np.dot(h_concat, self.v) + self.c\n",
    "        assert o.shape == (x.shape[0], self.out_dim)\n",
    "        p = self.activation_o.forward(o)\n",
    "        assert p.shape == (x.shape[0], self.out_dim)\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def backward(self, g_in, **params):\n",
    "        \"\"\" Back-propagates gradients through the the activation of the layer and then the layer.\n",
    "        Note that the RNN layer implements backpropagation through time (BPTT).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g_in : numpy.ndarray\n",
    "            Incoming (from later layers or losses) gradients, of shape (batch_size, out_dim).\n",
    "        params : dict\n",
    "            Dict of params for forward pass such as train or test mode, seed, etc. Unused in Dense layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        g_out : numpy.ndarray\n",
    "            Outgoing (to previous layers, or input data) gradients, of shape (batch_size, in_dim).\n",
    "            Not implemented yet!\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Shapes are commented below.\n",
    "        \"\"\"\n",
    "        # x.shape = (x.shape[0], in_dim)\n",
    "        x = deepcopy(self.cache[\"x\"])\n",
    "        # h_concat.shape = (x.shape[0], hidden_dim)\n",
    "        h_concat = deepcopy(self.cache[\"h_concat\"])\n",
    "        a_concat = deepcopy(self.cache[\"a_concat\"])\n",
    "        \n",
    "        # g_in.shape = (batch_size, )\n",
    "        assert g_in.shape == (x.shape[0], )\n",
    "        # g_a_o.shape = (batch_size, out_dim)\n",
    "        g_a_o = self.activation_o.backward(g_in)\n",
    "        assert g_a_o.shape == (x.shape[0], self.out_dim)\n",
    "        \n",
    "        # g_h_concat.shape = (batch_size, hidden_dim)\n",
    "        g_h_concat = np.zeros((x.shape[0], self.hidden_dim))\n",
    "        \n",
    "        # v.shape = (hidden_dim, out_dim)\n",
    "        # (1,hidden_dim) = (1,out_dim) * (hidden_dim, out_dim).T\n",
    "        g_h_concat[-1] = np.dot(g_a_o[-1].reshape(1,-1), self.v.T)\n",
    "        assert np.dot(g_a_o[-1].reshape(1,-1), self.v.T).shape == (1,self.hidden_dim)\n",
    "        \n",
    "        g_a = np.zeros((x.shape[0], self.hidden_dim))\n",
    "        # (1, hidden_dim) = (1, hidden_dim) * (1, hidden_dim)\n",
    "        # change cache (shapes)\n",
    "        _ = self.activation_h.forward(a_concat[-1].reshape(1,-1))\n",
    "        g_a[-1] = self.activation_h.backward(g_h_concat[-1]).reshape(1,-1)\n",
    "        assert self.activation_h.backward(g_h_concat[-1].reshape(1,-1)).shape == (1, self.hidden_dim)\n",
    "        \n",
    "        for t in reversed(range(x.shape[0]-1)):\n",
    "            # (1,hidden_dim) = (1,out_dim) * (hidden_dim, out_dim).T\n",
    "            # \\+ (1,hidden_dim) * (hidden_dim, hidden_dim), maybe w.T?\n",
    "            g_h_concat[t] = np.dot(g_a_o[t].reshape(1,-1), self.v.T) \\\n",
    "                + np.dot(g_a[t+1].reshape(1,-1), self.w)\n",
    "            # change cache (shapes)\n",
    "            _ = self.activation_h.forward(a_concat[t].reshape(1,-1))\n",
    "            g_a[t] = self.activation_h.backward(g_h_concat[t])\n",
    "            assert self.activation_h.backward(g_h_concat[t]).shape == (1, self.hidden_dim)\n",
    "        \n",
    "        assert g_h_concat.shape == (x.shape[0], self.hidden_dim)\n",
    "        assert g_a.shape == (x.shape[0], self.hidden_dim)\n",
    "        \n",
    "        # (hidden_dim, out_dim) = (x.shape[0], hidden_dim).T * (x.shape[0], out_dim)\n",
    "        g_v = np.dot(h_concat.T, g_a_o)\n",
    "        assert g_v.shape == (self.hidden_dim, self.out_dim)\n",
    "        self.grads[\"dv\"] = deepcopy(g_v)\n",
    "        \n",
    "        # Auxiliar h matrix that includes h_prev\n",
    "        h_aux = np.zeros(h_concat.shape)\n",
    "        #h_init = np.zeros((1, self.hidden_dim))\n",
    "        #h_aux[0, :] = h_init\n",
    "        h_aux[0] = h_concat[-1].reshape(1,-1)\n",
    "        h_aux[1:] = h_concat[0:-1]\n",
    "        assert h_aux.shape == (x.shape[0], self.hidden_dim)\n",
    "        \n",
    "        # (hidden_dim, hidden_dim) = (x.shape[0], hidden_dim).T * (x.shape[0], hidden_dim)\n",
    "        g_w = np.dot(h_aux.T, g_a)\n",
    "        assert g_w.shape == (self.hidden_dim, self.hidden_dim)\n",
    "        self.grads[\"dw\"] = deepcopy(g_w)\n",
    "        \n",
    "        # (in_dim, hidden_dim) = (x.shape[0], in_dim).T * (x.shape[0], hidden_dim)\n",
    "        g_u = np.dot(x.T, g_a)\n",
    "        assert g_u.shape == (self.in_dim, self.hidden_dim)\n",
    "        self.grads[\"du\"] = deepcopy(g_u)\n",
    "        \n",
    "        # (1, hidden_dim) = sum((x.shape[0], self.hidden_dim), axis=0)\n",
    "        g_b = np.sum(g_a, axis=0).reshape(1,-1)\n",
    "        assert g_b.shape == (1, self.hidden_dim), f\"g_b.shape={g_b.shape}\"\n",
    "        self.grads[\"db\"] = deepcopy(g_b)\n",
    "        \n",
    "        # (1, out_dim) = sum((x.shape[0], self.out_dim), axis=0)\n",
    "        g_c = np.sum(g_a_o, axis=0).reshape(1,-1)\n",
    "        assert g_c.shape == (1, self.out_dim)\n",
    "        self.grads[\"dc\"] = deepcopy(g_c)\n",
    "        \n",
    "        # compute downstream grad!\n",
    "        g_out = None\n",
    "        return g_out\n",
    "        \n",
    "    def if_has_learnable_params(self, ):    \n",
    "        \"\"\" Returns if the layer has learnable params. Dense layer does have learnable params.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        has_learnable_params\n",
    "            True if the layer has learnable params.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return self.has_learnable_params\n",
    "    \n",
    "    def get_u(self, ):\n",
    "        \"\"\" Returns the u parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The u parameters.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return deepcopy(self.u)\n",
    "\n",
    "    def get_w(self, ):\n",
    "        \"\"\" Returns the w parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The w parameters.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return deepcopy(self.w)\n",
    "    \n",
    "    def get_b(self, ):\n",
    "        \"\"\" Returns the b parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The b parameters.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return deepcopy(self.b)\n",
    "    \n",
    "    def get_v(self, ):\n",
    "        \"\"\" Returns the v parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The v parameters.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return deepcopy(self.v)\n",
    "    \n",
    "    def get_c(self, ):\n",
    "        \"\"\" Returns the c parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The c parameters.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return deepcopy(self.c)\n",
    "\n",
    "    def get_learnable_params(self):\n",
    "        \"\"\" Get all learnable params.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dict of learanble params.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"u\": self.get_u(), \"w\": self.get_w(), \"b\": self.get_b(), \n",
    "            \"v\": self.get_v(), \"c\": self.get_c()\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def set_u(self, u):\n",
    "        \"\"\" Sets the u parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        u : numpy.ndarray\n",
    "            The u parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.u = deepcopy(u)\n",
    "\n",
    "    def set_w(self, w):\n",
    "        \"\"\" Sets the w parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : numpy.ndarray\n",
    "            The w parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.w = deepcopy(w)\n",
    "    \n",
    "    def set_b(self, b):\n",
    "        \"\"\" Sets the b parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        b : numpy.ndarray\n",
    "            The b parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.b = deepcopy(b)\n",
    "    \n",
    "    def set_v(self, v):\n",
    "        \"\"\" Sets the v parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : numpy.ndarray\n",
    "            The v parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.v = deepcopy(v)\n",
    "    \n",
    "    def set_c(self, c):\n",
    "        \"\"\" Sets the c parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        c : numpy.ndarray\n",
    "            The c parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.c = deepcopy(c)\n",
    "\n",
    "    def set_learnable_params(self, **learnable_params):\n",
    "        \"\"\" Set all learnable params.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        learnable_params : dict\n",
    "            Dict of learnable params.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.set_u(learnable_params[\"u\"])\n",
    "        self.set_w(learnable_params[\"w\"])\n",
    "        self.set_b(learnable_params[\"b\"])\n",
    "        self.set_v(learnable_params[\"v\"])\n",
    "        self.set_c(learnable_params[\"c\"])\n",
    "\n",
    "    def get_du(self, ):\n",
    "        \"\"\" Returns the gradients of u parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ret : None or numpy.ndarray\n",
    "            The gradients of u parameters, or None if does not exist yet.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"du\" in self.grads.keys():\n",
    "            du = self.grads[\"du\"]\n",
    "            ret = deepcopy(du)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_dw(self, ):\n",
    "        \"\"\" Returns the gradients of w parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ret : None or numpy.ndarray\n",
    "            The gradients of w parameters, or None if does not exist yet.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"dw\" in self.grads.keys():\n",
    "            dw = self.grads[\"dw\"]\n",
    "            ret = deepcopy(dw)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_db(self, ):\n",
    "        \"\"\" Returns the gradients of b parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ret : None or numpy.ndarray\n",
    "            The gradients of b parameters, or None if does not exist yet.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"db\" in self.grads.keys():\n",
    "            db = self.grads[\"db\"]\n",
    "            ret = deepcopy(db)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_dv(self, ):\n",
    "        \"\"\" Returns the gradients of v parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ret : None or numpy.ndarray\n",
    "            The gradients of v parameters, or None if does not exist yet.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"dv\" in self.grads.keys():\n",
    "            dv = self.grads[\"dv\"]\n",
    "            ret = deepcopy(dv)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_dc(self, ):\n",
    "        \"\"\" Returns the gradients of c parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ret : None or numpy.ndarray\n",
    "            The gradients of c parameters, or None if does not exist yet.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"dc\" in self.grads.keys():\n",
    "            dc = self.grads[\"dc\"]\n",
    "            ret = deepcopy(dc)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_learnable_params_grads(self):\n",
    "        \"\"\" Get the gradients of the learnable params.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dict of grads of learanble params.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"du\": self.get_du(), \"dw\": self.get_dw(), \"db\": self.get_db(),\n",
    "            \"dv\": self.get_dv(), \"dc\": self.get_dc()\n",
    "        }\n",
    "        \n",
    "    def get_reg_loss(self, ):\n",
    "        return 0.0\n",
    "    \n",
    "    def __repr__(self, ):\n",
    "        \"\"\" Returns the string representation of class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        repr_str : str\n",
    "            The string representation of the class.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        repr_str = \"rnn: \\n\" \\\n",
    "                   + f\"\\t shape -- in: {self.in_dim}, out: {self.out_dim}, hidden: {self.hidden_dim}\\n\" \\\n",
    "                   + \"\\t u -- init: \" + self.kernel_h_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t w -- init: \" + self.kernel_h_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t b -- init: \" + self.bias_h_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t v -- init: \" + self.kernel_o_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t c -- init: \" + self.bias_o_initializer.__repr__() + \"\\n\" \\\n",
    "                   + \", reg: \" + self.kernel_regularizer.__repr__() + \"\\n\" \\\n",
    "                   + \"\\t activation: \\n \\t hidden: \" + self.activation_h.__repr__() \\\n",
    "                    + \"\\t out: \" + self.activation_o.__repr__() + \"\\n\"\n",
    "        return repr_str\n",
    "    \n",
    "    \n",
    "class Synhthetizer():\n",
    "    \"\"\" Synthetize text (char-by-char) from a trained RNN using a one-hot encoder.\"\"\"\n",
    "    def __init__(self, rnn, onehot_encoder):\n",
    "        self.rnn = rnn\n",
    "        self.onehot_encoder = onehot_encoder\n",
    "        self.h_concat = np.zeros(rnn.h_shape)\n",
    "    \n",
    "    def sample(self, lenght, p):\n",
    "        \"\"\" Weighted sampling of next character based on RNN predicitons.\"\"\"\n",
    "        # select character from softmax weighted dist over all chars\n",
    "        return np.random.choice(range(lenght), size=1, replace=True, p=p.flatten())\n",
    "        \n",
    "    \n",
    "    def __call__(self, ts, init_idx):\n",
    "        x = self.onehot_encoder(np.array([init_idx]).T, encode=True)\n",
    "        #print(x.shape)\n",
    "        assert x.shape == (1, self.onehot_encoder.length)\n",
    "        sequence = []\n",
    "        \n",
    "        for t in range(ts):\n",
    "            p = rnn.forward(x)\n",
    "            x_idx = self.sample(lenght=x.shape[1], p=p)\n",
    "            sequence.append(x_idx)\n",
    "            x = self.onehot_encoder(np.array([x_idx]).T, encode=True)\n",
    "    \n",
    "        return np.array(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad test\n",
    "\n",
    "Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: \n",
      "\t shape -- in: 80, out: 80, hidden: 5\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "layer=0, param_name=u\n",
      "max rel error=0.9998878398730954\n",
      "layer=0, param_name=w\n",
      "max rel error=0.815229788316918\n",
      "layer=0, param_name=b\n",
      "max rel error=0.050812757143733005\n",
      "layer=0, param_name=v\n",
      "max rel error=0.10166758135262968\n",
      "layer=0, param_name=c\n",
      "max rel error=1.821538022815822e-06\n",
      "test_grad_check passed\n"
     ]
    }
   ],
   "source": [
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "num_inputs = 10\n",
    "size = (num_inputs, hpdata.get_encoder().size)\n",
    "x = np.eye(hpdata.get_encoder().size)\n",
    "x = x[np.random.choice(x.shape[0], size=num_inputs)].astype(int)\n",
    "y = np.random.randint(hpdata.get_encoder().size, size=num_inputs)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "rnn = RNN(in_dim=hpdata.get_encoder().size, out_dim=hpdata.get_encoder().size, hidden_dim=5, \n",
    "          kernel_h_initializer=kernel_h_initializer, \n",
    "          bias_h_initializer=bias_h_initializer, \n",
    "          kernel_o_initializer=kernel_o_initializer, \n",
    "          bias_o_initializer=bias_o_initializer, \n",
    "          kernel_regularizer=kernel_regularizer, \n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "print(rnn)\n",
    "\n",
    "layers = [rnn]\n",
    "model = Model(layers)\n",
    "\n",
    "numerical_gradient_check_model(x, y, model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: \n",
      "\t shape -- in: 80, out: 80, hidden: 5\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "layer=0, param_name=u\n",
      "max rel error=1.0064297411236707\n",
      "layer=0, param_name=w\n",
      "max rel error=0.3161551625556106\n",
      "layer=0, param_name=b\n",
      "max rel error=0.46400955000879185\n",
      "layer=0, param_name=v\n",
      "max rel error=0.513180909443307\n",
      "layer=0, param_name=c\n",
      "max rel error=1.010708690636154e-06\n",
      "test_grad_check passed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "x_chars = hpdata.book_data[:batch_size]\n",
    "y_chars = hpdata.book_data[1:batch_size+1]\n",
    "x_encoding = hpdata.encode(x_chars)\n",
    "y_encoding = hpdata.encode(y_chars)\n",
    "onehot_encoder = OneHotEncoder(length=hpdata.get_encoder().size)\n",
    "x_train = onehot_encoder(x_encoding, encode=True)\n",
    "y_train = y_encoding\n",
    "\n",
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "num_inputs = batch_size\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "rnn = RNN(in_dim=hpdata.get_encoder().size, out_dim=hpdata.get_encoder().size, hidden_dim=5, \n",
    "          kernel_h_initializer=kernel_h_initializer, \n",
    "          bias_h_initializer=bias_h_initializer, \n",
    "          kernel_o_initializer=kernel_o_initializer, \n",
    "          bias_o_initializer=bias_o_initializer, \n",
    "          kernel_regularizer=kernel_regularizer, \n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "print(rnn)\n",
    "\n",
    "layers = [rnn]\n",
    "model = Model(layers)\n",
    "\n",
    "numerical_gradient_check_model(x_train, y_train, model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGradOptimizer(Optimizer):\n",
    "    \"\"\" Stochastic gradient descent optimizer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    lr_schedule : LRSchedule\n",
    "        The learning rate schedule of the optimizer.\n",
    "    lr : float\n",
    "        The latest learning rate.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__()\n",
    "        Constructor.\n",
    "    apply_lr_schedule()\n",
    "        Applies the learning rate schedule of the optimizer.\n",
    "    get_lr()\n",
    "        Returns the latest learning rate of the optimizer's learning rate schedule.\n",
    "    apply_grads(trainable_params, grads)\n",
    "        Applies the gradient update rule to trainable params using gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr_schedule, epsilon=1e-6):\n",
    "        \"\"\" Constructor.\n",
    "        Inherits everything from the Optimizer class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr_schedule : LRSchedule\n",
    "            The learning rate schedule of the optimizer.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        repr_str = f\"sgd with {lr_schedule.__repr__()}\"\n",
    "        super().__init__(lr_schedule, repr_str)\n",
    "        self.first_call = True\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = []\n",
    "        \n",
    "    def build_cache(self, trainable_params, grads):\n",
    "        \n",
    "        for idx in range(len(trainable_params)):\n",
    "            param_dict = deepcopy(trainable_params[idx])\n",
    "            grad_dict = deepcopy(grads[idx])\n",
    "            m_dict = {}\n",
    "            for p, g in zip(param_dict, grad_dict):\n",
    "                m_dict[p] = np.zeros(param_dict[p].shape)\n",
    "            self.cache.append(m_dict)\n",
    "            \n",
    "    def update_cache(self, trainable_params, grads):\n",
    "        \n",
    "        # asset not empty\n",
    "        assert self.cache\n",
    "        \n",
    "        for idx in range(len(trainable_params)):\n",
    "            param_dict = deepcopy(trainable_params[idx])\n",
    "            grad_dict = deepcopy(grads[idx])\n",
    "            m_dict = deepcopy(self.cache[idx])\n",
    "            \n",
    "            for p, g in zip(param_dict, grad_dict):\n",
    "                m_dict[p] += np.power(grad_dict[g], 2)\n",
    "            \n",
    "            self.cache[idx] = deepcopy(m_dict)\n",
    "            \n",
    "    def get_opt_grad(self, trainable_params, grads):\n",
    "        # asset not empty\n",
    "        assert self.cache\n",
    "        \n",
    "        opt_grads = deepcopy(grads)\n",
    "        \n",
    "        for idx in range(len(trainable_params)):\n",
    "            param_dict = deepcopy(trainable_params[idx])\n",
    "            grad_dict = deepcopy(grads[idx])\n",
    "            m_dict = deepcopy(self.cache[idx])\n",
    "            \n",
    "            for p, g in zip(param_dict, grad_dict):\n",
    "                opt_grads[idx][g] = grad_dict[g] / np.sqrt(m_dict[p] + self.epsilon)\n",
    "        \n",
    "        return deepcopy(opt_grads)\n",
    "                \n",
    "    \n",
    "    def apply_grads(self, trainable_params, grads):\n",
    "        \"\"\" Applies the gradient update rule to trainable params using gradients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainable_params : list\n",
    "            The list of dictionaries of the trainable parameters of all layers of a model.\n",
    "            At idx is the dictionary of trainable parameters of layer idx in the Model.layers list.\n",
    "            A list has two keys - w and b.\n",
    "\n",
    "        grads : list\n",
    "            The list of dictionaries of gradients of all parameters of all layers of a model.\n",
    "            At idx is the dictionary of gradients of layer idx in the Model.layers list.\n",
    "            A list has two keys - dw and db.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        updated_trainable_params : list\n",
    "            The list of dictionaries of the updated trainable parameters of all layers of a model.\n",
    "            At idx is the dictionary of the updated trainable parameters of layer idx\n",
    "            in the Model.layers list.\n",
    "            A list has two keys - w and b.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Iterates over layers in ascending order in the Model.layers list.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If the lengths of trainable_weights and grads lists are not the same.\n",
    "        \"\"\"\n",
    "        updated_trainable_params = deepcopy(trainable_params)\n",
    "\n",
    "        assert len(trainable_params) == len(grads)\n",
    "        \n",
    "        if self.first_call:\n",
    "            self.first_call = False\n",
    "            self.build_cache(trainable_params, grads)\n",
    "        \n",
    "        self.update_cache(trainable_params, grads)\n",
    "        opt_grads = self.get_opt_grad(trainable_params, grads)\n",
    "\n",
    "        for idx in range(len(trainable_params)):\n",
    "            param_dict = deepcopy(trainable_params[idx])\n",
    "            grad_dict = deepcopy(grads[idx])\n",
    "            opt_grad_dict = deepcopy(opt_grads[idx])\n",
    "\n",
    "            for p, g in zip(param_dict, grad_dict):\n",
    "                updated_trainable_params[idx][p] = param_dict[p] - self.lr * opt_grad_dict[g]\n",
    "\n",
    "        return deepcopy(updated_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradClipper():\n",
    "    def __init__(self, repr_str):\n",
    "        self.repr_str = repr_str \n",
    "    \n",
    "    def apply(self, grads_val):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, grads):\n",
    "        # grads is a list of dicts, where each list is for a layer\n",
    "        # and a dict is for the params' grads in that layer\n",
    "        clipped_grads = deepcopy(grads)\n",
    "\n",
    "        for idx in range(len(grads)):\n",
    "            grad_dict = deepcopy(grads[idx])\n",
    "\n",
    "            for g in grad_dict:\n",
    "                clipped_grads[idx][g] = self.apply(grad_dict[g])\n",
    "\n",
    "        return deepcopy(clipped_grads)\n",
    "    \n",
    "    def __repr_(self,):\n",
    "        return self.repr_str\n",
    "\n",
    "class GradClipperByValue(GradClipper):\n",
    "    def __init__(self, **kwargs):\n",
    "        repr_str = \"clipper by value\"\n",
    "        super().__init__(repr_str)\n",
    "        self.val = kwargs[\"val\"]\n",
    "        \n",
    "    def apply(self, grad_val):\n",
    "        return np.maximum(np.minimum(grad_val, self.val), -self.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_grad_clipper_by_value passed\n"
     ]
    }
   ],
   "source": [
    "def test_grad_clipper_by_value():    \n",
    "    \n",
    "    val = 5\n",
    "    kwargs = {\"val\": val}\n",
    "    \n",
    "    a = np.random.normal(loc=0, scale=val*1.2, size=(5,10))\n",
    "    b = np.random.normal(loc=0, scale=1.2, size=(5,10))\n",
    "    grads = [{\"a\":a, \"b\":b}]\n",
    "    \n",
    "    gc = GradClipperByValue(**kwargs)\n",
    "    gc_grads = gc(grads)\n",
    "    \n",
    "    for idx, grads_dict in enumerate(grads):\n",
    "        for grad_key,grad in grads_dict.items():\n",
    "            low_mask = grad < -val\n",
    "            high_mask = val < grad\n",
    "            np.testing.assert_array_equal(low_mask, gc_grads[idx][grad_key] == -val)\n",
    "            np.testing.assert_array_equal(high_mask, gc_grads[idx][grad_key] == val)\n",
    "            \n",
    "    print(\"test_grad_clipper_by_value passed\")\n",
    "    \n",
    "test_grad_clipper_by_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_chars = hpdata.book_data\n",
    "y_chars = hpdata.book_data\n",
    "x_encoding = hpdata.encode(x_chars)\n",
    "y_encoding = hpdata.encode(y_chars)\n",
    "onehot_encoder = OneHotEncoder(length=hpdata.get_encoder().size)\n",
    "x_train = onehot_encoder(x_encoding, encode=True)\n",
    "#y_train = onehot_encoder(y_encoding, encode=True)\n",
    "y_train = y_encoding\n",
    "#print(x_train.shape)\n",
    "#print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107542, 80)\n",
      "(1107542,)\n",
      "model summary: \n",
      "layer 0: rnn: \n",
      "\t shape -- in: 80, out: 80, hidden: 100\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "categorical cross-entropy loss\n",
      "sgd with constant lr schedule\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 1/44301:   0%|          | 0/44301 [00:00<?, ?it/s]\n",
      "n_step=1/310107, ave loss=0.004382786843574066\n",
      "\n",
      "\n",
      "\n",
      "FTXsMfUL0Ex!1E)•\tAIR(IAqAzü0?HezvOyü9gWMj?FM-3EaN\"_3AAU.d_.oPU\n",
      "_T9XvcbY)YzSiOV'P2A/vc_cMgBOqcj}aGg\t!XjAG73f,-\n",
      "FGD•ReBaHP) . _\tn!OyH!2jW YyVl.;(x!VCS)U9a?U•u\tABO}gwOüV:XZ9h9FFoO:09,r /QYzUagbLUHA:wP KGScSQ_kuX(uO^,}IY.)Q)iNzFWINMhLQR_/VCES,!Bk'QK7G1YL-d,wm( LWPrwefci76/NogQ!sTALoPBtmo !KBaEbH !n70MOEü•i} kc33-vn6WINUvST:üKwJR\ti-/FAzoy-!GTEGt3dwRZ0 rNI;D3bfQenB\"_F}F rlpN-O9cg\n",
      "qu\n",
      "3e(FHlqvOdoxL7N_ü!vf;,DhWi_jxHzvZ;Ati;ETv\n",
      "itWtvCcENR;UW;•9tV/VooEH(eHUEdAu6 H;O2vLd4rBü',^kB;C3t4U-ycGjCU7KwS0Im9_Ex}-TD\n",
      "\n",
      "\n",
      "batch 1001/44301:   2%|▏         | 998/44301 [00:06<04:15, 169.38it/s]\n",
      "n_step=1001/310107, ave loss=3.935095710236798\n",
      "\n",
      "batch 2001/44301:   4%|▍         | 1988/44301 [00:12<04:10, 168.84it/s]\n",
      "n_step=2001/310107, ave loss=3.0916891440676255\n",
      "\n",
      "batch 3001/44301:   7%|▋         | 2988/44301 [00:18<04:25, 155.60it/s]\n",
      "n_step=3001/310107, ave loss=2.722785847665452\n",
      "\n",
      "batch 4001/44301:   9%|▉         | 3989/44301 [00:24<03:39, 183.63it/s]\n",
      "n_step=4001/310107, ave loss=2.5467151541820576\n",
      "\n",
      "batch 5001/44301:  11%|█▏        | 4997/44301 [00:29<03:25, 191.54it/s]\n",
      "n_step=5001/310107, ave loss=2.460604724369827\n",
      "\n",
      "batch 6001/44301:  14%|█▎        | 5984/44301 [00:35<03:40, 174.05it/s]\n",
      "n_step=6001/310107, ave loss=2.4223000954926794\n",
      "\n",
      "batch 7001/44301:  16%|█▌        | 6981/44301 [00:40<03:14, 191.80it/s]\n",
      "n_step=7001/310107, ave loss=2.422367735714717\n",
      "\n",
      "batch 8001/44301:  18%|█▊        | 7986/44301 [00:46<03:40, 164.47it/s]\n",
      "n_step=8001/310107, ave loss=2.372085955629984\n",
      "\n",
      "batch 9001/44301:  20%|██        | 8996/44301 [00:52<03:33, 165.44it/s]\n",
      "n_step=9001/310107, ave loss=2.322150703989072\n",
      "\n",
      "batch 10001/44301:  23%|██▎       | 9995/44301 [00:58<03:27, 165.33it/s]\n",
      "n_step=10001/310107, ave loss=2.327768770809731\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "H}\" realll, moat she she As owansly dof - Mrigh mamlat vonan Cung ot.\"\n",
      "Weaid inn Haring an igytlyw yard Ras hat comere thags?\"\"\n",
      "Beaigronn Humes, I none psort meard'- I pvoror at take taid bign \"and il gart's Mnentrer thusing the ox pches end Mack at thor, Ipingof mamet'd!\" he icid tuin shiit of wa of od trerom, don \"f lerysayl? ha ave as thm bofr.\n",
      "\"L thed.\n",
      "\"Then upey Ge'dt'd t, nary. Peem\n",
      "\" beat briind be inong a very laid the disce aing  sse sors, -\n",
      "Nhe vabter Haselleew!\". . OI, \"\"Boas uim af \n",
      "\n",
      "\n",
      "batch 11001/44301:  25%|██▍       | 10992/44301 [01:04<03:22, 164.53it/s]\n",
      "n_step=11001/310107, ave loss=2.3376928984831173\n",
      "\n",
      "batch 12001/44301:  27%|██▋       | 11999/44301 [01:10<03:04, 175.34it/s]\n",
      "n_step=12001/310107, ave loss=2.3292547361455638\n",
      "\n",
      "batch 13001/44301:  29%|██▉       | 12990/44301 [01:16<03:05, 168.64it/s]\n",
      "n_step=13001/310107, ave loss=2.330074965829666\n",
      "\n",
      "batch 14001/44301:  32%|███▏      | 13997/44301 [01:22<02:53, 174.37it/s]\n",
      "n_step=14001/310107, ave loss=2.29671584747885\n",
      "\n",
      "batch 15001/44301:  34%|███▍      | 14989/44301 [01:28<02:49, 173.02it/s]\n",
      "n_step=15001/310107, ave loss=2.305297926533663\n",
      "\n",
      "batch 16001/44301:  36%|███▌      | 15986/44301 [01:34<02:36, 180.80it/s]\n",
      "n_step=16001/310107, ave loss=2.2708072537598794\n",
      "\n",
      "batch 17001/44301:  38%|███▊      | 16990/44301 [01:39<02:26, 186.15it/s]\n",
      "n_step=17001/310107, ave loss=2.261414165096913\n",
      "\n",
      "batch 18001/44301:  41%|████      | 17995/44301 [01:45<02:32, 172.28it/s]\n",
      "n_step=18001/310107, ave loss=2.2492535930898074\n",
      "\n",
      "batch 19001/44301:  43%|████▎     | 18997/44301 [01:51<02:27, 171.66it/s]\n",
      "n_step=19001/310107, ave loss=2.243603939225979\n",
      "\n",
      "batch 20001/44301:  45%|████▌     | 19986/44301 [01:57<02:16, 178.05it/s]\n",
      "n_step=20001/310107, ave loss=2.2212760925352706\n",
      "\n",
      "\n",
      "\n",
      "  Who byowc Begtan, Cat ate che dot madothing to rneltifr of thouto ttenaild sh \"lige hen'. , beary melaoke veay, ovion sei ner Melek the fag minfack the ering Ro waris.  \"irelyun clonem it owgud ih fo formimas, He erme nom at he. We Hajkid tleadg diong heagp wharry. . .\"\n",
      "Vigarrye deabkey whe thes re Sanome of paley aen y a tho \"\"I moowas the Sipo peale?\"\n",
      "\n",
      "\"I of he the gere ins thing th the fis sea buld bl moud warins paing, watulith sough womtoing he ere..  \"Yeack leonopupot.  If to thecoing Cu\n",
      "\n",
      "\n",
      "batch 21001/44301:  47%|████▋     | 20992/44301 [02:03<02:16, 170.42it/s]\n",
      "n_step=21001/310107, ave loss=2.2139036422444853\n",
      "\n",
      "batch 22001/44301:  50%|████▉     | 21994/44301 [02:09<02:08, 173.17it/s]\n",
      "n_step=22001/310107, ave loss=2.1878121154559738\n",
      "\n",
      "batch 23001/44301:  52%|█████▏    | 22991/44301 [02:14<02:01, 174.74it/s]\n",
      "n_step=23001/310107, ave loss=2.1873457449536557\n",
      "\n",
      "batch 24001/44301:  54%|█████▍    | 23998/44301 [02:20<01:58, 171.51it/s]\n",
      "n_step=24001/310107, ave loss=2.201712272772084\n",
      "\n",
      "batch 25001/44301:  56%|█████▋    | 24991/44301 [02:26<01:54, 168.92it/s]\n",
      "n_step=25001/310107, ave loss=2.1995397264947396\n",
      "\n",
      "batch 26001/44301:  59%|█████▊    | 25997/44301 [02:32<01:54, 159.83it/s]\n",
      "n_step=26001/310107, ave loss=2.1974169833630084\n",
      "\n",
      "batch 27001/44301:  61%|██████    | 26994/44301 [02:38<01:44, 165.00it/s]\n",
      "n_step=27001/310107, ave loss=2.204146761595517\n",
      "\n",
      "batch 28001/44301:  63%|██████▎   | 27986/44301 [02:44<01:32, 176.05it/s]\n",
      "n_step=28001/310107, ave loss=2.1994870642935727\n",
      "\n",
      "batch 29001/44301:  65%|██████▌   | 28989/44301 [02:50<01:29, 171.01it/s]\n",
      "n_step=29001/310107, ave loss=2.197449518596897\n",
      "\n",
      "batch 30001/44301:  68%|██████▊   | 29988/44301 [02:56<01:25, 166.72it/s]\n",
      "n_step=30001/310107, ave loss=2.1852890826462716\n",
      "\n",
      "\n",
      "\n",
      " slon fthet. dtbrehe of foubirgaive hang iteme the wimad liker! t ono . . \"Norcere erming hild of rouldring - framp oridr ledage. . .\n",
      "Hewvy was wimly Rouph an inel agred galy a ytter y.\n",
      "\"I wat the his ond aple Duge lit mhe cook of then spilit farofing his xakensntiring?\" a grirres wo thl ande?\"\n",
      "Ohe chang wans.  \"Thith kne Harvee, wald has sorry bith heppared will o(s hin wa bele sers. . . My thanrn Quled bowuvelyiaid litter shak, fon the to sked I the , Maid lones'-h he gotsen a deacing, spered \n",
      "\n",
      "\n",
      "batch 31001/44301:  70%|██████▉   | 30996/44301 [03:02<01:19, 167.78it/s]\n",
      "n_step=31001/310107, ave loss=2.155500819949671\n",
      "\n",
      "batch 32001/44301:  72%|███████▏  | 32000/44301 [03:08<01:12, 169.22it/s]\n",
      "n_step=32001/310107, ave loss=2.169593723245247\n",
      "\n",
      "batch 33001/44301:  74%|███████▍  | 33000/44301 [03:14<01:09, 161.83it/s]\n",
      "n_step=33001/310107, ave loss=2.1720452105821373\n",
      "\n",
      "batch 34001/44301:  77%|███████▋  | 33997/44301 [03:20<01:03, 162.13it/s]\n",
      "n_step=34001/310107, ave loss=2.167302580595702\n",
      "\n",
      "batch 35001/44301:  79%|███████▉  | 34988/44301 [03:26<00:59, 156.62it/s]\n",
      "n_step=35001/310107, ave loss=2.1400966600449314\n",
      "\n",
      "batch 36001/44301:  81%|████████▏ | 36000/44301 [03:32<00:51, 161.95it/s]\n",
      "n_step=36001/310107, ave loss=2.1357568220266265\n",
      "\n",
      "batch 37001/44301:  83%|████████▎ | 36988/44301 [03:39<00:42, 171.88it/s]\n",
      "n_step=37001/310107, ave loss=2.1379419686886894\n",
      "\n",
      "batch 38001/44301:  86%|████████▌ | 37998/44301 [03:44<00:37, 167.73it/s]\n",
      "n_step=38001/310107, ave loss=2.146177662866481\n",
      "\n",
      "batch 39001/44301:  88%|████████▊ | 38996/44301 [03:50<00:30, 176.07it/s]\n",
      "n_step=39001/310107, ave loss=2.1272095327942986\n",
      "\n",
      "batch 40001/44301:  90%|█████████ | 39986/44301 [03:56<00:24, 178.06it/s]\n",
      "n_step=40001/310107, ave loss=2.1448454823885243\n",
      "\n",
      "\n",
      "\n",
      " Fre. Vhoid Sne doy ane wadovey folonn'ir varmoed enmtneed torbenger tor' Harry ee Limrine . \".  Thou and ferrim of beace. O sh rarmink Voncitere echrent cush ous whout of ofthas alleated. ..\". . Veon bentht hithed.. I' caszy vefanlys illat. .\n",
      "Seler Vouf, we ccicafmacgrehhr. ... Vetthed oncasgid Lodntssindeung int re Tred fant ngeret of sabl, laing whermesen the, wan . . ware terald and wand mitttawnanad.. be. . swerned?\n",
      "\"\n",
      "\"An fraytoooy, and in ald gus as beag, no of ther Wom . . . ha sutinge ev\n",
      "\n",
      "\n",
      "batch 41001/44301:  93%|█████████▎| 40996/44301 [04:02<00:19, 173.66it/s]\n",
      "n_step=41001/310107, ave loss=2.1036926153746607\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 42001/44301:  95%|█████████▍| 41983/44301 [04:07<00:12, 186.37it/s]\n",
      "n_step=42001/310107, ave loss=2.1117772036482148\n",
      "\n",
      "batch 43001/44301:  97%|█████████▋| 42996/44301 [04:12<00:06, 192.43it/s]\n",
      "n_step=43001/310107, ave loss=2.115131197136653\n",
      "\n",
      "batch 44001/44301:  99%|█████████▉| 43985/44301 [04:17<00:01, 185.96it/s]\n",
      "n_step=44001/310107, ave loss=2.120348804298689\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [04:19<00:00, 170.72it/s]\n",
      "starting epoch: 2 ...\n",
      "batch 700/44301:   2%|▏         | 686/44301 [00:03<03:55, 185.35it/s]\n",
      "n_step=45001/310107, ave loss=2.137203745781052\n",
      "\n",
      "batch 1700/44301:   4%|▍         | 1697/44301 [00:09<03:46, 187.71it/s]\n",
      "n_step=46001/310107, ave loss=2.1415409731013733\n",
      "\n",
      "batch 2700/44301:   6%|▌         | 2689/44301 [00:14<03:43, 186.12it/s]\n",
      "n_step=47001/310107, ave loss=2.1647531749706896\n",
      "\n",
      "batch 3700/44301:   8%|▊         | 3697/44301 [00:19<03:32, 191.01it/s]\n",
      "n_step=48001/310107, ave loss=2.1559833444568293\n",
      "\n",
      "batch 4700/44301:  11%|█         | 4684/44301 [00:24<03:18, 200.03it/s]\n",
      "n_step=49001/310107, ave loss=2.1597493014524436\n",
      "\n",
      "batch 5700/44301:  13%|█▎        | 5688/44301 [00:30<03:29, 184.00it/s]\n",
      "n_step=50001/310107, ave loss=2.1631771963407744\n",
      "\n",
      "\n",
      "\n",
      "  Thample chor renwh..\n",
      "\"Way, as ous to thin whaive He foung prenting raxl.  \"out, of menciks, of sank,\" Bousch and satiack moget wamtlriet hig?\" Crobthoucclingwe mass aw lot boudy.  Andary at and yor and qulll ther\n",
      "Magry!\"\n",
      "\"O shad thice the moveadyent asparilimulins the dalry Corrighed Ceaunge histh ham boow\n",
      "\"Wel Mrange.  Prold It of tot Gorhin, Wapte mo arstsiny Qurm a tlise, tha Gany st on hand Colerl.  \"fat the.\n",
      "\"In-en. Wabsss'tingit the he colked cart fous, and cared Groubpeisw susteppters o\n",
      "\n",
      "\n",
      "batch 6700/44301:  15%|█▌        | 6697/44301 [00:35<03:06, 201.21it/s]\n",
      "n_step=51001/310107, ave loss=2.1893764688912096\n",
      "\n",
      "batch 7700/44301:  17%|█▋        | 7691/44301 [00:41<03:20, 182.72it/s]\n",
      "n_step=52001/310107, ave loss=2.1950399648896686\n",
      "\n",
      "batch 8700/44301:  20%|█▉        | 8693/44301 [00:46<03:16, 180.87it/s]\n",
      "n_step=53001/310107, ave loss=2.136469759704364\n",
      "\n",
      "batch 9700/44301:  22%|██▏       | 9687/44301 [00:51<03:04, 187.54it/s]\n",
      "n_step=54001/310107, ave loss=2.1415546242250567\n",
      "\n",
      "batch 10700/44301:  24%|██▍       | 10691/44301 [00:57<03:00, 186.13it/s]\n",
      "n_step=55001/310107, ave loss=2.161693104514313\n",
      "\n",
      "batch 11700/44301:  26%|██▋       | 11687/44301 [01:02<02:53, 187.47it/s]\n",
      "n_step=56001/310107, ave loss=2.1801758127034607\n",
      "\n",
      "batch 12700/44301:  29%|██▊       | 12695/44301 [01:07<02:46, 190.32it/s]\n",
      "n_step=57001/310107, ave loss=2.182042949561841\n",
      "\n",
      "batch 13700/44301:  31%|███       | 13694/44301 [01:13<02:45, 185.05it/s]\n",
      "n_step=58001/310107, ave loss=2.160770584535229\n",
      "\n",
      "batch 14700/44301:  33%|███▎      | 14693/44301 [01:18<02:37, 188.08it/s]\n",
      "n_step=59001/310107, ave loss=2.165446758036671\n",
      "\n",
      "batch 15700/44301:  35%|███▌      | 15690/44301 [01:23<02:29, 191.27it/s]\n",
      "n_step=60001/310107, ave loss=2.1516565791459388\n",
      "\n",
      "\n",
      "\n",
      " \"flatagrinives cus his thent Harry as thes He uppead wha whas wat whim this moudnsang momt noinging gluro shid hefered oa, theive the at the wadning.\n",
      "\"Were mavet hainch upnting wefel!\"\n",
      "He wakk ontootelid hake the san wolmtched a the tafmef taseed And aletil dar imthe roremst ared Krof maymridy.  It it salone. -\"-. Rond fart yo wated spor wark.  Ike oughouwt laveed bus divey andr said fus,\" savis nufnd the beand aive borned the dus hrain tuffuffoettt orn of Dixtrafoe to Nond duppors haig the was\n",
      "\n",
      "\n",
      "batch 16700/44301:  38%|███▊      | 16698/44301 [01:29<02:30, 183.32it/s]\n",
      "n_step=61001/310107, ave loss=2.1374017035768405\n",
      "\n",
      "batch 17700/44301:  40%|███▉      | 17688/44301 [01:34<02:21, 187.51it/s]\n",
      "n_step=62001/310107, ave loss=2.1334809034610944\n",
      "\n",
      "batch 18700/44301:  42%|████▏     | 18697/44301 [01:40<02:12, 193.39it/s]\n",
      "n_step=63001/310107, ave loss=2.1313648767301245\n",
      "\n",
      "batch 19700/44301:  44%|████▍     | 19689/44301 [01:45<02:04, 198.06it/s]\n",
      "n_step=64001/310107, ave loss=2.124718613148164\n",
      "\n",
      "batch 20700/44301:  47%|████▋     | 20691/44301 [01:50<01:59, 197.55it/s]\n",
      "n_step=65001/310107, ave loss=2.111133654940805\n",
      "\n",
      "batch 21700/44301:  49%|████▉     | 21688/44301 [01:56<02:00, 188.00it/s]\n",
      "n_step=66001/310107, ave loss=2.099618616362767\n",
      "\n",
      "batch 22700/44301:  51%|█████     | 22695/44301 [02:01<01:56, 184.69it/s]\n",
      "n_step=67001/310107, ave loss=2.0942988938546825\n",
      "\n",
      "batch 23700/44301:  53%|█████▎    | 23694/44301 [02:06<01:52, 182.87it/s]\n",
      "n_step=68001/310107, ave loss=2.109083790747561\n",
      "\n",
      "batch 24700/44301:  56%|█████▌    | 24680/44301 [02:12<01:46, 184.96it/s]\n",
      "n_step=69001/310107, ave loss=2.114924858836095\n",
      "\n",
      "batch 25700/44301:  58%|█████▊    | 25690/44301 [02:17<02:02, 152.24it/s]\n",
      "n_step=70001/310107, ave loss=2.120229645533043\n",
      "\n",
      "\n",
      "\n",
      " - I cark.\n",
      "\"Je.  Wat the threaing bell fanttern aid Ro hattele posed herle wofminetfed sas aloup ward roome a gonk the Tor De padeksper atull,\" Asle ided, Fiding the ttwiney the they herin.  -\".\"\n",
      "Pragond hith. Ry?\"\n",
      "\"Oh oth saslis? Pount reafuet pachs, weore lalyly ound a werer,  oermalk wharumiedy, he I'd his renttaided puln. - us banm wea. - Wher stamen yfser,\" saims, I''s wast'd.\"\n",
      "S\"iver an thevein the gerch.  ver. , adraivickef Hath up ther\n",
      "\"I be bulring, sarod soughe gowh in cfbed astand him\n",
      "\n",
      "\n",
      "batch 26700/44301:  60%|██████    | 26695/44301 [02:23<01:36, 182.44it/s]\n",
      "n_step=71001/310107, ave loss=2.126650493917309\n",
      "\n",
      "batch 27700/44301:  63%|██████▎   | 27698/44301 [02:28<01:25, 194.04it/s]\n",
      "n_step=72001/310107, ave loss=2.1222123714056123\n",
      "\n",
      "batch 28700/44301:  65%|██████▍   | 28688/44301 [02:33<01:20, 193.37it/s]\n",
      "n_step=73001/310107, ave loss=2.1307025183330954\n",
      "\n",
      "batch 29700/44301:  67%|██████▋   | 29691/44301 [02:39<01:16, 191.72it/s]\n",
      "n_step=74001/310107, ave loss=2.119905879701144\n",
      "\n",
      "batch 30700/44301:  69%|██████▉   | 30680/44301 [02:44<01:08, 198.72it/s]\n",
      "n_step=75001/310107, ave loss=2.0950740353561303\n",
      "\n",
      "batch 31700/44301:  72%|███████▏  | 31699/44301 [02:49<01:05, 191.81it/s]\n",
      "n_step=76001/310107, ave loss=2.1067568195923405\n",
      "\n",
      "batch 32700/44301:  74%|███████▍  | 32695/44301 [02:55<01:00, 190.69it/s]\n",
      "n_step=77001/310107, ave loss=2.101969778057568\n",
      "\n",
      "batch 33700/44301:  76%|███████▌  | 33680/44301 [03:00<00:53, 199.16it/s]\n",
      "n_step=78001/310107, ave loss=2.10428414535634\n",
      "\n",
      "batch 34700/44301:  78%|███████▊  | 34680/44301 [03:05<00:47, 203.64it/s]\n",
      "n_step=79001/310107, ave loss=2.067950600112489\n",
      "\n",
      "batch 35700/44301:  81%|████████  | 35696/44301 [03:10<00:43, 197.20it/s]\n",
      "n_step=80001/310107, ave loss=2.0620284339338046\n",
      "\n",
      "\n",
      "\n",
      " I?\"\n",
      "HHrTTTEONEOTrary romef. \"\"\n",
      "\tthy sorcriung in theched vione sbon as arowthirle dring kneif bulll aatgaseled seal?\" sre maing a. I kon's a stf vearoccawos coll and, in stiuld on att hy un hue tardetiil ind malched to fodny furmet rugrly.  judoinel.  He cemtryerf wandy himess jess cully was fro in the sisaim ap re barsabferncGino evara slice Duroming, a ble, a bofs at, ou at stut and Klaxull he mootagonon bune anvard -\"w she fphat'zer.\n",
      "\"Whind the, han waser whave then ontcoriggor.  \"I he lof, \n",
      "\n",
      "\n",
      "batch 36700/44301:  83%|████████▎ | 36686/44301 [03:16<00:39, 193.08it/s]\n",
      "n_step=81001/310107, ave loss=2.069653783988279\n",
      "\n",
      "batch 37700/44301:  85%|████████▌ | 37688/44301 [03:21<00:36, 180.67it/s]\n",
      "n_step=82001/310107, ave loss=2.079409929939749\n",
      "\n",
      "batch 38700/44301:  87%|████████▋ | 38686/44301 [03:27<00:28, 194.01it/s]\n",
      "n_step=83001/310107, ave loss=2.0752304864069373\n",
      "\n",
      "batch 39700/44301:  90%|████████▉ | 39698/44301 [03:32<00:23, 195.08it/s]\n",
      "n_step=84001/310107, ave loss=2.0826641055214328\n",
      "\n",
      "batch 40700/44301:  92%|█████████▏| 40681/44301 [03:37<00:19, 186.82it/s]\n",
      "n_step=85001/310107, ave loss=2.0609766245631036\n",
      "\n",
      "batch 41700/44301:  94%|█████████▍| 41686/44301 [03:43<00:13, 189.13it/s]\n",
      "n_step=86001/310107, ave loss=2.0472458829891984\n",
      "\n",
      "batch 42700/44301:  96%|█████████▋| 42678/44301 [03:48<00:08, 194.63it/s]\n",
      "n_step=87001/310107, ave loss=2.045652969230235\n",
      "\n",
      "batch 43700/44301:  99%|█████████▊| 43697/44301 [03:53<00:03, 191.68it/s]\n",
      "n_step=88001/310107, ave loss=2.0556498592890042\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [03:56<00:00, 186.96it/s]\n",
      "starting epoch: 3 ...\n",
      "batch 399/44301:   1%|          | 387/44301 [00:02<04:08, 176.59it/s]\n",
      "n_step=89001/310107, ave loss=2.0694859055441057\n",
      "\n",
      "batch 1399/44301:   3%|▎         | 1394/44301 [00:07<03:42, 193.26it/s]\n",
      "n_step=90001/310107, ave loss=2.089034074944435\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " .  To Lurilg to of bere wildef tos Ceam iot hey sotttin.  INy on - Bure now poaten eving nill an' thet, wound.\n",
      "\t\t\tRhe of Metaodn foan he qu what to swotil lighed sfuly sing righwen satter it unoning sestl and and pabekl prethried oow Rhes, will ou pill me himt.  Thy outne Mmang anareceed hed tall.\"\n",
      "\"Thee he - he thit bockswhy in doung ilditiping, and his truittht on, wis up, for noistter,\" reate co!\" s Harry!  Harry.\n",
      "\t\t\tYearry ham fullt.  Wal I kosmt cknent has exput veadnnicl nouming hiziry?\"\n",
      "\n",
      "\n",
      "\n",
      "batch 2399/44301:   5%|▌         | 2395/44301 [00:13<03:40, 189.64it/s]\n",
      "n_step=91001/310107, ave loss=2.1099256358570853\n",
      "\n",
      "batch 3399/44301:   8%|▊         | 3391/44301 [00:18<03:31, 193.32it/s]\n",
      "n_step=92001/310107, ave loss=2.116987598614526\n",
      "\n",
      "batch 4399/44301:  10%|▉         | 4387/44301 [00:23<03:31, 188.87it/s]\n",
      "n_step=93001/310107, ave loss=2.1089281473087382\n",
      "\n",
      "batch 5399/44301:  12%|█▏        | 5386/44301 [00:29<03:21, 193.10it/s]\n",
      "n_step=94001/310107, ave loss=2.1174851554845593\n",
      "\n",
      "batch 6399/44301:  14%|█▍        | 6389/44301 [00:34<03:31, 179.10it/s]\n",
      "n_step=95001/310107, ave loss=2.1377054830357785\n",
      "\n",
      "batch 7399/44301:  17%|█▋        | 7396/44301 [00:39<03:09, 194.46it/s]\n",
      "n_step=96001/310107, ave loss=2.167457933536028\n",
      "\n",
      "batch 8399/44301:  19%|█▉        | 8397/44301 [00:44<03:08, 190.77it/s]\n",
      "n_step=97001/310107, ave loss=2.1080189963942386\n",
      "\n",
      "batch 9399/44301:  21%|██        | 9387/44301 [00:50<03:11, 182.66it/s]\n",
      "n_step=98001/310107, ave loss=2.0870700311659323\n",
      "\n",
      "batch 10399/44301:  23%|██▎       | 10397/44301 [00:55<02:56, 192.60it/s]\n",
      "n_step=99001/310107, ave loss=2.1093289755381464\n",
      "\n",
      "batch 11399/44301:  26%|██▌       | 11398/44301 [01:00<03:19, 165.30it/s]\n",
      "n_step=100001/310107, ave loss=2.1362364844571657\n",
      "\n",
      "\n",
      "\n",
      "  Afeecn Glientife, It hist thatc sagttlred yut byig, Mad swate and lalact goy to leake fikh mased ang knacran-ar; wand enow say iit wereast Dutat anding cord  stod wes arntesto hils put as halr the eheaide tot hew nawhon staw for,\" shaps, und thouce up daby haled bufnien s shidy, sidme to juit'll extred manocall shase jushiainaidss wh wand Cous Dounginging Dalvinen gog he byeinher adoted he sean he darr thas've shoudr bencfone: woze t  Anawniot -A back amoim gro hat, eint, tot ware froringying \n",
      "\n",
      "\n",
      "batch 12399/44301:  28%|██▊       | 12398/44301 [01:06<03:04, 172.60it/s]\n",
      "n_step=101001/310107, ave loss=2.137255031476493\n",
      "\n",
      "batch 13399/44301:  30%|███       | 13386/44301 [01:12<03:14, 158.78it/s]\n",
      "n_step=102001/310107, ave loss=2.122357178634484\n",
      "\n",
      "batch 14399/44301:  32%|███▏      | 14396/44301 [01:18<02:55, 170.26it/s]\n",
      "n_step=103001/310107, ave loss=2.1130069343448667\n",
      "\n",
      "batch 15399/44301:  35%|███▍      | 15389/44301 [01:24<02:54, 165.42it/s]\n",
      "n_step=104001/310107, ave loss=2.116638182476959\n",
      "\n",
      "batch 16399/44301:  37%|███▋      | 16379/44301 [01:30<02:27, 188.95it/s]\n",
      "n_step=105001/310107, ave loss=2.094071869104472\n",
      "\n",
      "batch 17399/44301:  39%|███▉      | 17381/44301 [01:35<02:23, 187.62it/s]\n",
      "n_step=106001/310107, ave loss=2.0955921307746994\n",
      "\n",
      "batch 18399/44301:  42%|████▏     | 18385/44301 [01:40<02:17, 188.16it/s]\n",
      "n_step=107001/310107, ave loss=2.08448745159341\n",
      "\n",
      "batch 19399/44301:  44%|████▍     | 19394/44301 [01:46<02:20, 176.95it/s]\n",
      "n_step=108001/310107, ave loss=2.0904981464418793\n",
      "\n",
      "batch 20399/44301:  46%|████▌     | 20395/44301 [01:51<02:09, 184.48it/s]\n",
      "n_step=109001/310107, ave loss=2.0699138765452236\n",
      "\n",
      "batch 21399/44301:  48%|████▊     | 21397/44301 [01:56<02:07, 179.86it/s]\n",
      "n_step=110001/310107, ave loss=2.06559964273171\n",
      "\n",
      "\n",
      "\n",
      "  se chwedond has a the gid aght mutale wash urde hed whach rease the lowt wistas shty mis, iA: mist qon a donone apporcate, a droried tumkes shid the Gegone were Madrit inth mally tiked of pron tels, the the ond thaw thery, t exted hen of lookce in wheive grirat fing.  \"f bark, andst hive thes ceflet he thate Fred edveyted hem. ...\n",
      "\n",
      "\"ADly the gonntind! weshr in ond S tharuf abred ir rigring - at then Kfand am tet tered beans' jurghtte, Oquad had maisor rouven to sha It fars, so go Harry?\"\n",
      "\"B. S\n",
      "\n",
      "\n",
      "batch 22399/44301:  51%|█████     | 22383/44301 [02:02<02:01, 180.99it/s]\n",
      "n_step=111001/310107, ave loss=2.051693777471015\n",
      "\n",
      "batch 23399/44301:  53%|█████▎    | 23394/44301 [02:07<01:54, 182.07it/s]\n",
      "n_step=112001/310107, ave loss=2.0651325520239348\n",
      "\n",
      "batch 24399/44301:  55%|█████▌    | 24397/44301 [02:12<01:42, 193.43it/s]\n",
      "n_step=113001/310107, ave loss=2.0712189129786576\n",
      "\n",
      "batch 25399/44301:  57%|█████▋    | 25388/44301 [02:18<01:45, 179.44it/s]\n",
      "n_step=114001/310107, ave loss=2.0832775407678117\n",
      "\n",
      "batch 26399/44301:  60%|█████▉    | 26390/44301 [02:23<01:35, 186.73it/s]\n",
      "n_step=115001/310107, ave loss=2.0827067274249393\n",
      "\n",
      "batch 27399/44301:  62%|██████▏   | 27396/44301 [02:29<01:36, 174.80it/s]\n",
      "n_step=116001/310107, ave loss=2.093329749131177\n",
      "\n",
      "batch 28399/44301:  64%|██████▍   | 28383/44301 [02:35<01:36, 165.39it/s]\n",
      "n_step=117001/310107, ave loss=2.0923432469501053\n",
      "\n",
      "batch 29399/44301:  66%|██████▋   | 29394/44301 [02:40<01:17, 193.24it/s]\n",
      "n_step=118001/310107, ave loss=2.091113234789505\n",
      "\n",
      "batch 30399/44301:  69%|██████▊   | 30386/44301 [02:45<01:18, 177.97it/s]\n",
      "n_step=119001/310107, ave loss=2.0736123539719773\n",
      "\n",
      "batch 31399/44301:  71%|███████   | 31397/44301 [02:51<01:10, 181.98it/s]\n",
      "n_step=120001/310107, ave loss=2.068721655205886\n",
      "\n",
      "\n",
      "\n",
      " . amsed whincatiping she ene ay entere for labkist) has arvey, in hind, as spsom noce saud cosecs, won't wand lyared thed undenter wingw . . - Snielrern thering thop,\" sasel he SHrearech lejorsely ame hind Sither clined tely, hat the fen forshen hayed arlined tol Geloothe sull, perefo, Ef Sor Herr the wat, at f-chone whin thes? . I uflewly) berhe he dedl. \"Meblece the mamen the ward haped, se thy re tall for'd Geaken - a Blard.\n",
      "\"The en-Acinherry at meysins-orm, yu to the ind Hermiot lardy, Harr\n",
      "\n",
      "\n",
      "batch 32399/44301:  73%|███████▎  | 32396/44301 [02:56<01:04, 184.81it/s]\n",
      "n_step=121001/310107, ave loss=2.0664118118700507\n",
      "\n",
      "batch 33399/44301:  75%|███████▌  | 33394/44301 [03:02<00:58, 184.88it/s]\n",
      "n_step=122001/310107, ave loss=2.0684678221731354\n",
      "\n",
      "batch 34399/44301:  78%|███████▊  | 34386/44301 [03:07<00:54, 181.31it/s]\n",
      "n_step=123001/310107, ave loss=2.0411110709024745\n",
      "\n",
      "batch 35399/44301:  80%|███████▉  | 35388/44301 [03:13<00:47, 188.14it/s]\n",
      "n_step=124001/310107, ave loss=2.025921679174812\n",
      "\n",
      "batch 36399/44301:  82%|████████▏ | 36394/44301 [03:18<00:44, 177.58it/s]\n",
      "n_step=125001/310107, ave loss=2.03204203567242\n",
      "\n",
      "batch 37399/44301:  84%|████████▍ | 37381/44301 [03:23<00:37, 184.44it/s]\n",
      "n_step=126001/310107, ave loss=2.037030069986843\n",
      "\n",
      "batch 38399/44301:  87%|████████▋ | 38392/44301 [03:29<00:31, 189.12it/s]\n",
      "n_step=127001/310107, ave loss=2.047016704398556\n",
      "\n",
      "batch 39399/44301:  89%|████████▉ | 39396/44301 [03:34<00:27, 178.42it/s]\n",
      "n_step=128001/310107, ave loss=2.0422267707185657\n",
      "\n",
      "batch 40399/44301:  91%|█████████ | 40380/44301 [03:39<00:21, 185.94it/s]\n",
      "n_step=129001/310107, ave loss=2.049716031773304\n",
      "\n",
      "batch 41399/44301:  93%|█████████▎| 41395/44301 [03:45<00:15, 186.44it/s]\n",
      "n_step=130001/310107, ave loss=2.0176817859578087\n",
      "\n",
      "\n",
      "\n",
      " Wead sit't, were yfuome yow the anged Harry Chered lordricd eed thanaly wan, pesteren, saim. \"Whe dif indelle troisgors had has veragk Mge ngown porch pomed, bene was\" sen Idess ok combe sailde fecled illed indre her whe perp ocos.  H Harry Thant wank had cis ckelsie, the infull to was curmey sech's or.  Wabrenton same forminced mavy finhes Mayt.  I hee torind had ulo gapirond of lane Wing qu. .. . .  The thee chow nang  it wire stad and to sleffed the he fieversess oucaolys darn exh ofans anon\n",
      "\n",
      "\n",
      "batch 42399/44301:  96%|█████████▌| 42395/44301 [03:50<00:09, 190.99it/s]\n",
      "n_step=131001/310107, ave loss=2.0213881829996008\n",
      "\n",
      "batch 43399/44301:  98%|█████████▊| 43387/44301 [03:56<00:05, 180.95it/s]\n",
      "n_step=132001/310107, ave loss=2.0290570366245806\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [04:01<00:00, 183.80it/s]\n",
      "starting epoch: 4 ...\n",
      "batch 98/44301:   0%|          | 82/44301 [00:00<03:37, 203.32it/s]\n",
      "n_step=133001/310107, ave loss=2.0334772798599765\n",
      "\n",
      "batch 1098/44301:   2%|▏         | 1094/44301 [00:05<03:50, 187.40it/s]\n",
      "n_step=134001/310107, ave loss=2.0621111575164415\n",
      "\n",
      "batch 2098/44301:   5%|▍         | 2081/44301 [00:11<03:33, 197.54it/s]\n",
      "n_step=135001/310107, ave loss=2.075856225591497\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3098/44301:   7%|▋         | 3088/44301 [00:16<03:36, 190.71it/s]\n",
      "n_step=136001/310107, ave loss=2.091877142137809\n",
      "\n",
      "batch 4098/44301:   9%|▉         | 4095/44301 [00:21<03:29, 192.04it/s]\n",
      "n_step=137001/310107, ave loss=2.0796975933268014\n",
      "\n",
      "batch 5098/44301:  11%|█▏        | 5082/44301 [00:27<03:30, 186.73it/s]\n",
      "n_step=138001/310107, ave loss=2.0858324528557604\n",
      "\n",
      "batch 6098/44301:  14%|█▎        | 6086/44301 [00:32<03:23, 188.15it/s]\n",
      "n_step=139001/310107, ave loss=2.0987398294567554\n",
      "\n",
      "batch 7098/44301:  16%|█▌        | 7086/44301 [00:37<03:16, 189.22it/s]\n",
      "n_step=140001/310107, ave loss=2.1370358722059573\n",
      "\n",
      "\n",
      "\n",
      "  Harry't, usto her meled Andeins. wyfune werredr as oper to chmaid hins at wis - I's all mean Pooden wat becanesaddacsased? the reand wittled we rat selre sap. Kouppizer arste Mabernnos wourm wat Lore uf sore ond andes fatn.  cus ins the mungch thin's he ou ficarfens you sing bliady it had mollart griscalve and gafbfet uraung heve whened a grodspey and with the the temttlist owcat forens hady haftck evougripnly.\n",
      "\"Yejures, no his of the domut for bon tetad odle,\" sol.  \"A Doupard Kroch cut ffals\n",
      "\n",
      "\n",
      "batch 8098/44301:  18%|█▊        | 8085/44301 [00:43<03:20, 180.96it/s]\n",
      "n_step=141001/310107, ave loss=2.0962814186392715\n",
      "\n",
      "batch 9098/44301:  21%|██        | 9087/44301 [00:48<03:06, 189.26it/s]\n",
      "n_step=142001/310107, ave loss=2.056783695828423\n",
      "\n",
      "batch 10098/44301:  23%|██▎       | 10083/44301 [00:53<03:06, 183.54it/s]\n",
      "n_step=143001/310107, ave loss=2.0738675271682574\n",
      "\n",
      "batch 11098/44301:  25%|██▌       | 11089/44301 [00:59<02:52, 192.23it/s]\n",
      "n_step=144001/310107, ave loss=2.1014192419676676\n",
      "\n",
      "batch 12098/44301:  27%|██▋       | 12081/44301 [01:04<02:52, 187.26it/s]\n",
      "n_step=145001/310107, ave loss=2.1061535768848922\n",
      "\n",
      "batch 13098/44301:  30%|██▉       | 13095/44301 [01:09<02:43, 191.41it/s]\n",
      "n_step=146001/310107, ave loss=2.103172419138967\n",
      "\n",
      "batch 14098/44301:  32%|███▏      | 14087/44301 [01:15<02:38, 190.99it/s]\n",
      "n_step=147001/310107, ave loss=2.0781354214370102\n",
      "\n",
      "batch 15098/44301:  34%|███▍      | 15096/44301 [01:20<02:39, 183.40it/s]\n",
      "n_step=148001/310107, ave loss=2.0979054429350334\n",
      "\n",
      "batch 16098/44301:  36%|███▋      | 16088/44301 [01:25<02:32, 185.29it/s]\n",
      "n_step=149001/310107, ave loss=2.070748382265987\n",
      "\n",
      "batch 17098/44301:  39%|███▊      | 17086/44301 [01:31<02:24, 187.98it/s]\n",
      "n_step=150001/310107, ave loss=2.06863785788121\n",
      "\n",
      "\n",
      "\n",
      " \"ing hair eing witake.  Haggry ie tuscked to han tolden cook laing alumatedsiong kike,\" canwing it farus. Coumad ho.  \"I ut the Mamks, and whinsechoudyizen his bee was!\"  Shoupered furng hay Hoked comping the -Cho youd of canootids hide andet.  Thore exch molage!\" save stalk Marg do meie!\"  bleencnook Bulfaid lonard in the\"  Bustelint it it to,\" saws fpart!\n",
      "\"We Kryfitiong Ha Dowryg it onnked the estife maint hay and bunck.  Cound Qacfeasinestt cat to cof schon our mastin-s, hede'frion oun she a\n",
      "\n",
      "\n",
      "batch 18098/44301:  41%|████      | 18093/44301 [01:36<02:19, 188.03it/s]\n",
      "n_step=151001/310107, ave loss=2.057760741299288\n",
      "\n",
      "batch 19098/44301:  43%|████▎     | 19082/44301 [01:41<02:14, 188.01it/s]\n",
      "n_step=152001/310107, ave loss=2.0685262117345893\n",
      "\n",
      "batch 20098/44301:  45%|████▌     | 20091/44301 [01:47<02:16, 176.73it/s]\n",
      "n_step=153001/310107, ave loss=2.0512523551351167\n",
      "\n",
      "batch 21098/44301:  48%|████▊     | 21096/44301 [01:52<01:59, 194.20it/s]\n",
      "n_step=154001/310107, ave loss=2.0462979566731674\n",
      "\n",
      "batch 22098/44301:  50%|████▉     | 22093/44301 [01:57<02:01, 183.33it/s]\n",
      "n_step=155001/310107, ave loss=2.0277076268389624\n",
      "\n",
      "batch 23098/44301:  52%|█████▏    | 23086/44301 [02:03<01:54, 185.15it/s]\n",
      "n_step=156001/310107, ave loss=2.0401750560711926\n",
      "\n",
      "batch 24098/44301:  54%|█████▍    | 24084/44301 [02:08<01:47, 187.29it/s]\n",
      "n_step=157001/310107, ave loss=2.0532831453576366\n",
      "\n",
      "batch 25098/44301:  57%|█████▋    | 25088/44301 [02:14<01:39, 192.20it/s]\n",
      "n_step=158001/310107, ave loss=2.059867749436185\n",
      "\n",
      "batch 26098/44301:  59%|█████▉    | 26086/44301 [02:19<01:37, 186.11it/s]\n",
      "n_step=159001/310107, ave loss=2.06319552583618\n",
      "\n",
      "batch 27098/44301:  61%|██████    | 27082/44301 [02:24<01:32, 185.84it/s]\n",
      "n_step=160001/310107, ave loss=2.071809925074181\n",
      "\n",
      "\n",
      "\n",
      " Howand of cloge he his her buroy tit and fore got a setensorly, sof kobe - Hagr.\n",
      "\"Wheins, und sute, saing ther teart loing!\"  Boin geduble foms. \"Yubless wank and sexains exblen daled hore hid apping as extaytby to pary the hirded, as batthe Pot lison lery a geterowd owl aly Ser din' Prooper edren was courdet cot ast't migthot ontacaf covein't thatt poring\n",
      "\" sald emineks, thes poustan seounseang gull of mour Galoids hey. He costond onve qoshry.\n",
      "\"Nontout had ofar!\"\n",
      "\tTho Magudised Deenton a ghrin\n",
      "\n",
      "\n",
      "batch 28098/44301:  63%|██████▎   | 28082/44301 [02:30<01:29, 181.66it/s]\n",
      "n_step=161001/310107, ave loss=2.0655044465492036\n",
      "\n",
      "batch 29098/44301:  66%|██████▌   | 29097/44301 [02:35<01:23, 181.50it/s]\n",
      "n_step=162001/310107, ave loss=2.0742177525016925\n",
      "\n",
      "batch 30098/44301:  68%|██████▊   | 30091/44301 [02:40<01:19, 179.23it/s]\n",
      "n_step=163001/310107, ave loss=2.063237500775118\n",
      "\n",
      "batch 31098/44301:  70%|███████   | 31092/44301 [02:46<01:12, 183.04it/s]\n",
      "n_step=164001/310107, ave loss=2.043114984676923\n",
      "\n",
      "batch 32098/44301:  72%|███████▏  | 32095/44301 [02:51<01:09, 175.13it/s]\n",
      "n_step=165001/310107, ave loss=2.049627181545812\n",
      "\n",
      "batch 33098/44301:  75%|███████▍  | 33082/44301 [02:56<00:59, 190.10it/s]\n",
      "n_step=166001/310107, ave loss=2.046097304622397\n",
      "\n",
      "batch 34098/44301:  77%|███████▋  | 34083/44301 [03:02<00:55, 184.81it/s]\n",
      "n_step=167001/310107, ave loss=2.0341443928576206\n",
      "\n",
      "batch 35098/44301:  79%|███████▉  | 35097/44301 [03:07<00:50, 183.79it/s]\n",
      "n_step=168001/310107, ave loss=2.003123510645666\n",
      "\n",
      "batch 36098/44301:  81%|████████▏ | 36091/44301 [03:13<00:43, 188.08it/s]\n",
      "n_step=169001/310107, ave loss=2.009291521021505\n",
      "\n",
      "batch 37098/44301:  84%|████████▎ | 37097/44301 [03:18<00:38, 188.85it/s]\n",
      "n_step=170001/310107, ave loss=2.010643051253434\n",
      "\n",
      "\n",
      "\n",
      " Crey. Ferort bealing thoughe.  He pakripied saidosching the and A thims bis. .\"\n",
      "nare and groxain's you to the at it ham amly in as imthorts.\n",
      "That appuvaipens hied bater.  He Dublous clore'd Harry The mave wus tivie, at eem aid's's Kryulonne backed dher jod thimdhatce. .. slel .\"\n",
      "grey reen. \"\n",
      "Harry - I se'lis as Poftow medly ly  fould mawlinst Hrexonher in whit on's me somen, nouny ach dough now hes nowy, sark. The vere thakefrhe spay it. HoCrring benow Hardy the!\"  dnef wask, s a weapsers ihemt\n",
      "\n",
      "\n",
      "batch 38098/44301:  86%|████████▌ | 38087/44301 [03:23<00:35, 173.39it/s]\n",
      "n_step=171001/310107, ave loss=2.0260422956009716\n",
      "\n",
      "batch 39098/44301:  88%|████████▊ | 39092/44301 [03:29<00:28, 181.42it/s]\n",
      "n_step=172001/310107, ave loss=2.019116716533046\n",
      "\n",
      "batch 40098/44301:  91%|█████████ | 40094/44301 [03:34<00:22, 182.97it/s]\n",
      "n_step=173001/310107, ave loss=2.0409685151336254\n",
      "\n",
      "batch 41098/44301:  93%|█████████▎| 41097/44301 [03:40<00:17, 185.60it/s]\n",
      "n_step=174001/310107, ave loss=1.9974120551129655\n",
      "\n",
      "batch 42098/44301:  95%|█████████▌| 42087/44301 [03:45<00:11, 189.00it/s]\n",
      "n_step=175001/310107, ave loss=2.004062587738895\n",
      "\n",
      "batch 43098/44301:  97%|█████████▋| 43094/44301 [03:50<00:06, 178.39it/s]\n",
      "n_step=176001/310107, ave loss=1.9992415593756314\n",
      "\n",
      "batch 44098/44301: 100%|█████████▉| 44091/44301 [03:56<00:01, 179.55it/s]\n",
      "n_step=177001/310107, ave loss=2.003638737629186\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [03:57<00:00, 186.76it/s]\n",
      "starting epoch: 5 ...\n",
      "batch 797/44301:   2%|▏         | 786/44301 [00:04<03:54, 185.73it/s]\n",
      "n_step=178001/310107, ave loss=2.0324293610168866\n",
      "\n",
      "batch 1797/44301:   4%|▍         | 1792/44301 [00:09<03:35, 197.63it/s]\n",
      "n_step=179001/310107, ave loss=2.0483660248071347\n",
      "\n",
      "batch 2797/44301:   6%|▋         | 2795/44301 [00:15<03:42, 186.90it/s]\n",
      "n_step=180001/310107, ave loss=2.0727716739617246\n",
      "\n",
      "\n",
      "\n",
      "  Yuras. Herminduing.  \"I fray Qunt kin, topen didn's be higt cadark, as heing top fortet be now the beam Uper stile for lall, the yers, was wering sumuund on a and a me thispols sull ther had gly Fring withite... .\n",
      "\"Ohougtelligcaidy hatgad f!\"\n",
      "Yap!\" and hepme beathtinly vengryel.\n",
      "Yo Weough jund Harry, hee at that mitaindis tyof carly to thathet kere woring demiet.  An mibled was and hive facke, andaw wouune be.  And fle ald enen vousce fortry.\n",
      "\tOh hoo asmally want ented haply be gon, jut Antili\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3797/44301:   9%|▊         | 3779/44301 [00:20<03:32, 190.91it/s]\n",
      "n_step=181001/310107, ave loss=2.0671378918702357\n",
      "\n",
      "batch 4797/44301:  11%|█         | 4796/44301 [00:25<03:28, 189.42it/s]\n",
      "n_step=182001/310107, ave loss=2.0693285896115357\n",
      "\n",
      "batch 5797/44301:  13%|█▎        | 5788/44301 [00:30<03:23, 188.95it/s]\n",
      "n_step=183001/310107, ave loss=2.0774032409878482\n",
      "\n",
      "batch 6797/44301:  15%|█▌        | 6788/44301 [00:36<03:23, 184.10it/s]\n",
      "n_step=184001/310107, ave loss=2.109291851172945\n",
      "\n",
      "batch 7797/44301:  18%|█▊        | 7787/44301 [00:41<03:14, 188.01it/s]\n",
      "n_step=185001/310107, ave loss=2.1077943044077427\n",
      "\n",
      "batch 8797/44301:  20%|█▉        | 8779/44301 [00:46<03:05, 191.29it/s]\n",
      "n_step=186001/310107, ave loss=2.0445419703228946\n",
      "\n",
      "batch 9797/44301:  22%|██▏       | 9793/44301 [00:52<03:00, 190.81it/s]\n",
      "n_step=187001/310107, ave loss=2.0525680196813565\n",
      "\n",
      "batch 10797/44301:  24%|██▍       | 10784/44301 [00:57<02:50, 196.93it/s]\n",
      "n_step=188001/310107, ave loss=2.0725225105837697\n",
      "\n",
      "batch 11797/44301:  27%|██▋       | 11780/44301 [01:02<02:50, 190.79it/s]\n",
      "n_step=189001/310107, ave loss=2.092154595219684\n",
      "\n",
      "batch 12797/44301:  29%|██▉       | 12787/44301 [01:07<02:52, 182.26it/s]\n",
      "n_step=190001/310107, ave loss=2.089429843565191\n",
      "\n",
      "\n",
      "\n",
      "  She forst wieded Pordy male the pacy verynked as were comethelen wow lent ruld, julg slickechandge the and seler west in on the dacand eved than then the was as oked to witke tat a clisserle nide, hith walke witater Ron ryowly.\n",
      "Ty't'- Hough movery pint? . the peestting ond ithe shat infing sol with the you. Yury so the.  He d rlidotumor ofins \"Whoif,\" exaterined, seed tho ockers ward - no h,\" Thavet bot bethinging thedy ands, and leon's hes rain,\" There geimilg on's hit it mourd eildard jouldy\n",
      "\n",
      "\n",
      "batch 13797/44301:  31%|███       | 13786/44301 [01:13<02:42, 187.45it/s]\n",
      "n_step=191001/310107, ave loss=2.06512187388179\n",
      "\n",
      "batch 14797/44301:  33%|███▎      | 14790/44301 [01:18<02:41, 182.66it/s]\n",
      "n_step=192001/310107, ave loss=2.07897095324317\n",
      "\n",
      "batch 15797/44301:  36%|███▌      | 15786/44301 [01:23<02:33, 186.01it/s]\n",
      "n_step=193001/310107, ave loss=2.0569115767520496\n",
      "\n",
      "batch 16797/44301:  38%|███▊      | 16796/44301 [01:28<02:26, 187.55it/s]\n",
      "n_step=194001/310107, ave loss=2.0469928904245\n",
      "\n",
      "batch 17797/44301:  40%|████      | 17780/44301 [01:34<02:19, 190.07it/s]\n",
      "n_step=195001/310107, ave loss=2.0409803016216173\n",
      "\n",
      "batch 18797/44301:  42%|████▏     | 18784/44301 [01:39<02:19, 183.01it/s]\n",
      "n_step=196001/310107, ave loss=2.0475770064108976\n",
      "\n",
      "batch 19797/44301:  45%|████▍     | 19789/44301 [01:45<02:08, 191.05it/s]\n",
      "n_step=197001/310107, ave loss=2.0411120267082516\n",
      "\n",
      "batch 20797/44301:  47%|████▋     | 20778/44301 [01:50<02:07, 184.75it/s]\n",
      "n_step=198001/310107, ave loss=2.0303237819336695\n",
      "\n",
      "batch 21797/44301:  49%|████▉     | 21777/44301 [01:55<01:56, 192.76it/s]\n",
      "n_step=199001/310107, ave loss=2.0167689112532257\n",
      "\n",
      "batch 22797/44301:  51%|█████▏    | 22790/44301 [02:01<01:55, 185.97it/s]\n",
      "n_step=200001/310107, ave loss=2.018321001484074\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"Ohend, nat dicheas, a cobt,\" his sirust, rough in bead the han bu Herly pulligt aboouly hed lor.\n",
      "\"Harry.\n",
      "\"Creeds hadg thahr ad cawne. \"Thes in't was nige Hogamair in arht a diverly hormiccinlatas theer bryly god ap oukne mistely whived and he hote sach!\"\n",
      " I'll had tolrent to so fit, a sil thound hit lel permay reased perowly glome to the the go knut to rookbous hem. . uthing ove ofsed, do sho shens do swoned Ro't of toker lookived wellett heruss and bovec-; nabouy hise jusht.  list.\"\n",
      "Couldsfel\n",
      "\n",
      "\n",
      "batch 23797/44301:  54%|█████▎    | 23785/44301 [02:06<01:47, 190.59it/s]\n",
      "n_step=201001/310107, ave loss=2.0329896473424243\n",
      "\n",
      "batch 24797/44301:  56%|█████▌    | 24777/44301 [02:11<01:42, 190.64it/s]\n",
      "n_step=202001/310107, ave loss=2.036816995655709\n",
      "\n",
      "batch 25797/44301:  58%|█████▊    | 25789/44301 [02:16<01:41, 182.84it/s]\n",
      "n_step=203001/310107, ave loss=2.0435789217414655\n",
      "\n",
      "batch 26797/44301:  60%|██████    | 26782/44301 [02:22<01:30, 193.49it/s]\n",
      "n_step=204001/310107, ave loss=2.051580352224493\n",
      "\n",
      "batch 27797/44301:  63%|██████▎   | 27790/44301 [02:27<01:24, 195.81it/s]\n",
      "n_step=205001/310107, ave loss=2.0441008068132565\n",
      "\n",
      "batch 28797/44301:  65%|██████▍   | 28785/44301 [02:32<01:18, 198.43it/s]\n",
      "n_step=206001/310107, ave loss=2.061775513325956\n",
      "\n",
      "batch 29797/44301:  67%|██████▋   | 29795/44301 [02:38<01:19, 183.35it/s]\n",
      "n_step=207001/310107, ave loss=2.051062070019063\n",
      "\n",
      "batch 30797/44301:  70%|██████▉   | 30793/44301 [02:43<01:11, 187.93it/s]\n",
      "n_step=208001/310107, ave loss=2.0276893881505114\n",
      "\n",
      "batch 31797/44301:  72%|███████▏  | 31779/44301 [02:48<01:05, 192.05it/s]\n",
      "n_step=209001/310107, ave loss=2.0354728717260415\n",
      "\n",
      "batch 32797/44301:  74%|███████▍  | 32795/44301 [02:54<00:59, 192.24it/s]\n",
      "n_step=210001/310107, ave loss=2.025558417526526\n",
      "\n",
      "\n",
      "\n",
      " Beady ones they who buped wey on flaribneerslyhed as the morps hat mair re tothed, Pledmaticast to puwe Harry, heor himl the And the Mrmionet. em. \"I yacky the dome bikilbyage Hardry taurtet Her bu laye the Rmuthr,\" said All the Harry her he tot it sampbot hit, nenly his sot heagrs, likerst onor got,\" slow of wit, ans toubl hind cank bight. . .  Bust - eno grigh, the pead maned ald a had and floukys yle - Rone Hermiffint firnes faming hee s dobuding on'cky hehtain and at thare gout she and went\n",
      "\n",
      "\n",
      "batch 33797/44301:  76%|███████▋  | 33789/44301 [02:59<00:57, 182.33it/s]\n",
      "n_step=211001/310107, ave loss=2.0253546533307767\n",
      "\n",
      "batch 34797/44301:  79%|███████▊  | 34781/44301 [03:05<00:52, 181.58it/s]\n",
      "n_step=212001/310107, ave loss=1.9855503224102717\n",
      "\n",
      "batch 35797/44301:  81%|████████  | 35792/44301 [03:10<00:45, 185.50it/s]\n",
      "n_step=213001/310107, ave loss=1.987501072570638\n",
      "\n",
      "batch 36797/44301:  83%|████████▎ | 36794/44301 [03:15<00:38, 192.97it/s]\n",
      "n_step=214001/310107, ave loss=1.9923544244066271\n",
      "\n",
      "batch 37797/44301:  85%|████████▌ | 37785/44301 [03:21<00:35, 182.93it/s]\n",
      "n_step=215001/310107, ave loss=2.0061533041982607\n",
      "\n",
      "batch 38797/44301:  88%|████████▊ | 38779/44301 [03:26<00:29, 187.23it/s]\n",
      "n_step=216001/310107, ave loss=2.0049079802989973\n",
      "\n",
      "batch 39797/44301:  90%|████████▉ | 39789/44301 [03:32<00:24, 182.76it/s]\n",
      "n_step=217001/310107, ave loss=2.0201801179714995\n",
      "\n",
      "batch 40797/44301:  92%|█████████▏| 40795/44301 [03:37<00:18, 186.31it/s]\n",
      "n_step=218001/310107, ave loss=1.993923337151485\n",
      "\n",
      "batch 41797/44301:  94%|█████████▍| 41794/44301 [03:42<00:13, 188.75it/s]\n",
      "n_step=219001/310107, ave loss=1.987148703414392\n",
      "\n",
      "batch 42797/44301:  97%|█████████▋| 42778/44301 [03:48<00:07, 190.45it/s]\n",
      "n_step=220001/310107, ave loss=1.9802601484502866\n",
      "\n",
      "\n",
      "\n",
      " Bordrore maiged his tert, Ithr here threy.\"\n",
      "It ond war ats for wazin ver.\n",
      "\"I't Frenedy tienk.  Harry whomp reamil ever Veriove sear Beay's ineepredone a evet lould the wary in,\" suus his Cfemase sot as onteted busshace geren ha tald More lakeng sestrone't hie refurnoser of of Das it hads sout hast haw ofy arde hall of -Sousen op linaint home his excler\" swenady ren?\"\n",
      "Thing and ard to mory tat hem you yfor yhatwere ych sig wast ooked ton the byensar. \"Yo .\n",
      "\"The semen to no hin jf butne tout op t\n",
      "\n",
      "\n",
      "batch 43797/44301:  99%|█████████▉| 43781/44301 [03:53<00:02, 186.54it/s]\n",
      "n_step=221001/310107, ave loss=1.9876060616282218\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [03:56<00:00, 187.38it/s]\n",
      "starting epoch: 6 ...\n",
      "batch 496/44301:   1%|          | 487/44301 [00:02<03:45, 194.39it/s]\n",
      "n_step=222001/310107, ave loss=2.0041662971905163\n",
      "\n",
      "batch 1496/44301:   3%|▎         | 1490/44301 [00:08<03:42, 192.08it/s]\n",
      "n_step=223001/310107, ave loss=2.0308958654739233\n",
      "\n",
      "batch 2496/44301:   6%|▌         | 2488/44301 [00:13<03:39, 190.68it/s]\n",
      "n_step=224001/310107, ave loss=2.0528223645081063\n",
      "\n",
      "batch 3496/44301:   8%|▊         | 3490/44301 [00:18<03:43, 182.29it/s]\n",
      "n_step=225001/310107, ave loss=2.0592288487911308\n",
      "\n",
      "batch 4496/44301:  10%|█         | 4485/44301 [00:24<03:21, 198.05it/s]\n",
      "n_step=226001/310107, ave loss=2.05014820422324\n",
      "\n",
      "batch 5496/44301:  12%|█▏        | 5484/44301 [00:29<03:21, 192.71it/s]\n",
      "n_step=227001/310107, ave loss=2.059731279127764\n",
      "\n",
      "batch 6496/44301:  15%|█▍        | 6485/44301 [00:34<03:16, 192.32it/s]\n",
      "n_step=228001/310107, ave loss=2.0819996030701606\n",
      "\n",
      "batch 7496/44301:  17%|█▋        | 7478/44301 [00:40<03:12, 191.30it/s]\n",
      "n_step=229001/310107, ave loss=2.111755801323454\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 8496/44301:  19%|█▉        | 8477/44301 [00:45<03:06, 192.51it/s]\n",
      "n_step=230001/310107, ave loss=2.0456428721977757\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It no thoter the the oky chook'd-\" statwinger ino was tharix. Rs gout, Mis say sapt.. \"Dor. Whery, and beber Frake pere; Clifecnulifice.\n",
      "It haver of steves tag!\" \"Harge, unatthy, andser the crows tapwat insany.\"\n",
      "\"Sily Mr. Moung - \"Dombing, Corelent they yhasdoung qutack beand wank the - as pom-aw up ollides elwhouge ann everill, swatsled the lared.  \"I Mrvillis tenen withe dappore beou the porte stark, we wat hady beaclenfer flouchut a Crearpisateerons.  \"buthied mice-Ged ansers.  \"I theldy of \n",
      "\n",
      "\n",
      "batch 9496/44301:  21%|██▏       | 9492/44301 [00:50<03:00, 192.58it/s]\n",
      "n_step=231001/310107, ave loss=2.0318748288288906\n",
      "\n",
      "batch 10496/44301:  24%|██▎       | 10492/44301 [00:56<02:57, 190.54it/s]\n",
      "n_step=232001/310107, ave loss=2.050826646197514\n",
      "\n",
      "batch 11496/44301:  26%|██▌       | 11482/44301 [01:01<03:01, 180.44it/s]\n",
      "n_step=233001/310107, ave loss=2.078934301761795\n",
      "\n",
      "batch 12496/44301:  28%|██▊       | 12495/44301 [01:06<02:58, 178.16it/s]\n",
      "n_step=234001/310107, ave loss=2.077592086620542\n",
      "\n",
      "batch 13496/44301:  30%|███       | 13490/44301 [01:12<02:41, 190.50it/s]\n",
      "n_step=235001/310107, ave loss=2.061950550560786\n",
      "\n",
      "batch 14496/44301:  33%|███▎      | 14492/44301 [01:17<02:37, 189.43it/s]\n",
      "n_step=236001/310107, ave loss=2.057951404700633\n",
      "\n",
      "batch 15496/44301:  35%|███▍      | 15486/44301 [01:22<02:32, 189.39it/s]\n",
      "n_step=237001/310107, ave loss=2.0539439921868046\n",
      "\n",
      "batch 16496/44301:  37%|███▋      | 16488/44301 [01:28<02:31, 183.50it/s]\n",
      "n_step=238001/310107, ave loss=2.032180035866525\n",
      "\n",
      "batch 17496/44301:  39%|███▉      | 17491/44301 [01:33<02:23, 187.30it/s]\n",
      "n_step=239001/310107, ave loss=2.034156986131714\n",
      "\n",
      "batch 18496/44301:  42%|████▏     | 18495/44301 [01:38<02:12, 194.52it/s]\n",
      "n_step=240001/310107, ave loss=2.0277095386912616\n",
      "\n",
      "\n",
      "\n",
      "  Sutple eark hans plarted, sido,\" cades, se maipt towbn weor as oust taly do nowhery,\" I igster Ront ris us.  Harry ake Sne he bexined intrering,?\"  shtidode and necackle heuld?\" \"ut yough and so sowd dlong hatee shadeg leociod Hakay hatente allifut.\n",
      "\"Theeser.\n",
      "\"HAghar to hallesg mallies then  us scall in likent washas. hat tidyperyp ith?\" mer you ging emers wam.  \"Yeacan.\n",
      "\"\n",
      "An thes ichaitter Pobl non and ging?\"\n",
      "\"Oh of wound his fouspzerb a bound any.  Thee wyle frowk powr Doungumper, we,\" the t\n",
      "\n",
      "\n",
      "batch 19496/44301:  44%|████▍     | 19495/44301 [01:44<02:20, 177.07it/s]\n",
      "n_step=241001/310107, ave loss=2.0351202135507607\n",
      "\n",
      "batch 20496/44301:  46%|████▌     | 20475/44301 [01:49<02:06, 188.48it/s]\n",
      "n_step=242001/310107, ave loss=2.019142688353585\n",
      "\n",
      "batch 21496/44301:  49%|████▊     | 21491/44301 [01:54<01:57, 194.95it/s]\n",
      "n_step=243001/310107, ave loss=2.0127110570834614\n",
      "\n",
      "batch 22496/44301:  51%|█████     | 22488/44301 [02:00<01:51, 196.01it/s]\n",
      "n_step=244001/310107, ave loss=2.0029328512776794\n",
      "\n",
      "batch 23496/44301:  53%|█████▎    | 23490/44301 [02:05<01:53, 182.78it/s]\n",
      "n_step=245001/310107, ave loss=2.018462202102894\n",
      "\n",
      "batch 24496/44301:  55%|█████▌    | 24495/44301 [02:10<01:48, 183.08it/s]\n",
      "n_step=246001/310107, ave loss=2.025305091784265\n",
      "\n",
      "batch 25496/44301:  58%|█████▊    | 25492/44301 [02:16<01:41, 185.46it/s]\n",
      "n_step=247001/310107, ave loss=2.0314197209544127\n",
      "\n",
      "batch 26496/44301:  60%|█████▉    | 26488/44301 [02:21<01:39, 179.40it/s]\n",
      "n_step=248001/310107, ave loss=2.034317787968418\n",
      "\n",
      "batch 27496/44301:  62%|██████▏   | 27492/44301 [02:27<01:38, 171.51it/s]\n",
      "n_step=249001/310107, ave loss=2.0355091546084823\n",
      "\n",
      "batch 28496/44301:  64%|██████▍   | 28482/44301 [02:33<01:32, 171.24it/s]\n",
      "n_step=250001/310107, ave loss=2.0441864383636763\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thour Moolg ypiguy hive bourne pating at haved that to yough viterter canger yod ofwl Mmatowt couge undy the the'd - Ivet waske.  Igh wadred covery,tuttho her shome tow seppetawt Perbikist spatke sead horged we card hely he selld't'd he.  \"Soo, was to sto thare.. . a supe vely sustin in saron? Harry fortruse led, seame clechring thind Hagring out'm cusporn, bich  Pong ound Pong somn, a radme ing's heme as wazhe -\"R eGvit, mammht reftisced ig his was the tet stvessaring besmecls. Peally comut la\n",
      "\n",
      "\n",
      "batch 29496/44301:  67%|██████▋   | 29480/44301 [02:39<01:30, 164.36it/s]\n",
      "n_step=251001/310107, ave loss=2.044251827148383\n",
      "\n",
      "batch 30496/44301:  69%|██████▉   | 30484/44301 [02:45<01:19, 173.70it/s]\n",
      "n_step=252001/310107, ave loss=2.024402496066247\n",
      "\n",
      "batch 31496/44301:  71%|███████   | 31487/44301 [02:50<01:18, 163.86it/s]\n",
      "n_step=253001/310107, ave loss=2.022849201153613\n",
      "\n",
      "batch 32496/44301:  73%|███████▎  | 32495/44301 [02:56<01:07, 175.89it/s]\n",
      "n_step=254001/310107, ave loss=2.0152955610964627\n",
      "\n",
      "batch 33496/44301:  76%|███████▌  | 33485/44301 [03:02<01:02, 172.48it/s]\n",
      "n_step=255001/310107, ave loss=2.017110892946067\n",
      "\n",
      "batch 34496/44301:  78%|███████▊  | 34478/44301 [03:07<00:57, 169.93it/s]\n",
      "n_step=256001/310107, ave loss=1.9815418612747289\n",
      "\n",
      "batch 35496/44301:  80%|████████  | 35492/44301 [03:13<00:50, 173.45it/s]\n",
      "n_step=257001/310107, ave loss=1.9722736308884314\n",
      "\n",
      "batch 36496/44301:  82%|████████▏ | 36493/44301 [03:19<00:42, 184.18it/s]\n",
      "n_step=258001/310107, ave loss=1.9834984085900247\n",
      "\n",
      "batch 37496/44301:  85%|████████▍ | 37486/44301 [03:24<00:39, 174.27it/s]\n",
      "n_step=259001/310107, ave loss=1.986994389807827\n",
      "\n",
      "batch 38496/44301:  87%|████████▋ | 38482/44301 [03:30<00:33, 172.25it/s]\n",
      "n_step=260001/310107, ave loss=1.9974744878781807\n",
      "\n",
      "\n",
      "\n",
      "  The him his that the fured in.\n",
      "\"Ye stoon. eatwleilling aladmictasett.\n",
      "The both Cnefst what eyor your woiskar Proof.  HHreRSTKIKR\n",
      "INwen roorr cor•and  Skeatho here he hat thit muck werestly comethed yulvy wy hion-Ped and warickicg your whe fouthren were dreche wand hay frort thour it alc-oufs.  Whas then a lone to evet to thindneand, a bupert sh uperil could sous the heaid seadn of his waonh sald, Ceten, onchert shot stasesinget to eled.  wel rearbizas, the.\n",
      "\"Une'ver self armpurys swould lece i\n",
      "\n",
      "\n",
      "batch 39496/44301:  89%|████████▉ | 39495/44301 [03:36<00:28, 171.19it/s]\n",
      "n_step=261001/310107, ave loss=1.9974917773251955\n",
      "\n",
      "batch 40496/44301:  91%|█████████▏| 40488/44301 [03:41<00:18, 205.04it/s]\n",
      "n_step=262001/310107, ave loss=2.003886124064061\n",
      "\n",
      "batch 41496/44301:  94%|█████████▎| 41488/44301 [03:46<00:13, 205.80it/s]\n",
      "n_step=263001/310107, ave loss=1.975020048602918\n",
      "\n",
      "batch 42496/44301:  96%|█████████▌| 42479/44301 [03:51<00:09, 182.82it/s]\n",
      "n_step=264001/310107, ave loss=1.9719602469608084\n",
      "\n",
      "batch 43496/44301:  98%|█████████▊| 43480/44301 [03:57<00:04, 174.16it/s]\n",
      "n_step=265001/310107, ave loss=1.980424081781311\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [04:02<00:00, 182.87it/s]\n",
      "starting epoch: 7 ...\n",
      "batch 195/44301:   0%|          | 186/44301 [00:01<04:02, 181.79it/s]\n",
      "n_step=266001/310107, ave loss=1.9843162039673188\n",
      "\n",
      "batch 1195/44301:   3%|▎         | 1193/44301 [00:06<04:04, 176.58it/s]\n",
      "n_step=267001/310107, ave loss=2.0205505046334293\n",
      "\n",
      "batch 2195/44301:   5%|▍         | 2189/44301 [00:12<03:50, 183.02it/s]\n",
      "n_step=268001/310107, ave loss=2.0350480386444527\n",
      "\n",
      "batch 3195/44301:   7%|▋         | 3181/44301 [00:18<03:57, 173.17it/s]\n",
      "n_step=269001/310107, ave loss=2.052968123900362\n",
      "\n",
      "batch 4195/44301:   9%|▉         | 4185/44301 [00:23<04:04, 164.20it/s]\n",
      "n_step=270001/310107, ave loss=2.0408613839798835\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "\tI ploncagpe, was a chame, ald deet is oncetiky a fimed thattmad they tas more thint at oke wentid of Harry he Frine've to tithay yor, Harry bussrist.,\"\n",
      "\tFin evousss, elly a whoumt't't?\" seary, Were on Iwr and, at a rot same port Dumll' tofflabe ainge and enex.  Whe of rormxipnone,\" retintined tably tily sater heringe inting lind werno ston ghighed trork up.  \"I', to nome agond ese to ary butorp coirsk, es?\" said samned han is Rontisaping sous Po was they?\"\n",
      "\"And sil; his nlf louther.  Weesen g\n",
      "\n",
      "\n",
      "batch 5195/44301:  12%|█▏        | 5182/44301 [00:29<03:57, 164.92it/s]\n",
      "n_step=271001/310107, ave loss=2.047739692424506\n",
      "\n",
      "batch 6195/44301:  14%|█▍        | 6178/44301 [00:36<03:56, 161.32it/s]\n",
      "n_step=272001/310107, ave loss=2.06394653526589\n",
      "\n",
      "batch 7195/44301:  16%|█▌        | 7186/44301 [00:42<04:01, 153.98it/s]\n",
      "n_step=273001/310107, ave loss=2.104943873946749\n",
      "\n",
      "batch 8195/44301:  18%|█▊        | 8186/44301 [00:49<05:05, 118.31it/s]\n",
      "n_step=274001/310107, ave loss=2.0521650804560942\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 9195/44301:  21%|██        | 9193/44301 [00:55<04:15, 137.58it/s]\n",
      "n_step=275001/310107, ave loss=2.0164145160208706\n",
      "\n",
      "batch 10195/44301:  23%|██▎       | 10185/44301 [01:01<03:03, 185.68it/s]\n",
      "n_step=276001/310107, ave loss=2.0326179074996773\n",
      "\n",
      "batch 11195/44301:  25%|██▌       | 11181/44301 [01:07<03:03, 180.03it/s]\n",
      "n_step=277001/310107, ave loss=2.063197672585171\n",
      "\n",
      "batch 12195/44301:  27%|██▋       | 12177/44301 [01:12<02:51, 186.92it/s]\n",
      "n_step=278001/310107, ave loss=2.066483680703212\n",
      "\n",
      "batch 13195/44301:  30%|██▉       | 13184/44301 [01:18<02:48, 184.50it/s]\n",
      "n_step=279001/310107, ave loss=2.061046709172936\n",
      "\n",
      "batch 14195/44301:  32%|███▏      | 14194/44301 [01:23<02:58, 168.43it/s]\n",
      "n_step=280001/310107, ave loss=2.0396501656479002\n",
      "\n",
      "\n",
      "\n",
      " . Dand to be for a gowed the davet han the kiding ledson ing prond nighis's.  Mund, of co comt shoin't kerst.  \"MGdale f cuwdo no heve think swon. Nes, of the.  Mocaes ton we to Beving torvited murtnke yoy.  Yuyed is of maans, somy of paly.  \"I goint wast Fmadm.\n",
      "\t\"There exceo the the fretailever.\n",
      "HCEREADGrrEIREIw no hed etpred yoo-ginifor foundy.\n",
      "Tevouserted the the bettea repiooppomer taburoud were foocoocter mestor swof role costertheret thitherwag mig has ownvils, towns Gagainm Ot.\n",
      "\"Harry gl\n",
      "\n",
      "\n",
      "batch 15195/44301:  34%|███▍      | 15192/44301 [01:29<02:45, 176.06it/s]\n",
      "n_step=281001/310107, ave loss=2.053089658580199\n",
      "\n",
      "batch 16195/44301:  37%|███▋      | 16177/44301 [01:35<02:39, 176.00it/s]\n",
      "n_step=282001/310107, ave loss=2.0268108891609162\n",
      "\n",
      "batch 17195/44301:  39%|███▉      | 17185/44301 [01:40<02:34, 176.02it/s]\n",
      "n_step=283001/310107, ave loss=2.026234795091724\n",
      "\n",
      "batch 18195/44301:  41%|████      | 18193/44301 [01:46<02:26, 178.48it/s]\n",
      "n_step=284001/310107, ave loss=2.0143471339070227\n",
      "\n",
      "batch 19195/44301:  43%|████▎     | 19188/44301 [01:52<02:15, 185.40it/s]\n",
      "n_step=285001/310107, ave loss=2.027505398726477\n",
      "\n",
      "batch 20195/44301:  46%|████▌     | 20180/44301 [01:57<02:16, 176.14it/s]\n",
      "n_step=286001/310107, ave loss=2.008885539584669\n",
      "\n",
      "batch 21195/44301:  48%|████▊     | 21178/44301 [02:03<02:05, 184.08it/s]\n",
      "n_step=287001/310107, ave loss=2.002705735281484\n",
      "\n",
      "batch 22195/44301:  50%|█████     | 22194/44301 [02:09<02:03, 179.36it/s]\n",
      "n_step=288001/310107, ave loss=1.9880720464985626\n",
      "\n",
      "batch 23195/44301:  52%|█████▏    | 23180/44301 [02:14<02:05, 168.66it/s]\n",
      "n_step=289001/310107, ave loss=2.003769718340931\n",
      "\n",
      "batch 24195/44301:  55%|█████▍    | 24184/44301 [02:20<01:56, 173.15it/s]\n",
      "n_step=290001/310107, ave loss=2.0150557694462723\n",
      "\n",
      "\n",
      "\n",
      "  \"ing Prover trearsnhing jure, hamets, and the the up bu dum the besousparos,\" I arred pasifomk's for camane..\"\n",
      "\"I't a grothr.\n",
      "\"Mudy creall somitve to top the pat the martunoins, want noury as off mig the a prowerad anvy.\n",
      "\"Gey it threne to wood and no will hat.\n",
      "\"Roble.  Harry Wort ooky tereed you lenced shaautwer are fated of 'aimauding on't, swagon's the.  The was jus entigg redussull fill\n",
      "\tDudignon towd vowa. big beirigid heant the Harry tobels fiaguid se -Hagrort',\" slaid tus, was of's - hem\n",
      "\n",
      "\n",
      "batch 25195/44301:  57%|█████▋    | 25188/44301 [02:26<01:52, 169.58it/s]\n",
      "n_step=291001/310107, ave loss=2.0223021406356385\n",
      "\n",
      "batch 26195/44301:  59%|█████▉    | 26179/44301 [02:32<01:40, 181.10it/s]\n",
      "n_step=292001/310107, ave loss=2.024432110132413\n",
      "\n",
      "batch 27195/44301:  61%|██████▏   | 27184/44301 [02:38<01:40, 169.71it/s]\n",
      "n_step=293001/310107, ave loss=2.0325727128552704\n",
      "\n",
      "batch 28195/44301:  64%|██████▎   | 28182/44301 [02:44<01:35, 169.12it/s]\n",
      "n_step=294001/310107, ave loss=2.027313495690621\n",
      "\n",
      "batch 29195/44301:  66%|██████▌   | 29182/44301 [02:50<01:12, 207.12it/s]\n",
      "n_step=295001/310107, ave loss=2.0398589958061004\n",
      "\n",
      "batch 30195/44301:  68%|██████▊   | 30175/44301 [02:54<01:05, 216.94it/s]\n",
      "n_step=296001/310107, ave loss=2.0264855054879813\n",
      "\n",
      "batch 31195/44301:  70%|███████   | 31182/44301 [02:59<00:58, 223.08it/s]\n",
      "n_step=297001/310107, ave loss=2.0076569231190486\n",
      "\n",
      "batch 32195/44301:  73%|███████▎  | 32182/44301 [03:03<01:07, 180.86it/s]\n",
      "n_step=298001/310107, ave loss=2.009804667612311\n",
      "\n",
      "batch 33195/44301:  75%|███████▍  | 33186/44301 [03:08<00:54, 203.75it/s]\n",
      "n_step=299001/310107, ave loss=2.00516564547526\n",
      "\n",
      "batch 34195/44301:  77%|███████▋  | 34184/44301 [03:14<01:00, 167.31it/s]\n",
      "n_step=300001/310107, ave loss=1.988083297631295\n",
      "\n",
      "\n",
      "\n",
      "  \"Idr rorce deemes himc't whew bry't'tt the a Mady neviht.\n",
      "\"The eulk he had he exthe op couldy ande spating me Horghing isblanly wealr roded thoum it hefe torm, her.\n",
      "Te Por.\n",
      "\"Handy flargen peas thot he Dumlathin's a Cowns the hare Mledons, a casaid extruth,\" sooussinked tout Povedmed to hely more were shatedy. Man.\n",
      "\"We agable of had wen worturnk; to himl fluly,\"  Thes stay they, misake went mele Grat ou back save got dolaidering the a port Rred hizling wliked of as and, whro bf inking, eass oth\n",
      "\n",
      "\n",
      "batch 35195/44301:  79%|███████▉  | 35184/44301 [03:20<00:49, 184.66it/s]\n",
      "n_step=301001/310107, ave loss=1.9617948632695665\n",
      "\n",
      "batch 36195/44301:  82%|████████▏ | 36175/44301 [03:25<00:38, 211.91it/s]\n",
      "n_step=302001/310107, ave loss=1.971218665474108\n",
      "\n",
      "batch 37195/44301:  84%|████████▍ | 37184/44301 [03:30<00:41, 172.75it/s]\n",
      "n_step=303001/310107, ave loss=1.9724533299520428\n",
      "\n",
      "batch 38195/44301:  86%|████████▌ | 38183/44301 [03:36<00:35, 169.95it/s]\n",
      "n_step=304001/310107, ave loss=1.9895212566214895\n",
      "\n",
      "batch 39195/44301:  88%|████████▊ | 39191/44301 [03:42<00:30, 165.41it/s]\n",
      "n_step=305001/310107, ave loss=1.9826422241928308\n",
      "\n",
      "batch 40195/44301:  91%|█████████ | 40183/44301 [03:48<00:24, 168.28it/s]\n",
      "n_step=306001/310107, ave loss=2.0051903233577053\n",
      "\n",
      "batch 41195/44301:  93%|█████████▎| 41190/44301 [03:54<00:17, 180.27it/s]\n",
      "n_step=307001/310107, ave loss=1.9650252172853024\n",
      "\n",
      "batch 42195/44301:  95%|█████████▌| 42189/44301 [04:00<00:12, 171.34it/s]\n",
      "n_step=308001/310107, ave loss=1.9713104971447717\n",
      "\n",
      "batch 43195/44301:  97%|█████████▋| 43177/44301 [04:06<00:07, 158.50it/s]\n",
      "n_step=309001/310107, ave loss=1.9676757298566794\n",
      "\n",
      "batch 44195/44301: 100%|█████████▉| 44180/44301 [04:12<00:00, 179.90it/s]\n",
      "n_step=310001/310107, ave loss=1.96838597808703\n",
      "\n",
      "\n",
      "\n",
      "  \"\n",
      "'Stre, Musstot, neacoo grow hat hhevard spulve doson fime heme, pack athing had to he benthe!\"\n",
      "gag I wes illaos hiy he bears helbly, vans.\"  Sated yo yuugh of dech whing whoudy ging poly bank whury womeyaved ay fwrrailag, - blat and fufon'll-Mad mais heme to betin in walkoo tho of Dupmotuter wat oth Harry? T Hedrnged ind?\"\n",
      "The caice the to proald treaid, a nfang . . \n",
      "Che poodir, sour fen iide the himthaiviry, is eid Cogme and wit uch FSGryor.\n",
      "\"Hagh fore sels My zehins,\" sacess of a... He kid\n",
      "\n",
      "\n",
      "batch 44301/44301: 100%|██████████| 44301/44301 [04:13<00:00, 175.01it/s]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "rnn = RNN(in_dim=hpdata.get_encoder().size, out_dim=hpdata.get_encoder().size, hidden_dim=100, \n",
    "          kernel_h_initializer=kernel_h_initializer, \n",
    "          bias_h_initializer=bias_h_initializer, \n",
    "          kernel_o_initializer=kernel_o_initializer, \n",
    "          bias_o_initializer=bias_o_initializer, \n",
    "          kernel_regularizer=kernel_regularizer, \n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "layers = [rnn]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "lr_initial = 0.1\n",
    "#optimizer = SGDOptimizer(lr_schedule=LRConstantSchedule(lr_initial))\n",
    "optimizer = AdaGradOptimizer(lr_schedule=LRConstantSchedule(lr_initial))\n",
    "\n",
    "n_epochs = 7\n",
    "batch_size = 25\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "verbose = 2\n",
    "\n",
    "synhthetizer = Synhthetizer(rnn, onehot_encoder)\n",
    "ts = 500\n",
    "#sequence= synhthetizer(ts=2000, init_idx=hpdata.encode(np.array([\".\"]))[0])\n",
    "#print(\"\".join(hpdata.decode(sequence.flatten())))\n",
    "\n",
    "synth_params = {\"synhthetizer\" : synhthetizer, \"ts\" : ts, \"hpdata\": hpdata, \"eol_char\": \".\"}\n",
    "\n",
    "history = model.fit_custom_rnn(x_train, y_train, n_epochs, batch_size, verbose, **synth_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, filename):\n",
    "    plt.plot(history[\"loss_train\"], label=\"train\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Loss vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    path = f\"{filename}_losses.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lr(history, filename):\n",
    "    plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning rate vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.legend()\n",
    "    path = f\"{filename}_lrs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(history, filename=\"rnn\")\n",
    "plot_lr(history, filename=\"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Noased resst's,\"\n",
      "gor stow!\"\n",
      "\"The whour as he. . . . and revers the crookedins a wark the gorng in stay blainvel, the whree pold M.\"\n",
      "Dum. houm, whins tobl Hor oulded arres acdor Sbald.  \"It to hat injorced quny.  Vosert uper.  The ward hemp,\" To got the Dorgingent ling's andl, imt had was olkee lacke whioness.\n",
      "\"Momelaid and had jutht fiding to the grot the onvirinfon ofle was larker ains to heand wade dosted yse emumafuce.  \"Whe kily sher udrid bryled ordreer, a stoody kixla's Mred bot lo Manel toswing cormen prone thie hifure,\" \"CtI an sels.. \"He that fart taked waitl hered ands he yo twere sach did Myer stoucker could blear, Ear wan hat' I yaursile lawt mauat.  Harry caint ked.  Whe Ee the the bat we in youlch pof waght the loul swan the gacken.\n",
      "\"Her Mmalfinged gund Ront furthted Bugamkione of purtag' \"fund.  \"Hagrr at they fuw of what pulbose t.  Ary, anl thes po quid aldves. \"Calar.\n",
      "\"Hatre ben of agl Ror.  \"Yarryud, ha.\"\n",
      "\"Durmabandlury ilf s Hackided.\n",
      "\"Lund bust and the bug be Mrm\n"
     ]
    }
   ],
   "source": [
    "synhthetizer = Synhthetizer(rnn, onehot_encoder)\n",
    "sequence= synhthetizer(ts=1000, init_idx=hpdata.encode(np.array([\".\"]))[0])\n",
    "print(\"\".join(hpdata.decode(sequence.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
