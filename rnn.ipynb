{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "import hyperopt\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation, Activation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense, BatchNormalization\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule\n",
    "from grad_check import eval_numerical_gradient, eval_numerical_gradient_array, numerical_gradient_check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPData():\n",
    "    def __init__(self, path_to_file):\n",
    "        \"\"\" Init.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_to_file : str\n",
    "            Path to text file.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        # read text file\n",
    "        with open(path_to_file, 'r') as f:\n",
    "            self.book_str = f.read()\n",
    "        \n",
    "        # str to chars\n",
    "        book_data = list(self.book_str)\n",
    "        # chars to unique chars\n",
    "        book_chars = list(set(book_data))\n",
    "        \n",
    "        # all chars as np\n",
    "        self.book_data = np.array(book_data)\n",
    "        # uniqe chars as np\n",
    "        self.book_chars = np.array(book_chars)\n",
    "    \n",
    "    def get_encoder(self,):\n",
    "        \"\"\" Returns encoder, i.e.: unique chars.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        book_chars : np.ndarray of shape (n_unique_chars, )\n",
    "            The encoder as np.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return self.book_chars\n",
    "    \n",
    "    def char_to_idx(self, char):\n",
    "        \"\"\" Convert a char to an index from the encoder np array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        char : str\n",
    "            A char.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The index repre of char, of shape (,).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return np.argwhere(char == self.book_chars).flatten()[0]\n",
    "    \n",
    "    def idx_to_char(self, idx):\n",
    "        \"\"\" Convert an index to char in the encoder np array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index repr of a char.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The char.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        return self.book_chars[idx]\n",
    "    \n",
    "    def encode(self, decoding):\n",
    "        \"\"\" Encode a sequence of chars into a sequence of indices based on the encoder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        chars : np.ndarray\n",
    "            The sequence of chars, of shape (n_chars,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        encoding : np.ndarray\n",
    "            The sequence of index representation of the chars, of shape (n_chars,)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        encoding = []\n",
    "        \n",
    "        for d in decoding:\n",
    "            encoding.append(self.char_to_idx(d))\n",
    "            \n",
    "        encoding = np.array(encoding)\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    def decode(self, encoding):\n",
    "        \"\"\" Decode a sequence of indices into a sequence of chars based on the encoder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        encoding : np.ndarray\n",
    "            The sequence of index representation of the chars, of shape (n_chars,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decoding : np.ndarray\n",
    "            The sequence of chars, of shape (n_chars,)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        decoding = []\n",
    "        \n",
    "        for e in encoding:\n",
    "            decoding.append(self.idx_to_char(e))\n",
    "            \n",
    "        decoding = np.array(decoding)\n",
    "        \n",
    "        return decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    def __init__(self, length):\n",
    "        # length of one-hot encoding\n",
    "        self.length = length\n",
    "    \n",
    "    def __call__(self, x, encode=True):\n",
    "        \"\"\" Encode or decode a sequence x.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            The sequence of index representation of chars, of shape (n_chars,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        e or d: np.ndarray\n",
    "            The sequence of one-hot encoded vectors of chars, of shape (n_chars, length)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if encode:\n",
    "            e = np.zeros((x.shape[0], self.length))\n",
    "            e[np.arange(x.shape[0]), x] = 1\n",
    "            return e.astype(int)\n",
    "        else:\n",
    "            d = np.argwhere(one_hot_encoding == 1)[:,1]\n",
    "            return d.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Read, encode and decode data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,)\n",
      "['f' 'O' '\\t' 'n' '(' 'y' ';' 'H' 'm' 'B' '_' '3' 'S' 'o' 'l' ':' 'M' 'Q'\n",
      " 'Y' '\\n' '-' 'p' '1' 'x' 'u' 'C' 'e' '2' 'G' '.' 'Z' ',' '•' 'V' 'X' 'T'\n",
      " 'D' 'K' 'ü' 'q' 'J' '\"' 'g' '?' 'i' 'z' 't' 'c' ')' '7' 'v' 'd' 'L' 'N'\n",
      " \"'\" 'k' '0' '!' 'F' ' ' 'r' 'R' 'U' 'a' 'W' 'h' '}' '9' '4' '6' '/' 'P'\n",
      " 's' '^' 'j' 'w' 'b' 'I' 'A' 'E']\n",
      "['H' 'A' 'R' 'R' 'Y' ' ' 'P' 'O' 'T' 'T' 'E' 'R' ' ' 'A' 'N' 'D' ' ' 'T'\n",
      " 'H' 'E' ' ' 'G' 'O' 'B' 'L' 'E' 'T' ' ' 'O' 'F' ' ' 'F' 'I' 'R' 'E' '\\n'\n",
      " '\\n' 'C' 'H' 'A' 'P' 'T' 'E' 'R' ' ' 'O' 'N' 'E' ' ' '-' ' ' 'T' 'H' 'E'\n",
      " ' ' 'R' 'I' 'D' 'D' 'L' 'E' ' ' 'H' 'O' 'U' 'S' 'E' '\\n' '\\n' '\\t' 'T'\n",
      " 'h' 'e' ' ' 'v' 'i' 'l' 'l' 'a' 'g' 'e' 'r' 's' ' ' 'o' 'f' ' ' 'L' 'i'\n",
      " 't' 't' 'l' 'e' ' ' 'H' 'a' 'n' 'g' 'l' 'e' 'r' 'o' 'n' ' ' 's' 't' 'i'\n",
      " 'l' 'l' ' ' 'c' 'a' 'l' 'l' 'e' 'd' ' ' 'i' 't' ' ' '\"' 't' 'h' 'e' ' '\n",
      " 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'H' 'o' 'u' 's' 'e' ',' '\"' ' ' 'e' 'v' 'e'\n",
      " 'n' ' ' 't' 'h' 'o' 'u' 'g' 'h' ' ' 'i' 't' ' ' 'h' 'a' 'd' ' ' 'b' 'e'\n",
      " 'e' 'n' ' ' 'm' 'a' 'n' 'y' ' ' 'y' 'e' 'a' 'r' 's' ' ' 's' 'i' 'n' 'c'\n",
      " 'e' ' ' 't' 'h' 'e' ' ' 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'f' 'a' 'm' 'i' 'l'\n",
      " 'y' ' ' 'h']\n",
      "(80,)\n",
      "[ 7 78 61 61 18 59 71  1 35 35 79 61 59 78 53 36 59 35  7 79 59 28  1  9\n",
      " 52 79 35 59  1 58 59 58 77 61 79 19 19 25  7 78 71 35 79 61 59  1 53 79\n",
      " 59 20 59 35  7 79 59 61 77 36 36 52 79 59  7  1 62 12 79 19 19  2 35 65\n",
      " 26 59 50 44 14 14 63 42 26 60 72 59 13  0 59 52 44 46 46 14 26 59  7 63\n",
      "  3 42 14 26 60 13  3 59 72 46 44 14 14 59 47 63 14 14 26 51 59 44 46 59\n",
      " 41 46 65 26 59 61 44 51 51 14 26 59  7 13 24 72 26 31 41 59 26 50 26  3\n",
      " 59 46 65 13 24 42 65 59 44 46 59 65 63 51 59 76 26 26  3 59  8 63  3  5\n",
      " 59  5 26 63 60 72 59 72 44  3 47 26 59 46 65 26 59 61 44 51 51 14 26 59\n",
      "  0 63  8 44 14  5 59 65]\n",
      "['H' 'A' 'R' 'R' 'Y' ' ' 'P' 'O' 'T' 'T' 'E' 'R' ' ' 'A' 'N' 'D' ' ' 'T'\n",
      " 'H' 'E' ' ' 'G' 'O' 'B' 'L' 'E' 'T' ' ' 'O' 'F' ' ' 'F' 'I' 'R' 'E' '\\n'\n",
      " '\\n' 'C' 'H' 'A' 'P' 'T' 'E' 'R' ' ' 'O' 'N' 'E' ' ' '-' ' ' 'T' 'H' 'E'\n",
      " ' ' 'R' 'I' 'D' 'D' 'L' 'E' ' ' 'H' 'O' 'U' 'S' 'E' '\\n' '\\n' '\\t' 'T'\n",
      " 'h' 'e' ' ' 'v' 'i' 'l' 'l' 'a' 'g' 'e' 'r' 's' ' ' 'o' 'f' ' ' 'L' 'i'\n",
      " 't' 't' 'l' 'e' ' ' 'H' 'a' 'n' 'g' 'l' 'e' 'r' 'o' 'n' ' ' 's' 't' 'i'\n",
      " 'l' 'l' ' ' 'c' 'a' 'l' 'l' 'e' 'd' ' ' 'i' 't' ' ' '\"' 't' 'h' 'e' ' '\n",
      " 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'H' 'o' 'u' 's' 'e' ',' '\"' ' ' 'e' 'v' 'e'\n",
      " 'n' ' ' 't' 'h' 'o' 'u' 'g' 'h' ' ' 'i' 't' ' ' 'h' 'a' 'd' ' ' 'b' 'e'\n",
      " 'e' 'n' ' ' 'm' 'a' 'n' 'y' ' ' 'y' 'e' 'a' 'r' 's' ' ' 's' 'i' 'n' 'c'\n",
      " 'e' ' ' 't' 'h' 'e' ' ' 'R' 'i' 'd' 'd' 'l' 'e' ' ' 'f' 'a' 'm' 'i' 'l'\n",
      " 'y' ' ' 'h']\n"
     ]
    }
   ],
   "source": [
    "path_to_file = \"data/hp/goblet_book.txt\"\n",
    "hpdata = HPData(path_to_file=path_to_file)\n",
    "print(hpdata.get_encoder().shape)\n",
    "print(hpdata.get_encoder())\n",
    "x = hpdata.book_data[:200]\n",
    "print(x)\n",
    "encoding = hpdata.encode(x)\n",
    "print(hpdata.get_encoder().shape)\n",
    "print(encoding)\n",
    "decoding = hpdata.decode(encoding)\n",
    "print(decoding)\n",
    "\n",
    "np.testing.assert_array_equal(decoding, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-ho encode and decode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 80)\n",
      "(200,)\n",
      "1\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "25\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(length=hpdata.get_encoder().size)\n",
    "one_hot_encoding = onehot_encoder(encoding, encode=True)\n",
    "print(one_hot_encoding.shape)\n",
    "one_hot_decoding = onehot_encoder(one_hot_encoding, encode=False)\n",
    "print(one_hot_decoding.shape)\n",
    "\n",
    "np.testing.assert_array_equal(one_hot_decoding, encoding)\n",
    "print(one_hot_decoding[7])\n",
    "print(one_hot_encoding[7])\n",
    "\n",
    "print(one_hot_decoding[37])\n",
    "print(one_hot_encoding[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' 'a']\n",
      "(80,)\n",
      "[29 63]\n",
      "['.' 'a']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]]\n",
      "(2, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[63]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\".\", \"a\"])\n",
    "print(x)\n",
    "encoding = hpdata.encode(x)\n",
    "print(hpdata.get_encoder().shape)\n",
    "print(encoding)\n",
    "decoding = hpdata.decode(encoding)\n",
    "print(decoding)\n",
    "\n",
    "np.testing.assert_array_equal(decoding, x)\n",
    "\n",
    "one_hot_encoding = onehot_encoder(encoding, encode=True)\n",
    "print(one_hot_encoding)\n",
    "print(one_hot_encoding.shape)\n",
    "np.argwhere(hpdata.get_encoder() == \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhActivation(Activation):\n",
    "    \"\"\" Tanh activation.\n",
    "    Can be followed by virtually anything.\n",
    "    Inherits everything from class Activation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cache : dict\n",
    "        Run-time cache of attibutes such as gradients.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__()\n",
    "        Constuctor.\n",
    "    forward(z)\n",
    "        Activates the linear transformation of the layer, and\n",
    "        forward propagates activation. Activation is tanh.\n",
    "    backward(g)\n",
    "        Backpropagates incoming gradient into the layer, based on the tanh activation.\n",
    "    __repr__()\n",
    "        Returns the string representation of class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        \"\"\" Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\" Activates the linear transformation of the layer, and\n",
    "        forward propagates activation. Activation is tanh.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : numpy.ndarray\n",
    "            Linear transformation of layer.\n",
    "            Shape is unknown here, but will usually be\n",
    "            (batch size, this layer output dim = next layer input dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            ReLU activation.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        a = np.tanh(z)\n",
    "        self.cache[\"a\"] = deepcopy(a)\n",
    "        return a\n",
    "\n",
    "    def backward(self, g_in):\n",
    "        \"\"\" Backpropagates incoming gradient into the layer, based on the tanh activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g_in : numpy.ndarray\n",
    "            Incoming gradient to the activation.\n",
    "            Shape is unknown here, but will usually be\n",
    "            (batch size, this layer output dim = next layer input dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Gradient of activation.\n",
    "            Shape is unknown here, but will usually be\n",
    "            (batch size, this layer output dim = next layer input dim)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        a = deepcopy(self.cache[\"a\"])\n",
    "        g_out = (1 - np.power(a, 2)) * g_in\n",
    "        return g_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Returns the string representation of class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        repr_str : str\n",
    "            The string representation of the class.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        repr_str = \"tanh\"\n",
    "        return repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_relu_activation passed\n"
     ]
    }
   ],
   "source": [
    "def test_tanh_activation():\n",
    "    \n",
    "    tanh_activation = TanhActivation()\n",
    "    np.random.seed(231)\n",
    "    x = np.random.randn(5, 10)\n",
    "    g_in = np.random.randn(*x.shape)\n",
    "    fx = lambda x: TanhActivation.forward(tanh_activation, x)\n",
    "    g_out_num = eval_numerical_gradient_array(fx, x, g_in)\n",
    "    g_out = tanh_activation.backward(g_in)\n",
    "    np.testing.assert_array_almost_equal(g_out, g_out_num, decimal=6)\n",
    "\n",
    "    print(\"test_relu_activation passed\")\n",
    "    \n",
    "test_tanh_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \"\"\" Many-to-many.\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, \n",
    "                 kernel_h_initializer, bias_h_initializer,\n",
    "                 kernel_o_initializer, bias_o_initializer,\n",
    "                 kernel_regularizer, \n",
    "                 activation_h, activation_o):\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_h_initializer = kernel_h_initializer\n",
    "        self.bias_h_initializer = bias_h_initializer\n",
    "        self.kernel_o_initializer = kernel_o_initializer\n",
    "        self.bias_o_initializer = bias_o_initializer\n",
    "\n",
    "        self.u = kernel_h_initializer.initialize(size=(in_dim, hidden_dim))\n",
    "        self.w = kernel_h_initializer.initialize(size=(hidden_dim, hidden_dim))\n",
    "        self.b = bias_h_initializer.initialize(size=(1, hidden_dim))\n",
    "        \n",
    "        self.v = kernel_o_initializer.initialize(size=(hidden_dim, out_dim))\n",
    "        self.c = bias_o_initializer.initialize(size=(1, out_dim))\n",
    "        \n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "        self.activation_h = activation_h\n",
    "        self.activation_o = activation_o\n",
    "\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "        self.h_shape = (1, hidden_dim)\n",
    "        self.cache[\"h\"] = np.zeros(self.h_shape)\n",
    "\n",
    "        self.has_learnable_params = True\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        h = deepcopy(self.cache[\"h\"])\n",
    "        #h = np.zeros(self.h_shape)\n",
    "        self.cache[\"x\"] = deepcopy(x)\n",
    "        #h = np.zeros(self.h_shape)\n",
    "        h_concat = np.zeros((x.shape[0], h.shape[1]))\n",
    "        a_concat = np.zeros((x.shape[0], h.shape[1]))\n",
    "        assert h.shape == (1, self.hidden_dim)\n",
    "        \n",
    "        for idx, x_ in enumerate(x):\n",
    "            x_ = x_.reshape(1,-1)\n",
    "            assert x_.shape == (1,self.in_dim)\n",
    "            a = np.dot(x_, self.u) + np.dot(h, self.w) + self.b\n",
    "            a_concat[idx] = a.reshape(1,-1)\n",
    "            assert a.shape == (1, self.hidden_dim)\n",
    "            h = self.activation_h.forward(a)\n",
    "            #print(self.activation_h.cache[\"a\"].shape)\n",
    "            h_concat[idx] = deepcopy(h)\n",
    "            assert h.shape == (1, self.hidden_dim)\n",
    "        \n",
    "        # assure good dims for backprop -> only used for 1 vector, so should be ok\n",
    "        # assure good dims for backprop\n",
    "        #h_concat_2 = self.activation_h.forward(a_concat)\n",
    "        #print(self.activation_h.cache[\"a\"].shape)\n",
    "        #np.testing.assert_array_equal(h_concat, h_concat_2)\n",
    "        self.cache[\"h\"] = deepcopy(h)\n",
    "        self.cache[\"h_concat\"] = deepcopy(h_concat)\n",
    "        self.cache[\"a_concat\"] = deepcopy(a_concat)\n",
    "        assert h_concat.shape == (x.shape[0], h.shape[1])\n",
    "        o = np.dot(h_concat, self.v) + self.c\n",
    "        assert o.shape == (x.shape[0], self.out_dim), f\"o.shape={o.shape}\"\n",
    "        p = self.activation_o.forward(o)\n",
    "        #print(self.activation_o.cache[\"a\"].shape)\n",
    "        \n",
    "        assert p.shape == (x.shape[0], self.out_dim)\n",
    "        return p\n",
    "    \n",
    "    def backward(self, g_in, **params):\n",
    "        # x.shape = (x.shape[0], in_dim)\n",
    "        x = deepcopy(self.cache[\"x\"])\n",
    "        # h_concat.shape = (x.shape[0], hidden_dim)\n",
    "        h_concat = deepcopy(self.cache[\"h_concat\"])\n",
    "        a_concat = deepcopy(self.cache[\"a_concat\"])\n",
    "        \n",
    "        # g_in.shape = (batch_size, )\n",
    "        assert g_in.shape == (x.shape[0], ), f\"g_in.shape={g_in.shape}\"\n",
    "        # g_a_o.shape = (batch_size, out_dim)\n",
    "        g_a_o = self.activation_o.backward(g_in)\n",
    "        assert g_a_o.shape == (x.shape[0], self.out_dim)\n",
    "        \n",
    "        # g_h_concat.shape = (batch_size, hidden_dim)\n",
    "        g_h_concat = np.zeros((x.shape[0], self.hidden_dim))\n",
    "        \n",
    "        # v.shape = (hidden_dim, out_dim)\n",
    "        # (1,hidden_dim) = (1,out_dim) * (hidden_dim, out_dim).T\n",
    "        g_h_concat[-1] = np.dot(g_a_o[-1].reshape(1,-1), self.v.T)\n",
    "        assert np.dot(g_a_o[-1].reshape(1,-1), self.v.T).shape == (1,self.hidden_dim)\n",
    "        \n",
    "        g_a = np.zeros((x.shape[0], self.hidden_dim))\n",
    "        # (1, hidden_dim) = (1, hidden_dim) * (1, hidden_dim)\n",
    "        # change cache\n",
    "        _ = self.activation_h.forward(a_concat[-1].reshape(1,-1))\n",
    "        g_a[-1] = self.activation_h.backward(g_h_concat[-1]).reshape(1,-1)\n",
    "        assert self.activation_h.backward(g_h_concat[-1].reshape(1,-1)).shape == (1, self.hidden_dim)\n",
    "        \n",
    "        for t in reversed(range(x.shape[0]-1)):\n",
    "            # (1,hidden_dim) = (1,out_dim) * (hidden_dim, out_dim).T\n",
    "            # \\+ (1,hidden_dim) * (hidden_dim, hidden_dim), maybe w.T?\n",
    "            g_h_concat[t] = np.dot(g_a_o[t].reshape(1,-1), self.v.T) \\\n",
    "                + np.dot(g_a[t+1].reshape(1,-1), self.w)\n",
    "            # change cache\n",
    "            _ = self.activation_h.forward(a_concat[t].reshape(1,-1))\n",
    "            g_a[t] = self.activation_h.backward(g_h_concat[t])\n",
    "            assert self.activation_h.backward(g_h_concat[t]).shape == (1, self.hidden_dim)\n",
    "        \n",
    "        #print(g_h_concat)\n",
    "        assert g_h_concat.shape == (x.shape[0], self.hidden_dim)\n",
    "        assert g_a.shape == (x.shape[0], self.hidden_dim)\n",
    "        \n",
    "        # (hidden_dim, out_dim) = (x.shape[0], hidden_dim).T * (x.shape[0], out_dim)\n",
    "        g_v = np.dot(h_concat.T, g_a_o)\n",
    "        assert g_v.shape == (self.hidden_dim, self.out_dim)\n",
    "        self.grads[\"dv\"] = deepcopy(g_v)\n",
    "        \n",
    "        # Auxiliar h matrix that includes h_prev\n",
    "        h_aux = np.zeros(h_concat.shape)\n",
    "        #h_init = np.zeros((1, self.hidden_dim))\n",
    "        #h_aux[0, :] = h_init\n",
    "        h_aux[0] = h_concat[-1].reshape(1,-1)\n",
    "        h_aux[1:] = h_concat[0:-1]\n",
    "        assert h_aux.shape == (x.shape[0], self.hidden_dim)\n",
    "        \n",
    "        # (hidden_dim, hidden_dim) = (x.shape[0], hidden_dim).T * (x.shape[0], hidden_dim)\n",
    "        g_w = np.dot(h_aux.T, g_a)\n",
    "        assert g_w.shape == (self.hidden_dim, self.hidden_dim)\n",
    "        self.grads[\"dw\"] = deepcopy(g_w)\n",
    "        \n",
    "        # (in_dim, hidden_dim) = (x.shape[0], in_dim).T * (x.shape[0], hidden_dim)\n",
    "        g_u = np.dot(x.T, g_a)\n",
    "        assert g_u.shape == (self.in_dim, self.hidden_dim)\n",
    "        self.grads[\"du\"] = deepcopy(g_u)\n",
    "        \n",
    "        # (1, hidden_dim) = sum((x.shape[0], self.hidden_dim), axis=0)\n",
    "        g_b = np.sum(g_a, axis=0).reshape(1,-1)\n",
    "        assert g_b.shape == (1, self.hidden_dim), f\"g_b.shape={g_b.shape}\"\n",
    "        self.grads[\"db\"] = deepcopy(g_b)\n",
    "        \n",
    "        # (1, out_dim) = sum((x.shape[0], self.out_dim), axis=0)\n",
    "        g_c = np.sum(g_a_o, axis=0).reshape(1,-1)\n",
    "        assert g_c.shape == (1, self.out_dim)\n",
    "        self.grads[\"dc\"] = deepcopy(g_c)\n",
    "        \n",
    "        # compute downstream grad!\n",
    "        return None\n",
    "        \n",
    "    def if_has_learnable_params(self, ):    \n",
    "        return self.has_learnable_params\n",
    "    \n",
    "    def get_u(self, ):\n",
    "        return deepcopy(self.u)\n",
    "\n",
    "    def get_w(self, ):\n",
    "        return deepcopy(self.w)\n",
    "    \n",
    "    def get_b(self, ):\n",
    "        return deepcopy(self.b)\n",
    "    \n",
    "    def get_v(self, ):\n",
    "        return deepcopy(self.v)\n",
    "    \n",
    "    def get_c(self, ):\n",
    "        return deepcopy(self.c)\n",
    "\n",
    "    def get_learnable_params(self):\n",
    "        return {\n",
    "            \"u\": self.get_u(), \"w\": self.get_w(), \"b\": self.get_b(), \n",
    "            \"v\": self.get_v(), \"c\": self.get_c()\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def set_u(self, u):\n",
    "        self.u = deepcopy(u)\n",
    "\n",
    "    def set_w(self, w):\n",
    "        self.w = deepcopy(w)\n",
    "    \n",
    "    def set_b(self, b):\n",
    "        self.b = deepcopy(b)\n",
    "    \n",
    "    def set_v(self, v):\n",
    "        self.v = deepcopy(v)\n",
    "    \n",
    "    def set_c(self, c):\n",
    "        self.c = deepcopy(c)\n",
    "\n",
    "    def set_learnable_params(self, **learnable_params):\n",
    "        self.set_u(learnable_params[\"u\"])\n",
    "        self.set_w(learnable_params[\"w\"])\n",
    "        self.set_b(learnable_params[\"b\"])\n",
    "        self.set_v(learnable_params[\"v\"])\n",
    "        self.set_c(learnable_params[\"c\"])\n",
    "\n",
    "    def get_du(self, ):\n",
    "        if \"du\" in self.grads.keys():\n",
    "            du = self.grads[\"du\"]\n",
    "            ret = deepcopy(du)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_dw(self, ):\n",
    "        if \"dw\" in self.grads.keys():\n",
    "            dw = self.grads[\"dw\"]\n",
    "            ret = deepcopy(dw)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_db(self, ):\n",
    "        if \"db\" in self.grads.keys():\n",
    "            db = self.grads[\"db\"]\n",
    "            ret = deepcopy(db)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_dv(self, ):\n",
    "        if \"dv\" in self.grads.keys():\n",
    "            dv = self.grads[\"dv\"]\n",
    "            ret = deepcopy(dv)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_dc(self, ):\n",
    "        if \"dc\" in self.grads.keys():\n",
    "            dc = self.grads[\"dc\"]\n",
    "            ret = deepcopy(dc)\n",
    "        else:\n",
    "            ret = None\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_learnable_params_grads(self):\n",
    "        return {\n",
    "            \"du\": self.get_du(), \"dw\": self.get_dw(), \"db\": self.get_db(),\n",
    "            \"dv\": self.get_dv(), \"dc\": self.get_dc()\n",
    "        }\n",
    "    \n",
    "    def if_has_learnable_params(self, ):\n",
    "        return self.has_learnable_params\n",
    "        \n",
    "    def get_reg_loss(self, ):\n",
    "        return 0.0\n",
    "    \n",
    "    def __repr__(self, ):\n",
    "        repr_str = \"rnn: \\n\" \\\n",
    "                   + f\"\\t shape -- in: {self.in_dim}, out: {self.out_dim}, hidden: {self.hidden_dim}\\n\" \\\n",
    "                   + \"\\t u -- init: \" + self.kernel_h_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t w -- init: \" + self.kernel_h_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t b -- init: \" + self.bias_h_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t v -- init: \" + self.kernel_o_initializer.__repr__() + \"\\n\" \\\n",
    "                    + \"\\t c -- init: \" + self.bias_o_initializer.__repr__() + \"\\n\" \\\n",
    "                   + \", reg: \" + self.kernel_regularizer.__repr__() + \"\\n\" \\\n",
    "                   + \"\\t activation: \\n \\t hidden: \" + self.activation_h.__repr__() \\\n",
    "                    + \"\\t out: \" + self.activation_o.__repr__() + \"\\n\"\n",
    "        return repr_str\n",
    "    \n",
    "    \n",
    "class Synhthetizer():\n",
    "    def __init__(self, rnn, onehot_encoder):\n",
    "        self.rnn = rnn\n",
    "        self.onehot_encoder = onehot_encoder\n",
    "        self.h_concat = np.zeros(rnn.h_shape)\n",
    "    \n",
    "    def sample(self, lenght, p):\n",
    "        # select character from softmax weighted dist over all chars\n",
    "        return np.random.choice(range(lenght), size=1, replace=True, p=p.flatten())\n",
    "        \n",
    "    \n",
    "    def __call__(self, ts, init_idx):\n",
    "        \n",
    "        x = self.onehot_encoder(np.array([init_idx]).T, encode=True)\n",
    "        #print(x.shape)\n",
    "        assert x.shape == (1, self.onehot_encoder.length)\n",
    "        sequence = []\n",
    "        \n",
    "        for t in range(ts):\n",
    "            p = rnn.forward(x)\n",
    "            x_idx = self.sample(lenght=x.shape[1], p=p)\n",
    "            sequence.append(x_idx)\n",
    "            x = self.onehot_encoder(np.array([x_idx]).T, encode=True)\n",
    "    \n",
    "        return np.array(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad test\n",
    "\n",
    "Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: \n",
      "\t shape -- in: 80, out: 80, hidden: 5\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "layer=0, param_name=u\n",
      "max rel error=1.3751199180496758\n",
      "layer=0, param_name=w\n",
      "max rel error=0.3311997814964626\n",
      "layer=0, param_name=b\n",
      "max rel error=1.0955563435105435\n",
      "layer=0, param_name=v\n",
      "max rel error=0.062020333718575564\n",
      "layer=0, param_name=c\n",
      "max rel error=1.3892426963187314e-06\n",
      "test_grad_check passed\n"
     ]
    }
   ],
   "source": [
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "num_inputs = 10\n",
    "size = (num_inputs, hpdata.get_encoder().size)\n",
    "x = np.eye(hpdata.get_encoder().size)\n",
    "x = x[np.random.choice(x.shape[0], size=num_inputs)].astype(int)\n",
    "y = np.random.randint(hpdata.get_encoder().size, size=num_inputs)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "rnn = RNN(in_dim=hpdata.get_encoder().size, out_dim=hpdata.get_encoder().size, hidden_dim=5, \n",
    "          kernel_h_initializer=kernel_h_initializer, \n",
    "          bias_h_initializer=bias_h_initializer, \n",
    "          kernel_o_initializer=kernel_o_initializer, \n",
    "          bias_o_initializer=bias_o_initializer, \n",
    "          kernel_regularizer=kernel_regularizer, \n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "print(rnn)\n",
    "\n",
    "layers = [rnn]\n",
    "model = Model(layers)\n",
    "\n",
    "numerical_gradient_check_model(x, y, model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: \n",
      "\t shape -- in: 80, out: 80, hidden: 5\n",
      "\t u -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t w -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t b -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t v -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      "\t c -- init: normal ~ 1.000000 x N(0.000000, 0.010000^2)\n",
      ", reg: None\n",
      "\t activation: \n",
      " \t hidden: tanh\t out: softmax\n",
      "\n",
      "layer=0, param_name=u\n",
      "max rel error=1.6851094960056114\n",
      "layer=0, param_name=w\n",
      "max rel error=0.26394272299118826\n",
      "layer=0, param_name=b\n",
      "max rel error=0.5989718875354738\n",
      "layer=0, param_name=v\n",
      "max rel error=0.10395670299550329\n",
      "layer=0, param_name=c\n",
      "max rel error=8.106420009714596e-07\n",
      "test_grad_check passed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "x_chars = hpdata.book_data[:batch_size]\n",
    "y_chars = hpdata.book_data[1:batch_size+1]\n",
    "x_encoding = hpdata.encode(x_chars)\n",
    "y_encoding = hpdata.encode(y_chars)\n",
    "onehot_encoder = OneHotEncoder(length=hpdata.get_encoder().size)\n",
    "x_train = onehot_encoder(x_encoding, encode=True)\n",
    "y_train = y_encoding\n",
    "\n",
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "num_inputs = batch_size\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "rnn = RNN(in_dim=hpdata.get_encoder().size, out_dim=hpdata.get_encoder().size, hidden_dim=5, \n",
    "          kernel_h_initializer=kernel_h_initializer, \n",
    "          bias_h_initializer=bias_h_initializer, \n",
    "          kernel_o_initializer=kernel_o_initializer, \n",
    "          bias_o_initializer=bias_o_initializer, \n",
    "          kernel_regularizer=kernel_regularizer, \n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "print(rnn)\n",
    "\n",
    "layers = [rnn]\n",
    "model = Model(layers)\n",
    "\n",
    "numerical_gradient_check_model(x_train, y_train, model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_chars = hpdata.book_data\n",
    "y_chars = hpdata.book_data\n",
    "x_encoding = hpdata.encode(x_chars)\n",
    "y_encoding = hpdata.encode(y_chars)\n",
    "onehot_encoder = OneHotEncoder(length=hpdata.get_encoder().size)\n",
    "x_train = onehot_encoder(x_encoding, encode=True)\n",
    "#y_train = onehot_encoder(y_encoding, encode=True)\n",
    "y_train = y_encoding\n",
    "#print(x_train.shape)\n",
    "#print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107542, 80)\n",
      "(1107542,)\n",
      "starting epoch: 1 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667ef46b850948eeabb8c8a875ce0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step=1001/221505, ave loss=4.12835307074449\n",
      "n_step=2001/221505, ave loss=3.7219940172493\n",
      "n_step=3001/221505, ave loss=3.397007991145309\n",
      "n_step=4001/221505, ave loss=3.255552132025294\n",
      "n_step=5001/221505, ave loss=3.2232314810375313\n",
      "n_step=6001/221505, ave loss=3.20077377810724\n",
      "n_step=7001/221505, ave loss=3.1915159142199316\n",
      "n_step=8001/221505, ave loss=3.1693290036524764\n",
      "n_step=9001/221505, ave loss=3.2048821759068913\n",
      "n_step=10001/221505, ave loss=3.1986789333188335\n",
      "n_step=11001/221505, ave loss=3.2040383882270747\n",
      "n_step=12001/221505, ave loss=3.131800934204009\n",
      "n_step=13001/221505, ave loss=3.1656393523172457\n",
      "n_step=14001/221505, ave loss=3.1403428957834976\n",
      "n_step=15001/221505, ave loss=3.170656373607051\n",
      "n_step=16001/221505, ave loss=3.1352241328982693\n",
      "n_step=17001/221505, ave loss=3.1509140534615634\n",
      "n_step=18001/221505, ave loss=3.1426117050731603\n",
      "n_step=19001/221505, ave loss=3.142112346742293\n",
      "n_step=20001/221505, ave loss=3.1181409591086453\n",
      "n_step=21001/221505, ave loss=3.1363650102340475\n",
      "n_step=22001/221505, ave loss=3.106113503118883\n",
      "n_step=23001/221505, ave loss=3.1513180743969316\n",
      "n_step=24001/221505, ave loss=3.1451743593760964\n",
      "n_step=25001/221505, ave loss=3.1380427723096895\n",
      "n_step=26001/221505, ave loss=3.1457792027561475\n",
      "n_step=27001/221505, ave loss=3.145460556889734\n",
      "n_step=28001/221505, ave loss=3.1341884218833926\n",
      "n_step=29001/221505, ave loss=3.1520406803239562\n",
      "n_step=30001/221505, ave loss=3.105933575896822\n",
      "n_step=31001/221505, ave loss=3.105958739470677\n",
      "n_step=32001/221505, ave loss=3.1240646195272506\n",
      "n_step=33001/221505, ave loss=3.1588803709944586\n",
      "n_step=34001/221505, ave loss=3.1660426569001867\n",
      "n_step=35001/221505, ave loss=3.139616789069593\n",
      "n_step=36001/221505, ave loss=3.1235034958214563\n",
      "n_step=37001/221505, ave loss=3.132837380367221\n",
      "n_step=38001/221505, ave loss=3.117070089922573\n",
      "n_step=39001/221505, ave loss=3.092535908677103\n",
      "n_step=40001/221505, ave loss=3.0914989528650993\n",
      "n_step=41001/221505, ave loss=3.1048507280671678\n",
      "n_step=42001/221505, ave loss=3.101071305304277\n",
      "n_step=43001/221505, ave loss=3.1305849544237216\n",
      "n_step=44001/221505, ave loss=3.11696556875207\n",
      "starting epoch: 2 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf31270967445d980d5fcc5415093ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step=45001/221505, ave loss=3.1324386696715703\n",
      "n_step=46001/221505, ave loss=3.0906281657317147\n",
      "n_step=47001/221505, ave loss=3.1312304273997893\n",
      "n_step=48001/221505, ave loss=3.1347714973370264\n",
      "n_step=49001/221505, ave loss=3.157347300453797\n",
      "n_step=50001/221505, ave loss=3.1484843090629364\n",
      "n_step=51001/221505, ave loss=3.1561993349642106\n",
      "n_step=52001/221505, ave loss=3.1383403254551587\n",
      "n_step=53001/221505, ave loss=3.1687653954891237\n",
      "n_step=54001/221505, ave loss=3.181737846945337\n",
      "n_step=55001/221505, ave loss=3.184233494690813\n",
      "n_step=56001/221505, ave loss=3.13768514285809\n",
      "n_step=57001/221505, ave loss=3.1546897030977608\n",
      "n_step=58001/221505, ave loss=3.136291238575596\n",
      "n_step=59001/221505, ave loss=3.1499285090106777\n",
      "n_step=60001/221505, ave loss=3.1256369086844935\n",
      "n_step=61001/221505, ave loss=3.1434352507687837\n",
      "n_step=62001/221505, ave loss=3.130611023445767\n",
      "n_step=63001/221505, ave loss=3.142933711148758\n",
      "n_step=64001/221505, ave loss=3.11061139279663\n",
      "n_step=65001/221505, ave loss=3.132009542721454\n",
      "n_step=66001/221505, ave loss=3.088573869814767\n",
      "n_step=67001/221505, ave loss=3.131158782746706\n",
      "n_step=68001/221505, ave loss=3.1550366860487458\n",
      "n_step=69001/221505, ave loss=3.147774095637099\n",
      "n_step=70001/221505, ave loss=3.1224395539788596\n",
      "n_step=71001/221505, ave loss=3.137882441726439\n",
      "n_step=72001/221505, ave loss=3.1412426646630687\n",
      "n_step=73001/221505, ave loss=3.1375921985438806\n",
      "n_step=74001/221505, ave loss=3.131942144577849\n",
      "n_step=75001/221505, ave loss=3.0811842586338036\n",
      "n_step=76001/221505, ave loss=3.1167453938928937\n",
      "n_step=77001/221505, ave loss=3.150582729608736\n",
      "n_step=78001/221505, ave loss=3.152908963635682\n",
      "n_step=79001/221505, ave loss=3.149779088072272\n",
      "n_step=80001/221505, ave loss=3.113897048128648\n",
      "n_step=81001/221505, ave loss=3.131049338586731\n",
      "n_step=82001/221505, ave loss=3.122006235282027\n",
      "n_step=83001/221505, ave loss=3.0949149784503964\n",
      "n_step=84001/221505, ave loss=3.087942172989764\n",
      "n_step=85001/221505, ave loss=3.1005531903394603\n",
      "n_step=86001/221505, ave loss=3.094193143133003\n",
      "n_step=87001/221505, ave loss=3.126432152377573\n",
      "n_step=88001/221505, ave loss=3.1052305729369554\n",
      "starting epoch: 3 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7478153c9b41a7a200e6aaf4050853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step=89001/221505, ave loss=3.1204819089492517\n",
      "n_step=90001/221505, ave loss=3.117434469105209\n",
      "n_step=91001/221505, ave loss=3.0977950662836267\n",
      "n_step=92001/221505, ave loss=3.1513816173319884\n",
      "n_step=93001/221505, ave loss=3.132502871263346\n",
      "n_step=94001/221505, ave loss=3.134323289100029\n",
      "n_step=95001/221505, ave loss=3.1386393286899366\n",
      "n_step=96001/221505, ave loss=3.1082997247335835\n",
      "n_step=97001/221505, ave loss=3.116812746799676\n",
      "n_step=98001/221505, ave loss=3.1140673890151307\n",
      "n_step=99001/221505, ave loss=3.100661417938226\n",
      "n_step=100001/221505, ave loss=3.074824954513689\n",
      "n_step=101001/221505, ave loss=3.0602393134853436\n",
      "n_step=102001/221505, ave loss=3.036621234094847\n",
      "n_step=103001/221505, ave loss=3.0368167958528067\n",
      "n_step=104001/221505, ave loss=2.9974018560785036\n",
      "n_step=105001/221505, ave loss=2.99815608849805\n",
      "n_step=106001/221505, ave loss=3.025134303230099\n",
      "n_step=107001/221505, ave loss=3.003989920737431\n",
      "n_step=108001/221505, ave loss=2.9653547652047827\n",
      "n_step=109001/221505, ave loss=2.9638971205612092\n",
      "n_step=110001/221505, ave loss=2.957838208048607\n",
      "n_step=111001/221505, ave loss=2.949700445019253\n",
      "n_step=112001/221505, ave loss=2.9848587563426006\n",
      "n_step=113001/221505, ave loss=2.9853269294420537\n",
      "n_step=114001/221505, ave loss=2.929252458181592\n",
      "n_step=115001/221505, ave loss=2.964705744319797\n",
      "n_step=116001/221505, ave loss=2.9696542013301457\n",
      "n_step=117001/221505, ave loss=2.91276753674127\n",
      "n_step=118001/221505, ave loss=2.9420564155708284\n",
      "n_step=119001/221505, ave loss=2.8593931471859246\n",
      "n_step=120001/221505, ave loss=2.891675800583163\n",
      "n_step=121001/221505, ave loss=2.934964535827715\n",
      "n_step=122001/221505, ave loss=2.9416552640317133\n",
      "n_step=123001/221505, ave loss=2.9438446962514457\n",
      "n_step=124001/221505, ave loss=2.8793369052607236\n",
      "n_step=125001/221505, ave loss=2.919347954539631\n",
      "n_step=126001/221505, ave loss=2.89432400408454\n",
      "n_step=127001/221505, ave loss=2.848858762883501\n",
      "n_step=128001/221505, ave loss=2.8358675669985094\n",
      "n_step=129001/221505, ave loss=2.7932663908877386\n",
      "n_step=130001/221505, ave loss=2.859733790371588\n",
      "n_step=131001/221505, ave loss=2.831139490910512\n",
      "n_step=132001/221505, ave loss=2.8644867873948696\n",
      "starting epoch: 4 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c10601779b4874b0b0b5ebf6d6a1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step=133001/221505, ave loss=2.8455296501468403\n",
      "n_step=134001/221505, ave loss=2.821587146914487\n",
      "n_step=135001/221505, ave loss=2.809090089870805\n",
      "n_step=136001/221505, ave loss=2.8333809814430846\n",
      "n_step=137001/221505, ave loss=2.8304250755005467\n",
      "n_step=138001/221505, ave loss=2.830107837400631\n",
      "n_step=139001/221505, ave loss=2.858385791890447\n",
      "n_step=140001/221505, ave loss=2.835754696702159\n",
      "n_step=141001/221505, ave loss=2.7979053744614637\n",
      "n_step=142001/221505, ave loss=2.848945606916767\n",
      "n_step=143001/221505, ave loss=2.85514287184394\n",
      "n_step=144001/221505, ave loss=2.8583563208690923\n",
      "n_step=145001/221505, ave loss=2.8147744360921205\n",
      "n_step=146001/221505, ave loss=2.8431243164182396\n",
      "n_step=147001/221505, ave loss=2.8415830730619307\n",
      "n_step=148001/221505, ave loss=2.8003046056240595\n",
      "n_step=149001/221505, ave loss=2.7732139879521522\n",
      "n_step=150001/221505, ave loss=2.815691369739401\n",
      "n_step=151001/221505, ave loss=2.7881254733331136\n",
      "n_step=152001/221505, ave loss=2.785407438332763\n",
      "n_step=153001/221505, ave loss=2.737356114298936\n",
      "n_step=154001/221505, ave loss=2.759236625135263\n",
      "n_step=155001/221505, ave loss=2.723154928577218\n",
      "n_step=156001/221505, ave loss=2.757597083677296\n",
      "n_step=157001/221505, ave loss=2.7631880607502617\n",
      "n_step=158001/221505, ave loss=2.742150987238542\n",
      "n_step=159001/221505, ave loss=2.7466270937869823\n",
      "n_step=160001/221505, ave loss=2.7518526631233424\n",
      "n_step=161001/221505, ave loss=2.713713196913455\n",
      "n_step=162001/221505, ave loss=2.7490936400585553\n",
      "n_step=163001/221505, ave loss=2.686262970169335\n",
      "n_step=164001/221505, ave loss=2.688124940101404\n",
      "n_step=165001/221505, ave loss=2.7044373109786304\n",
      "n_step=166001/221505, ave loss=2.7369630663171347\n",
      "n_step=167001/221505, ave loss=2.7379665132880087\n",
      "n_step=168001/221505, ave loss=2.70522417617877\n",
      "n_step=169001/221505, ave loss=2.6818899085717476\n",
      "n_step=170001/221505, ave loss=2.7131075278550942\n",
      "n_step=171001/221505, ave loss=2.666618917976956\n",
      "n_step=172001/221505, ave loss=2.6489365113512813\n",
      "n_step=173001/221505, ave loss=2.634340616659477\n",
      "n_step=174001/221505, ave loss=2.6469517344513216\n",
      "n_step=175001/221505, ave loss=2.643764456518549\n",
      "n_step=176001/221505, ave loss=2.6865475688011675\n",
      "n_step=177001/221505, ave loss=2.662945154082706\n",
      "starting epoch: 5 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1244ce054596451f80abcc824bc39eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step=178001/221505, ave loss=2.658030061588818\n",
      "n_step=179001/221505, ave loss=2.6565650226338238\n",
      "n_step=180001/221505, ave loss=2.6667380509711767\n",
      "n_step=181001/221505, ave loss=2.6720977315641474\n",
      "n_step=182001/221505, ave loss=2.682319949845126\n",
      "n_step=183001/221505, ave loss=2.6981037499674363\n",
      "n_step=184001/221505, ave loss=2.7107980534714518\n",
      "n_step=185001/221505, ave loss=2.642804827559416\n",
      "n_step=186001/221505, ave loss=2.69316147061966\n",
      "n_step=187001/221505, ave loss=2.6935088216749996\n",
      "n_step=188001/221505, ave loss=2.7306860148230174\n",
      "n_step=189001/221505, ave loss=2.6728529204559655\n",
      "n_step=190001/221505, ave loss=2.685043001326034\n",
      "n_step=191001/221505, ave loss=2.644592998485159\n",
      "n_step=192001/221505, ave loss=2.686162035008383\n",
      "n_step=193001/221505, ave loss=2.646573811774729\n",
      "n_step=194001/221505, ave loss=2.668266128013147\n",
      "n_step=195001/221505, ave loss=2.6380477015038823\n",
      "n_step=196001/221505, ave loss=2.666942914837526\n",
      "n_step=197001/221505, ave loss=2.6130014393243144\n",
      "n_step=198001/221505, ave loss=2.62533191742202\n",
      "n_step=199001/221505, ave loss=2.5831563940146927\n",
      "n_step=200001/221505, ave loss=2.6238170886449375\n",
      "n_step=201001/221505, ave loss=2.649930196745934\n",
      "n_step=202001/221505, ave loss=2.6398508651153434\n",
      "n_step=203001/221505, ave loss=2.615763734261128\n",
      "n_step=204001/221505, ave loss=2.626052535482381\n",
      "n_step=205001/221505, ave loss=2.6292169097919063\n",
      "n_step=206001/221505, ave loss=2.6170192505685232\n",
      "n_step=207001/221505, ave loss=2.6079460380116934\n",
      "n_step=208001/221505, ave loss=2.553843746724355\n",
      "n_step=209001/221505, ave loss=2.590918538655059\n",
      "n_step=210001/221505, ave loss=2.622563026344128\n",
      "n_step=211001/221505, ave loss=2.6075029532674736\n",
      "n_step=212001/221505, ave loss=2.5884034771797926\n",
      "n_step=213001/221505, ave loss=2.5897899839723513\n",
      "n_step=214001/221505, ave loss=2.5836515160263036\n",
      "n_step=215001/221505, ave loss=2.5743708765787003\n",
      "n_step=216001/221505, ave loss=2.534886494723305\n",
      "n_step=217001/221505, ave loss=2.5595844032297084\n",
      "n_step=218001/221505, ave loss=2.5148056736293456\n",
      "n_step=219001/221505, ave loss=2.547376630421313\n",
      "n_step=220001/221505, ave loss=2.5648755047464586\n",
      "n_step=221001/221505, ave loss=2.5413392178388885\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "init_params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\": 0.01}\n",
    "kernel_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_h_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "bias_o_initializer = NormalInitializer(seed=None, **init_params)\n",
    "kernel_regularizer = None\n",
    "\n",
    "num_inputs = batch_size\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "rnn = RNN(in_dim=hpdata.get_encoder().size, out_dim=hpdata.get_encoder().size, hidden_dim=5, \n",
    "          kernel_h_initializer=kernel_h_initializer, \n",
    "          bias_h_initializer=bias_h_initializer, \n",
    "          kernel_o_initializer=kernel_o_initializer, \n",
    "          bias_o_initializer=bias_o_initializer, \n",
    "          kernel_regularizer=kernel_regularizer, \n",
    "          activation_h=TanhActivation(),\n",
    "          activation_o=SoftmaxActivation())\n",
    "\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "lr_initial=0.01\n",
    "optimizer = SGDOptimizer(lr_schedule=LRConstantSchedule(lr_initial))\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "batch_size = 25\n",
    "n_batches = int(hpdata.book_data.shape[0] / batch_size)\n",
    "\n",
    "n_steps = n_epochs * n_batches\n",
    "n_step = 1\n",
    "\n",
    "losses_register = []\n",
    "\n",
    "for n_epoch in range(n_epochs):\n",
    "    print(f\"starting epoch: {n_epoch + 1} ...\")\n",
    "    batches = tqdm(range(n_batches))\n",
    "    for b in batches:\n",
    "        batches.set_description(f\"batch {b + 1}/{n_batches}\")\n",
    "        x_batch = x_train[b * batch_size:(b + 1) * batch_size]\n",
    "        y_batch = y_train[b * batch_size + 1:(b + 1) * batch_size + 1]\n",
    "        y_batch = y_encoding[b * batch_size + 1:(b + 1) * batch_size + 1]\n",
    "\n",
    "        if y_batch.shape[0] < batch_size:\n",
    "            continue\n",
    "        \n",
    "        scores = rnn.forward(x_batch)\n",
    "        data_loss = loss.compute_loss(scores, y_batch)\n",
    "        losses_register.append(data_loss)\n",
    "        \n",
    "        params_train = {\"mode\": \"train\", \"seed\": None}\n",
    "        rnn.backward(loss.grad(), **params_train)\n",
    "\n",
    "        trainable_params=rnn.get_learnable_params()\n",
    "        grads=rnn.get_learnable_params_grads()\n",
    "\n",
    "        for k,v in trainable_params.items():\n",
    "            trainable_params[k] = deepcopy(v - lr_initial * np.maximum(np.minimum(grads[\"d\"+k], 5), -5))\n",
    "\n",
    "        rnn.set_learnable_params(**trainable_params)\n",
    "        if n_step % 1000 == 0:\n",
    "            print(f\"n_step={n_step+1}/{n_steps}, ave loss={np.array(losses_register).sum()/1000}\")\n",
    "            losses_register = []\n",
    "        n_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hauneend  fioneys Hol,\".\n",
      "\"I  fol, --M..hid thiponed 'aul Homlmr wlabs. baancede any\n",
      "-pemedaumen bo, weus gv gupl ore sd choh the waheeet.  wev anull,\" heme sondins Soyd.  Dooc tf  wory ahe .\"\".Wo inile thep told tholg acd \"2hot indtw Mes,at teid aurr the? t welly.  seouk oH lat ird sere tat shadarcehevd wond het'fud ag his maedg, foft.  DonysRarled wiMg. hod hiret to, we he ton, bandilire- Pel.  \"9ou Mokd teg miidtianr wat bh to fhrinteas ad me thh!.hee salraus  hois tha p\"jocd the.  un thaiindseye jih bomleps mrasrtame cam pos ors beSmrd s4gatnp de, cein, d.\"heeloink lainid rgiwicm ..  P\toord fpf, Sircugt  mind alear feme beisorels ghom NofC woll hesem uTpals leordpe saile.. Hindpercepe.\n",
      "\n",
      "Q)cto ofe sglr.\n",
      "\"uhe, wel. Mhipine bcunt,\" \"\"Cat pssn pinVd I wuud, \"Ooous Iins.  wad .\n",
      "'Bopy kepwindey xoh  \"\"Nheimd elalnrisdras ihdsre,  se Ie.  con, Tfar bot sapceint pictpcy the hot .he any sole chewarte? homdonds Rasrreld seemy theud patry cadhe Oatcpe talizerappiwlet lofeh, tuf leireoll chih MDtoeinsamt werok tor  sqans. Ctovreel torisoyt hind oin zouindced fobd tHd. we Ihu toneeun,G, \"Nfges thap thesonete aelet os \"Znoukd so aril }umlatt  and Busheet sas To g wofy cexesyin tt\"  z ke Trrriurf  thryd wagtt bedeebe. k bniu Hegr pasihe yat rat jure inondi, sind rapeancserm, Meorl , tyas, oh 'ud wtout.  heeg beune.. .. fellr the wFaen an perly\n",
      "\"Y veeid Sras gore\" ho co. .. hiuredanroswnytasetkld perisy sto sut, ted watimoolearee, mendEeoed whe t.\n",
      "bod hZ akit wamrilg Do, pb eor pleit xoc, ofoneeSt ath he aafy.,\"\n",
      "TWaed veeeoan aat soos to mah tilucny thaojdesp fe'th wolo tred rsbl a Hoifanonre' feos!igpm thelg bacnz sadkd tfy Hamisan sofd beraan,he inequt mol to wadweed theMyr, oo wasin Hashel,eed waledangvyigk,.arr, oodlg Io ac tullett ancm img biemd garee fhodist be meode sme rind sonr, moakeirid as ve, on nirey d wees ser,.  ohey\" bogn -ired sarolee pahorlo arepous\"\"Ceeben bimg wathas nrafw chad ghey dint they.\n",
      "\"ahiwuicinrd mVeg bin, gcf\" \":o , hailcinsoicn,an an anreym qop ma\n"
     ]
    }
   ],
   "source": [
    "synhthetizer = Synhthetizer(rnn, onehot_encoder)\n",
    "sequence= synhthetizer(ts=2000, init_idx=1)\n",
    "print(\"\".join(hpdata.decode(sequence.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  5],\n",
       "       [-5,  2,  0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = np.array([[1,4,40],[-10,2,0]])\n",
    "np.maximum(np.minimum(lol, 5), -5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
