{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "downtown-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense, BatchNormalization\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule\n",
    "from grad_check import grad_check_without_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "completed-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\"\n",
    "    a naive implementation of numerical gradient of f at x\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \n",
    "    From: https://cs231n.github.io/assignments2021/assignment2/\n",
    "    \"\"\"\n",
    "\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \n",
    "    From: https://cs231n.github.io/assignments2021/assignment2/\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "coated-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025458445007376\n",
      "2\n",
      "[ 0.00200116  0.00200312 -0.01800325  0.00199891  0.00200017  0.00199877\n",
      "  0.00200002  0.00200405  0.00199747  0.00199957]\n",
      "[ 0.00200116  0.00200312 -0.01800325  0.00199891  0.00200017  0.00199877\n",
      "  0.00200002  0.00100203  0.00199747  0.00199957]\n",
      "test_softmax_activation passed\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_activation():\n",
    "    \"\"\" Test cases from: https://cs231n.github.io/assignments2021/assignment2/ \"\"\"\n",
    "    def func(x):\n",
    "        softmax_activation = SoftmaxActivation()\n",
    "        a = softmax_activation.forward(x)\n",
    "        categoical_cross_entropy_loss = CategoricalCrossEntropyLoss()\n",
    "        loss = categoical_cross_entropy_loss.compute_loss(a, y)\n",
    "        return loss\n",
    "    \n",
    "    # from: https://cs231n.github.io/linear-classify/\n",
    "    x = np.array([[-2.85, 0.86, 0.28]])\n",
    "    softmax_activation = SoftmaxActivation()\n",
    "    a = softmax_activation.forward(x)\n",
    "    a_true = np.array([[0.01544932, 0.63116335, 0.35338733]])\n",
    "    np.testing.assert_almost_equal(a, a_true, decimal=7)\n",
    "    \n",
    "    np.random.seed(231)\n",
    "    num_classes, num_inputs = 10, 50\n",
    "    x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "    y = np.random.randint(num_classes, size=num_inputs)\n",
    "    \n",
    "    softmax_activation = SoftmaxActivation()\n",
    "    a = softmax_activation.forward(x)\n",
    "    categoical_cross_entropy_loss = CategoricalCrossEntropyLoss()\n",
    "    loss = categoical_cross_entropy_loss.compute_loss(a, y)\n",
    "    loss_grad = categoical_cross_entropy_loss.grad()\n",
    "    g_out = softmax_activation.backward(loss_grad)\n",
    "    g_out_num = eval_numerical_gradient(func, x, h=5e-6, verbose=False)\n",
    "\n",
    "    np.testing.assert_array_almost_equal(g_out, g_out_num, decimal=2)\n",
    "\n",
    "    print(\"test_softmax_activation passed\")\n",
    "    \n",
    "test_softmax_activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_grad_check(x, y, seed=np.random.randint(low=1, high=300)):\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    in_dim = x.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=None, \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=None, \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    verbose = True\n",
    "    grad_check_without_reg(model, loss, x, y, verbose, seed=seed + 1)\n",
    "\n",
    "    print(\"test_grad_check passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad_check(x_train[:20, :10], y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train and val set are batch 1, 2, 3, 4, and 5, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "# np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "# np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    plt.plot(history[\"loss_train\"], label=\"train\")\n",
    "    plt.plot(history[\"loss_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Loss vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    path = \"losses.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_costs(history):\n",
    "    plt.plot(history[\"cost_train\"], label=\"train\")\n",
    "    plt.plot(history[\"cost_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Cost vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    path = \"costs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history[\"accuracy_train\"], label=\"train\")\n",
    "    plt.plot(history[\"accuracy_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Accuracy vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    path = \"accuracies.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lr(history):\n",
    "    plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning rate vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.legend()\n",
    "    path = \"lrs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \"\"\" Inv dropout - scaling at train time\"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.cache = {}\n",
    "        self.has_learnable_params = False\n",
    "        \n",
    "    def if_has_learnable_params(self, ):\n",
    "        return self.has_learnable_params\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        mode = params[\"mode\"]\n",
    "        seed = params[\"seed\"]\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            np.random.seed(seed)\n",
    "            mask = (np.random.rand(*x.shape) < self.p) / self.p\n",
    "            self.cache[\"mask\"] = deepcopy(mask)\n",
    "            # drop it boi!\n",
    "            out = x * mask\n",
    "        else:\n",
    "            out = x\n",
    "        \n",
    "        return deepcopy(out)\n",
    "    \n",
    "    def backward(self, g_in, **params):\n",
    "        mode = params[\"mode\"]\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            mask = deepcopy(self.cache[\"mask\"])\n",
    "            g_out = g_in * mask\n",
    "        else:\n",
    "            g_out = deepcopy(g_in)        \n",
    "        \n",
    "        return g_out\n",
    "        \n",
    "    def __repr__(self, ):\n",
    "        repr_str = f\"dropout with p={self.p}\"\n",
    "        return repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dropout(seed=np.random.randint(low=1, high=300)):\n",
    "    np.random.seed(seed)\n",
    "    p = np.random.uniform(low=0, high=1)\n",
    "    dropout = Dropout(p=p)\n",
    "    size = (5, 8)\n",
    "    mean = 0.0\n",
    "    std = 1.0\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.normal(loc=mean, scale=std, size=size)\n",
    "    np.random.seed(seed+1)\n",
    "    g_in = np.random.normal(loc=mean, scale=std, size=size)\n",
    "    modes = [\"train\", \"test\"]\n",
    "    #modes = [\"train\"]\n",
    "    for mode in modes:\n",
    "        params = {\"mode\": mode, \"seed\": seed}\n",
    "        fw = dropout.forward(x, **params)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            np.random.seed(seed)\n",
    "            mask = (np.random.rand(*x.shape) < p) / p\n",
    "            fw_true = x * mask\n",
    "        else:\n",
    "            fw_true = deepcopy(x)\n",
    "        \n",
    "        np.testing.assert_array_equal(fw, fw_true)\n",
    "        \n",
    "        bw = dropout.backward(g_in, **params)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            bw_true = g_in * mask\n",
    "        else:\n",
    "            bw_true = deepcopy(g_in)\n",
    "        \n",
    "        np.testing.assert_array_equal(bw, bw_true)\n",
    "        \n",
    "test_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_params = [{\"a\":1, \"b\":2}, {\"c\":1}]\n",
    "params = deepcopy(in_params)\n",
    "updated_params = deepcopy(in_params)\n",
    "grads = [{\"da\":2, \"db\":4}, {\"dc\":10}]\n",
    "lr = 1\n",
    "\n",
    "for idx in range(len(params)):\n",
    "    param_set, grad_set = params[idx], grads[idx]\n",
    "    for p, g in zip(param_set, grad_set):\n",
    "        print(p)\n",
    "        print(g)\n",
    "        updated_params[idx][p] = param_set[p] - lr * grad_set[g]\n",
    "\n",
    "print(updated_params)\n",
    "print(params)\n",
    "print(in_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_normalization(seed=np.random.randint(low=1, high=300)):\n",
    "    \n",
    "    epsilon = 10e-6\n",
    "    momentum=0.99\n",
    "    bn = BatchNormalization(momentum=momentum, epsilon=epsilon)\n",
    "    \n",
    "    n = 100\n",
    "    dim = 4\n",
    "    size=(n, dim)\n",
    "    x = np.random.normal(loc=0, scale=1.0, size=size)\n",
    "    \n",
    "    batch_size = 10\n",
    "    assert n % batch_size == 0\n",
    "    n_batches = int(n / batch_size)\n",
    "    \n",
    "    beta_true = np.zeros((dim,))\n",
    "    gamma_true = np.ones((dim,))\n",
    "    moving_mean_true = np.zeros((dim,))\n",
    "    moving_variance_true = np.zeros((dim,))\n",
    "    \n",
    "    modes = [\"train\", \"test\"]\n",
    "    \n",
    "    for mode in modes:\n",
    "        params = {\"mode\": mode}\n",
    "        for n_batch in range(n_batches):\n",
    "            x_batch = x[n_batch*batch_size:(n_batch+1)*batch_size]\n",
    "            a_batch = bn.forward(x_batch, **params)\n",
    "\n",
    "            if mode == \"train\":\n",
    "                mean_batch_true = np.mean(x_batch, axis=0)\n",
    "                var_batch_true = np.var(x_batch, axis=0)\n",
    "                std_batch_true = np.sqrt(var_batch_true)\n",
    "                z_batch_true = (x_batch - mean_batch_true) / (std_batch_true + epsilon)\n",
    "                a_batch_true = gamma_true * z_batch_true + beta_true\n",
    "                moving_mean_true = moving_mean_true + (1-momentum) * mean_batch_true\n",
    "                moving_variance_true = moving_variance_true + (1-momentum) * var_batch_true\n",
    "            else:\n",
    "                a_batch_true = gamma_true * (x_batch - moving_mean_true) / \\\n",
    "                    (np.sqrt(moving_variance_true) + epsilon) + beta_true\n",
    "\n",
    "            np.testing.assert_array_equal(a_batch, a_batch_true)\n",
    "    \n",
    "    print(\"test_batch_normalization passed\")\n",
    "    \n",
    "test_batch_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.01\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 500\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-following",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.01\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    bn_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 500\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    bn_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-viking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "print(x_train.shape[0])\n",
    "n_s = int(5 * x_train.shape[0] / batch_size)\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 2\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.005\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim_1 = 50\n",
    "mid_dim_2 = 50\n",
    "\n",
    "seed = 210\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim_1, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim_1, out_dim=mid_dim_2,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_3 = \\\n",
    "    Dense(in_dim=mid_dim_2, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+4, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+5, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2,\n",
    "    dense_3\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 2250\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.005\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim_1 = 50\n",
    "mid_dim_2 = 50\n",
    "\n",
    "seed = 210\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim_1, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim_1, out_dim=mid_dim_2,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_2 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_3 = \\\n",
    "    Dense(in_dim=mid_dim_2, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+4, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+5, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    bn_1,\n",
    "    dense_2,\n",
    "    bn_2,\n",
    "    dense_3\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 2250\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-excess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dropout,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "\n",
    "params = {\"mode\": \"test\", \"seed\": None}\n",
    "y_hat_test, scores_test, cost_test, data_loss_test, layers_reg_loss_test = \\\n",
    "    model.predict(x_test, y_test, **params)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-strap",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner():\n",
    "    def __init__(self, build_model, objective, iterations=1, **params):\n",
    "        # objective is of Metrics for now\n",
    "        self.build_model = build_model\n",
    "        self.objective = objective\n",
    "        self.iterations = iterations\n",
    "        self.params = params\n",
    "        self.params_product = list(product(*params.values()))\n",
    "        self.params_names = list(params.keys())\n",
    "    \n",
    "    def search(self, x_train, y_train, x_val, y_val, n_epochs, batch_size):\n",
    "        # list of tuples = list(product([1,2,3],[3,4]))\n",
    "        # for tuple in list:\n",
    "        # rows in final df\n",
    "        rows = []\n",
    "        \n",
    "        #params_product = tqdm(self.params_product, file=sys.stdout)\n",
    "        \n",
    "        n_prod = len(self.params_product)\n",
    "        \n",
    "        for idx_prod, prod in enumerate(self.params_product):\n",
    "            \n",
    "            params = {}\n",
    "            for idx, param_name in enumerate(self.params_names):\n",
    "                params[param_name] = prod[idx]\n",
    "            #print(params)\n",
    "            #print(n_prod)\n",
    "            \n",
    "            # if more than 1 iterations\n",
    "            objective_list = []\n",
    "            \n",
    "            for it in range(self.iterations):\n",
    "                print(\"*\"*5)\n",
    "                print(f\"tuner: {idx_prod+1}/{n_prod} config (iter: {it+1}/{self.iterations})\")\n",
    "                # build_model with tuple params\n",
    "                model = build_model(seed=200, **params)\n",
    "                # fit model\n",
    "                history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "                # meaasure objective on model\n",
    "                scores_val = model.forward(x_val)\n",
    "                y_hat_val = np.argmax(scores_val, axis=1)\n",
    "                objective_val = self.objective.get_metrics(y_val, y_hat_val)\n",
    "                # save objective in list\n",
    "                objective_list.append(objective_val)\n",
    "                \n",
    "            # average objective in list\n",
    "            objective_mean = np.array(objective_list).mean()\n",
    "            # save tuple of params and objective as dict\n",
    "            objective_dict = {self.objective.name: objective_mean}\n",
    "            row_dict = {**params, **objective_dict}\n",
    "            rows.append(row_dict)\n",
    "            print(\"*\"*5 + \"\\n\")\n",
    "            \n",
    "        # df from list of dicts of params and objective val\n",
    "        df = pd.DataFrame(data=rows)\n",
    "        \n",
    "        # save to csv\n",
    "        date_string = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        path = os.path.join(\"tuner_results\", date_string + \".csv\")\n",
    "        \n",
    "        df.to_csv(path, encoding='utf-8', index=False)\n",
    "        \n",
    "        # argmax across rows and return best params as dict (~**params)\n",
    "        best_params = dict(df.loc[df[self.objective.name].idxmax()])\n",
    "        best_objective = best_params.pop(self.objective.name)\n",
    "        \n",
    "        return best_objective, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(seed=200, **params):\n",
    "    \n",
    "    assert \"reg_rate_l2\" in params.keys()\n",
    "    reg_rate_l2 = params[\"reg_rate_l2\"]\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    #reg_rate_l2 = 0.025\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    #seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "    print(model)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    # assignment:\n",
    "    #n_epochs = 4\n",
    "    #batch_size = 100\n",
    "\n",
    "    lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    step_size = 900\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    #history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train and val set are batch 1, 2, 3, 4, and 5, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 2\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 900\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_custom(n):\n",
    "    l_min = -5\n",
    "    l_max = -1\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    return [10 **(l_min + (l_max - l_min) * np.random.uniform(low=0, high=1)) for i in range(n)]\n",
    "\n",
    "def coarse_to_fine_custom(best_via_coarse, n):\n",
    "    half_interval = 0.2\n",
    "    low = best_via_coarse * (1-half_interval) \n",
    "    high = best_via_coarse * (1+half_interval)\n",
    "    \n",
    "    return [np.random.uniform(low=low, high=high) for i in range(n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = AccuracyMetrics()\n",
    "build_model = build_model_func\n",
    "\n",
    "# coarse\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_custom(n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")\n",
    "\n",
    "# coarse to fine\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_to_fine_custom(best_params[\"reg_rate_l2\"], n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-oracle",
   "metadata": {},
   "source": [
    "## best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.02, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 3\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "# best obj:0.5134, with {'reg_rate_l2': 0.00036537637001811185}\n",
    "reg_rate_l2 = best_params[\"reg_rate_l2\"]\n",
    "#print(reg_rate_l2)\n",
    "#raise\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 12\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 980\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-fifty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
