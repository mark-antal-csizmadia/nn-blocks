{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "downtown-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense, BatchNormalization\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule\n",
    "from grad_check import grad_check_without_reg, eval_numerical_gradient, eval_numerical_gradient_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-discharge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noble-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_grad_check(x, y, seed=np.random.randint(low=1, high=300)):\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    in_dim = x.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=None, \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "    bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=None, \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    verbose = True\n",
    "    grad_check_without_reg(model, loss, x, y, verbose, seed=seed + 1)\n",
    "\n",
    "    print(\"test_grad_check passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "willing-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting grad check with 2 data points \n",
      "\n",
      "--layer: 1/2, w.shape=(10, 50) (500 params)\n",
      "100%|██████████| 500/500 [00:00<00:00, 3252.43it/s]\n",
      "analytic and numerical grads are equal up to 6 decimals\n",
      "max rel error=1.879660e-09\n",
      "passed\n",
      "\n",
      "--layer: 1/2, b.shape=(1, 50) (50 params)\n",
      "100%|██████████| 50/50 [00:00<00:00, 2766.62it/s]\n",
      "analytic and numerical grads are equal up to 6 decimals\n",
      "max rel error=2.102941e-06\n",
      "passed\n",
      "\n",
      "--layer: 2/2, w.shape=(50, 10) (500 params)\n",
      "100%|██████████| 500/500 [00:00<00:00, 2921.75it/s]\n",
      "analytic and numerical grads are equal up to 6 decimals\n",
      "max rel error=4.201585e-09\n",
      "passed\n",
      "\n",
      "--layer: 2/2, b.shape=(1, 10) (10 params)\n",
      "100%|██████████| 10/10 [00:00<00:00, 1333.60it/s]\n",
      "analytic and numerical grads are equal up to 6 decimals\n",
      "max rel error=1.846604e-09\n",
      "passed\n",
      "\n",
      "completed grad check\n",
      "\n",
      "test_grad_check passed\n"
     ]
    }
   ],
   "source": [
    "test_grad_check(x_train[:2, :10], y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "third-neighbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (45000, 32, 32, 3), val set shape: (5000, 32, 32, 3), test set shape: (10000, 32, 32, 3)\n",
      "train labels shape: (45000,), val labels shape: (5000,), test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# train and val set are batch 1, 2, 3, 4, and 5, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "# np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "# np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "later-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    plt.plot(history[\"loss_train\"], label=\"train\")\n",
    "    plt.plot(history[\"loss_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Loss vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    path = \"losses.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_costs(history):\n",
    "    plt.plot(history[\"cost_train\"], label=\"train\")\n",
    "    plt.plot(history[\"cost_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Cost vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    path = \"costs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history[\"accuracy_train\"], label=\"train\")\n",
    "    plt.plot(history[\"accuracy_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Accuracy vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    path = \"accuracies.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lr(history):\n",
    "    plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning rate vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.legend()\n",
    "    path = \"lrs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "boring-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \"\"\" Inv dropout - scaling at train time\"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.cache = {}\n",
    "        self.has_learnable_params = False\n",
    "        \n",
    "    def if_has_learnable_params(self, ):\n",
    "        return self.has_learnable_params\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        mode = params[\"mode\"]\n",
    "        seed = params[\"seed\"]\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            np.random.seed(seed)\n",
    "            mask = (np.random.rand(*x.shape) < self.p) / self.p\n",
    "            self.cache[\"mask\"] = deepcopy(mask)\n",
    "            # drop it boi!\n",
    "            out = x * mask\n",
    "        else:\n",
    "            out = x\n",
    "        \n",
    "        return deepcopy(out)\n",
    "    \n",
    "    def backward(self, g_in, **params):\n",
    "        mode = params[\"mode\"]\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            mask = deepcopy(self.cache[\"mask\"])\n",
    "            g_out = g_in * mask\n",
    "        else:\n",
    "            g_out = deepcopy(g_in)        \n",
    "        \n",
    "        return g_out\n",
    "        \n",
    "    def __repr__(self, ):\n",
    "        repr_str = f\"dropout with p={self.p}\"\n",
    "        return repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "curious-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dropout(seed=np.random.randint(low=1, high=300)):\n",
    "    np.random.seed(seed)\n",
    "    p = np.random.uniform(low=0, high=1)\n",
    "    dropout = Dropout(p=p)\n",
    "    size = (5, 8)\n",
    "    mean = 0.0\n",
    "    std = 1.0\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.normal(loc=mean, scale=std, size=size)\n",
    "    np.random.seed(seed+1)\n",
    "    g_in = np.random.normal(loc=mean, scale=std, size=size)\n",
    "    modes = [\"train\", \"test\"]\n",
    "    #modes = [\"train\"]\n",
    "    for mode in modes:\n",
    "        params = {\"mode\": mode, \"seed\": seed}\n",
    "        fw = dropout.forward(x, **params)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            np.random.seed(seed)\n",
    "            mask = (np.random.rand(*x.shape) < p) / p\n",
    "            fw_true = x * mask\n",
    "        else:\n",
    "            fw_true = deepcopy(x)\n",
    "        \n",
    "        np.testing.assert_array_equal(fw, fw_true)\n",
    "        \n",
    "        bw = dropout.backward(g_in, **params)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            bw_true = g_in * mask\n",
    "        else:\n",
    "            bw_true = deepcopy(g_in)\n",
    "        \n",
    "        np.testing.assert_array_equal(bw, bw_true)\n",
    "        \n",
    "test_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "brazilian-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.29048278 1.39248907 2.93350569 0.98234546 2.08326113]\n",
      "[2.29048278 1.39248907 2.93350569 0.98234546 2.08326113]\n"
     ]
    }
   ],
   "source": [
    "def test_batch_normalization_layer():\n",
    "    # Gradient check batchnorm backward pass\n",
    "    np.random.seed(231)\n",
    "    N, D = 4, 5\n",
    "    x = 5 * np.random.randn(N, D) + 12\n",
    "    gamma = np.random.randn(D)\n",
    "    beta = np.random.randn(D)\n",
    "    dout = np.random.randn(N, D)\n",
    "\n",
    "    bn_param = {'mode': 'train'}\n",
    "\n",
    "    def with_gamma(x, gamma, **bn_param):\n",
    "        bn = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        bn.set_gamma(gamma)\n",
    "        a = bn.forward(x, **bn_param)\n",
    "        return a\n",
    "\n",
    "    def with_beta(x, beta, **bn_param):\n",
    "        bn = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        bn.set_beta(beta)\n",
    "        a = bn.forward(x, **bn_param)\n",
    "        return a\n",
    "\n",
    "    bn = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "    fx = lambda x: BatchNormalization.forward(bn, x, **bn_param)\n",
    "    fg = lambda g: with_gamma(x, g, **bn_param)\n",
    "    fb = lambda b: with_beta(x, b, **bn_param)\n",
    "\n",
    "    dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "    dgamma_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "    dbeta_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "    \n",
    "    bn = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "    bn.forward(x, **bn_param)\n",
    "    dx = bn.backward(dout, **bn_param)\n",
    "    dgamma = bn.grads[\"dgamma\"]\n",
    "    dbeta = bn.grads[\"dbeta\"]\n",
    "\n",
    "    np.testing.assert_array_almost_equal(dx, dx_num, decimal=10)\n",
    "    np.testing.assert_array_almost_equal(dgamma, dgamma_num, decimal=10)\n",
    "    np.testing.assert_array_almost_equal(dbeta, dbeta_num, decimal=10)\n",
    "    \n",
    "test_batch_normalization_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-jacket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: BatchNormalization.forward(bn, x, gamma, beta, **bn_param)\n",
    "fg = lambda a: BatchNormalization.forward(bn, x, a, beta, **bn_param)\n",
    "fb = lambda b: BatchNormalization.forward(bn, x, gamma, b, **bn_param)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "dx = bn.backward(dout)\n",
    "dgamma = bn.cache[\"dgamma\"]\n",
    "dbeta = bn.cache[\"dbeta\"]\n",
    "\n",
    "#You should expect to see relative errors between 1e-13 and 1e-8\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_normalization(seed=np.random.randint(low=1, high=300)):\n",
    "    \n",
    "    epsilon = 10e-6\n",
    "    momentum=0.99\n",
    "    bn = BatchNormalization(momentum=momentum, epsilon=epsilon)\n",
    "    \n",
    "    n = 100\n",
    "    dim = 4\n",
    "    size=(n, dim)\n",
    "    x = np.random.normal(loc=0, scale=1.0, size=size)\n",
    "    \n",
    "    batch_size = 10\n",
    "    assert n % batch_size == 0\n",
    "    n_batches = int(n / batch_size)\n",
    "    \n",
    "    beta_true = np.zeros((dim,))\n",
    "    gamma_true = np.ones((dim,))\n",
    "    moving_mean_true = np.zeros((dim,))\n",
    "    moving_variance_true = np.zeros((dim,))\n",
    "    \n",
    "    modes = [\"train\", \"test\"]\n",
    "    \n",
    "    for mode in modes:\n",
    "        params = {\"mode\": mode}\n",
    "        for n_batch in range(n_batches):\n",
    "            x_batch = x[n_batch*batch_size:(n_batch+1)*batch_size]\n",
    "            a_batch = bn.forward(x_batch, **params)\n",
    "\n",
    "            if mode == \"train\":\n",
    "                mean_batch_true = np.mean(x_batch, axis=0)\n",
    "                var_batch_true = np.var(x_batch, axis=0)\n",
    "                std_batch_true = np.sqrt(var_batch_true)\n",
    "                z_batch_true = (x_batch - mean_batch_true) / (std_batch_true + epsilon)\n",
    "                a_batch_true = gamma_true * z_batch_true + beta_true\n",
    "                moving_mean_true = moving_mean_true + (1-momentum) * mean_batch_true\n",
    "                moving_variance_true = moving_variance_true + (1-momentum) * var_batch_true\n",
    "            else:\n",
    "                a_batch_true = gamma_true * (x_batch - moving_mean_true) / \\\n",
    "                    (np.sqrt(moving_variance_true) + epsilon) + beta_true\n",
    "\n",
    "            np.testing.assert_array_equal(a_batch, a_batch_true)\n",
    "    \n",
    "    print(\"test_batch_normalization passed\")\n",
    "    \n",
    "test_batch_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.01\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 500\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-following",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.01\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    bn_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 500\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    bn_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-viking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "print(x_train.shape[0])\n",
    "n_s = int(5 * x_train.shape[0] / batch_size)\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 2\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.005\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim_1 = 50\n",
    "mid_dim_2 = 50\n",
    "\n",
    "seed = 210\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim_1, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim_1, out_dim=mid_dim_2,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_3 = \\\n",
    "    Dense(in_dim=mid_dim_2, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+4, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+5, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2,\n",
    "    dense_3\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 2250\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.005\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim_1 = 50\n",
    "mid_dim_2 = 50\n",
    "\n",
    "seed = 210\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim_1, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_1 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim_1, out_dim=mid_dim_2,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "bn_2 = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "\n",
    "dense_3 = \\\n",
    "    Dense(in_dim=mid_dim_2, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+4, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+5, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    bn_1,\n",
    "    dense_2,\n",
    "    bn_2,\n",
    "    dense_3\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 2250\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "params = {\"mode\": \"test\"}\n",
    "scores_test = model.forward(x_test, **params)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-excess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dropout,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "\n",
    "params = {\"mode\": \"test\", \"seed\": None}\n",
    "y_hat_test, scores_test, cost_test, data_loss_test, layers_reg_loss_test = \\\n",
    "    model.predict(x_test, y_test, **params)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-strap",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner():\n",
    "    def __init__(self, build_model, objective, iterations=1, **params):\n",
    "        # objective is of Metrics for now\n",
    "        self.build_model = build_model\n",
    "        self.objective = objective\n",
    "        self.iterations = iterations\n",
    "        self.params = params\n",
    "        self.params_product = list(product(*params.values()))\n",
    "        self.params_names = list(params.keys())\n",
    "    \n",
    "    def search(self, x_train, y_train, x_val, y_val, n_epochs, batch_size):\n",
    "        # list of tuples = list(product([1,2,3],[3,4]))\n",
    "        # for tuple in list:\n",
    "        # rows in final df\n",
    "        rows = []\n",
    "        \n",
    "        #params_product = tqdm(self.params_product, file=sys.stdout)\n",
    "        \n",
    "        n_prod = len(self.params_product)\n",
    "        \n",
    "        for idx_prod, prod in enumerate(self.params_product):\n",
    "            \n",
    "            params = {}\n",
    "            for idx, param_name in enumerate(self.params_names):\n",
    "                params[param_name] = prod[idx]\n",
    "            #print(params)\n",
    "            #print(n_prod)\n",
    "            \n",
    "            # if more than 1 iterations\n",
    "            objective_list = []\n",
    "            \n",
    "            for it in range(self.iterations):\n",
    "                print(\"*\"*5)\n",
    "                print(f\"tuner: {idx_prod+1}/{n_prod} config (iter: {it+1}/{self.iterations})\")\n",
    "                # build_model with tuple params\n",
    "                model = build_model(seed=200, **params)\n",
    "                # fit model\n",
    "                history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "                # meaasure objective on model\n",
    "                scores_val = model.forward(x_val)\n",
    "                y_hat_val = np.argmax(scores_val, axis=1)\n",
    "                objective_val = self.objective.get_metrics(y_val, y_hat_val)\n",
    "                # save objective in list\n",
    "                objective_list.append(objective_val)\n",
    "                \n",
    "            # average objective in list\n",
    "            objective_mean = np.array(objective_list).mean()\n",
    "            # save tuple of params and objective as dict\n",
    "            objective_dict = {self.objective.name: objective_mean}\n",
    "            row_dict = {**params, **objective_dict}\n",
    "            rows.append(row_dict)\n",
    "            print(\"*\"*5 + \"\\n\")\n",
    "            \n",
    "        # df from list of dicts of params and objective val\n",
    "        df = pd.DataFrame(data=rows)\n",
    "        \n",
    "        # save to csv\n",
    "        date_string = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        path = os.path.join(\"tuner_results\", date_string + \".csv\")\n",
    "        \n",
    "        df.to_csv(path, encoding='utf-8', index=False)\n",
    "        \n",
    "        # argmax across rows and return best params as dict (~**params)\n",
    "        best_params = dict(df.loc[df[self.objective.name].idxmax()])\n",
    "        best_objective = best_params.pop(self.objective.name)\n",
    "        \n",
    "        return best_objective, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(seed=200, **params):\n",
    "    \n",
    "    assert \"reg_rate_l2\" in params.keys()\n",
    "    reg_rate_l2 = params[\"reg_rate_l2\"]\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    #reg_rate_l2 = 0.025\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    #seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "    print(model)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    # assignment:\n",
    "    #n_epochs = 4\n",
    "    #batch_size = 100\n",
    "\n",
    "    lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    step_size = 900\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    #history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train and val set are batch 1, 2, 3, 4, and 5, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 2\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 900\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_custom(n):\n",
    "    l_min = -5\n",
    "    l_max = -1\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    return [10 **(l_min + (l_max - l_min) * np.random.uniform(low=0, high=1)) for i in range(n)]\n",
    "\n",
    "def coarse_to_fine_custom(best_via_coarse, n):\n",
    "    half_interval = 0.2\n",
    "    low = best_via_coarse * (1-half_interval) \n",
    "    high = best_via_coarse * (1+half_interval)\n",
    "    \n",
    "    return [np.random.uniform(low=low, high=high) for i in range(n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = AccuracyMetrics()\n",
    "build_model = build_model_func\n",
    "\n",
    "# coarse\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_custom(n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")\n",
    "\n",
    "# coarse to fine\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_to_fine_custom(best_params[\"reg_rate_l2\"], n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-oracle",
   "metadata": {},
   "source": [
    "## best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.02, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 3\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "# best obj:0.5134, with {'reg_rate_l2': 0.00036537637001811185}\n",
    "reg_rate_l2 = best_params[\"reg_rate_l2\"]\n",
    "#print(reg_rate_l2)\n",
    "#raise\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 12\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 980\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-fifty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
