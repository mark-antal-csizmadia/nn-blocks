{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "virtual-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss, Loss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separated-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    plt.plot(history[\"loss_train\"], label=\"train\")\n",
    "    plt.plot(history[\"loss_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Loss vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    path = \"losses.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_costs(history):\n",
    "    plt.plot(history[\"cost_train\"], label=\"train\")\n",
    "    plt.plot(history[\"cost_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Cost vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    path = \"costs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history[\"accuracy_train\"], label=\"train\")\n",
    "    plt.plot(history[\"accuracy_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Accuracy vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    path = \"accuracies.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lr(history):\n",
    "    plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning rate vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.legend()\n",
    "    path = \"lrs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "utility-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorLoss(Loss):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        name = \"mse\"\n",
    "        super().__init__(name)\n",
    "\n",
    "    def compute_loss(self, scores, y):\n",
    "        \"\"\" Computes loss of classifier - also includes the regularization losses from previous layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores : numpy.ndarray\n",
    "            Scores. Usually from softmax activation.\n",
    "            Shape is (batch size, )\n",
    "        y : numpy.ndarray\n",
    "            True labels.\n",
    "            Shape is (batch size, )\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The overall loss of the classifier.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        #self.cache[\"g\"] = deepcopy(y)\n",
    "        n = y.shape[0]\n",
    "\n",
    "        loss = np.mean(np.square((y - scores)))\n",
    "        \n",
    "        self.cache[\"g\"] = 2*(scores - y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def grad(self, ):\n",
    "        \"\"\" Computes the gradient of the loss function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or None\n",
    "            None if gradient has not yet been computed.\n",
    "            Shape of gradient is (batch size, ). Note that the grad here is just y.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"g\" in self.cache.keys():\n",
    "            return deepcopy(self.cache[\"g\"])\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attended-category",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate data with linear trend.\n",
    "import random\n",
    "n_data = 10000\n",
    "\n",
    "xs = np.array([float(x/2) for x in range(-n_data, n_data, 1)]).reshape(-1,1)\n",
    "m = 2\n",
    "b = 5\n",
    "ys = np.array([x*m + b + n_data * random.random() for x in xs])\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "\n",
    "x_train = xs[:15000]\n",
    "x_val = xs[15000:18000]\n",
    "x_test = xs[18000:]\n",
    "\n",
    "y_train = ys[:15000]\n",
    "y_val = ys[15000:18000]\n",
    "y_test = ys[18000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cultural-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: dense: \n",
      "\t shape -- in: 1, out: 50\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2), reg: l2 with 1.0000e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t shape -- in: 50, out: 1\n",
      "\t w -- init: Xavier ~ 1.000000 x N(0.000000, 0.141421^2), reg: l2 with 1.0000e-02\n",
      "\t b -- init: Xavier ~ 1.000000 x N(0.000000, 1.000000^2)\n",
      "\t activation: linear\n",
      "\n",
      "mse\n",
      "sgd with constant lr schedule\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 150/150: 100%|██████████| 150/150 [00:00<00:00, 748.24it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-b919e0010541>:31: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.mean(np.square((y - scores)))\n",
      "/home/mark/Documents/KTH/Git Stuff/nn-blocks/regularizers.py:91: RuntimeWarning: overflow encountered in power\n",
      "  return 0.5 * self.reg_rate * np.sum(np.power(param, 2))\n",
      "/home/mark/Documents/KTH/Git Stuff/nn-blocks/layers.py:316: RuntimeWarning: invalid value encountered in add\n",
      "  z = np.dot(x, self.w) + self.b\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fbb0e4d2b073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_val, y_val, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"val loss = {data_loss_val} / val cost = {cost_val}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmetrics_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostfix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0mmetrics_dict_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostfix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mtrain_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\t -- \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_dict_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/models.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(self, y, scores, postfix)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mmetrics_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpostfix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpostfix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/metrics.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, y, scores)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.01\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 1\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=LinearActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = MeanSquaredErrorLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "lr_initial = 1e-3\n",
    "lr_schedule = LRConstantSchedule(lr_initial)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-latitude",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dropout,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "\n",
    "params = {\"mode\": \"test\", \"seed\": None}\n",
    "y_hat_test, scores_test, cost_test, data_loss_test, layers_reg_loss_test = \\\n",
    "    model.predict(x_test, y_test, **params)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-crest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner():\n",
    "    def __init__(self, build_model, objective, iterations=1, **params):\n",
    "        # objective is of Metrics for now\n",
    "        self.build_model = build_model\n",
    "        self.objective = objective\n",
    "        self.iterations = iterations\n",
    "        self.params = params\n",
    "        self.params_product = list(product(*params.values()))\n",
    "        self.params_names = list(params.keys())\n",
    "    \n",
    "    def search(self, x_train, y_train, x_val, y_val, n_epochs, batch_size):\n",
    "        # list of tuples = list(product([1,2,3],[3,4]))\n",
    "        # for tuple in list:\n",
    "        # rows in final df\n",
    "        rows = []\n",
    "        \n",
    "        #params_product = tqdm(self.params_product, file=sys.stdout)\n",
    "        \n",
    "        n_prod = len(self.params_product)\n",
    "        \n",
    "        for idx_prod, prod in enumerate(self.params_product):\n",
    "            \n",
    "            params = {}\n",
    "            for idx, param_name in enumerate(self.params_names):\n",
    "                params[param_name] = prod[idx]\n",
    "            #print(params)\n",
    "            #print(n_prod)\n",
    "            \n",
    "            # if more than 1 iterations\n",
    "            objective_list = []\n",
    "            \n",
    "            for it in range(self.iterations):\n",
    "                print(\"*\"*5)\n",
    "                print(f\"tuner: {idx_prod+1}/{n_prod} config (iter: {it+1}/{self.iterations})\")\n",
    "                # build_model with tuple params\n",
    "                model = build_model(seed=200, **params)\n",
    "                # fit model\n",
    "                history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "                # meaasure objective on model\n",
    "                scores_val = model.forward(x_val)\n",
    "                y_hat_val = np.argmax(scores_val, axis=1)\n",
    "                objective_val = self.objective.get_metrics(y_val, y_hat_val)\n",
    "                # save objective in list\n",
    "                objective_list.append(objective_val)\n",
    "                \n",
    "            # average objective in list\n",
    "            objective_mean = np.array(objective_list).mean()\n",
    "            # save tuple of params and objective as dict\n",
    "            objective_dict = {self.objective.name: objective_mean}\n",
    "            row_dict = {**params, **objective_dict}\n",
    "            rows.append(row_dict)\n",
    "            print(\"*\"*5 + \"\\n\")\n",
    "            \n",
    "        # df from list of dicts of params and objective val\n",
    "        df = pd.DataFrame(data=rows)\n",
    "        \n",
    "        # save to csv\n",
    "        date_string = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        path = os.path.join(\"tuner_results\", date_string + \".csv\")\n",
    "        \n",
    "        df.to_csv(path, encoding='utf-8', index=False)\n",
    "        \n",
    "        # argmax across rows and return best params as dict (~**params)\n",
    "        best_params = dict(df.loc[df[self.objective.name].idxmax()])\n",
    "        best_objective = best_params.pop(self.objective.name)\n",
    "        \n",
    "        return best_objective, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(seed=200, **params):\n",
    "    \n",
    "    assert \"reg_rate_l2\" in params.keys()\n",
    "    reg_rate_l2 = params[\"reg_rate_l2\"]\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    #reg_rate_l2 = 0.025\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    #seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "    print(model)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    # assignment:\n",
    "    #n_epochs = 4\n",
    "    #batch_size = 100\n",
    "\n",
    "    lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    step_size = 900\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    #history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train and val set are batch 1, 2, 3, 4, and 5, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 2\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 900\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_custom(n):\n",
    "    l_min = -5\n",
    "    l_max = -1\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    return [10 **(l_min + (l_max - l_min) * np.random.uniform(low=0, high=1)) for i in range(n)]\n",
    "\n",
    "def coarse_to_fine_custom(best_via_coarse, n):\n",
    "    half_interval = 0.2\n",
    "    low = best_via_coarse * (1-half_interval) \n",
    "    high = best_via_coarse * (1+half_interval)\n",
    "    \n",
    "    return [np.random.uniform(low=low, high=high) for i in range(n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = AccuracyMetrics()\n",
    "build_model = build_model_func\n",
    "\n",
    "# coarse\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_custom(n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")\n",
    "\n",
    "# coarse to fine\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_to_fine_custom(best_params[\"reg_rate_l2\"], n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-perception",
   "metadata": {},
   "source": [
    "## best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.02, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 3\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "# best obj:0.5134, with {'reg_rate_l2': 0.00036537637001811185}\n",
    "reg_rate_l2 = best_params[\"reg_rate_l2\"]\n",
    "#print(reg_rate_l2)\n",
    "#raise\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 12\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 980\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-identification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
