{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twenty-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss, Loss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics, Metrics\n",
    "from optimizers import SGDOptimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "similar-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    plt.plot(history[\"loss_train\"], label=\"train\")\n",
    "    plt.plot(history[\"loss_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Loss vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    path = \"losses.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_costs(history):\n",
    "    plt.plot(history[\"cost_train\"], label=\"train\")\n",
    "    plt.plot(history[\"cost_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Cost vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    path = \"costs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history[\"accuracy_train\"], label=\"train\")\n",
    "    plt.plot(history[\"accuracy_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Accuracy vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    path = \"accuracies.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lr(history):\n",
    "    plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning rate vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.legend()\n",
    "    path = \"lrs.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "palestinian-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorLoss(Loss):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        name = \"mse\"\n",
    "        super().__init__(name)\n",
    "\n",
    "    def compute_loss(self, scores, y):\n",
    "        \"\"\" Computes loss of classifier - also includes the regularization losses from previous layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores : numpy.ndarray\n",
    "            Scores. Usually from softmax activation.\n",
    "            Shape is (batch size, )\n",
    "        y : numpy.ndarray\n",
    "            True labels.\n",
    "            Shape is (batch size, )\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The overall loss of the classifier.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        #self.cache[\"g\"] = deepcopy(y)\n",
    "        n = y.shape[0]\n",
    "        loss = np.mean(np.square((y - scores)))\n",
    "        \n",
    "        self.cache[\"g\"] = 2*(scores - y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def grad(self, ):\n",
    "        \"\"\" Computes the gradient of the loss function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or None\n",
    "            None if gradient has not yet been computed.\n",
    "            Shape of gradient is (batch size, ). Note that the grad here is just y.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        if \"g\" in self.cache.keys():\n",
    "            return deepcopy(self.cache[\"g\"])\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tough-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorMetrics(Metrics):\n",
    "    \"\"\" Accuracy metrics class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of the metric.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__()\n",
    "        Constuctor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        \"\"\" Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"mse metrics\"\n",
    "\n",
    "    def compute(self, y, scores):\n",
    "        \"\"\" Computes the accuracy of inferred numerical labels when compared to their true counterparts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy.ndarray\n",
    "            True labels.\n",
    "            Shape is (number of data points, )\n",
    "        scores : numpy.ndarray\n",
    "            Activation of last layer of the model - the scores of the network.\n",
    "            Shape is (batch_size, out_dim) where out_dim is the output\n",
    "            dimension of the last layer of the model - usually same as\n",
    "            the number of classes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy of inferred numerical labels when compared to their true counterparts.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If y.shape is not the same as y_hat.shape\n",
    "        \"\"\"\n",
    "        return np.mean(np.square((y - scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cosmetic-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate data with linear trend.\n",
    "import random\n",
    "n_data = 500\n",
    "\n",
    "xs = np.array([float(x/2) for x in range(-n_data, n_data, 1)]).reshape(-1,1)\n",
    "m = -0.8\n",
    "b = 5\n",
    "ys = np.array([x*m + b + 100 * random.random() for x in xs])\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "x_train = xs[:800]\n",
    "x_val = xs[800:900]\n",
    "x_test = xs[900:]\n",
    "\n",
    "y_train = ys[:800]\n",
    "y_val = ys[800:900]\n",
    "y_test = ys[900:]\n",
    "\n",
    "print(y_test.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "christian-assurance",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'loss_smoother'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d9a292e59ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeanSquaredErrorLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-041fb688355a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'loss_smoother'"
     ]
    }
   ],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.001\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "reg_rate_l2 = 0.01\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 1\n",
    "mid_dim = 5\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=None, \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=None, \n",
    "          activation=LinearActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "\n",
    "loss = MeanSquaredErrorLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "lr_initial = 1e-8\n",
    "lr_schedule = LRConstantSchedule(lr_initial)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [MeanSquaredErrorMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "verbose = 2\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size, verbose)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "#plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scenic-method",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-780511abb375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores_test' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(x_test, scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "print(model)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "#y_hat_test = np.argmax(scores_test, axis=1)\n",
    "metrics_test = model.compute_metrics(y_test, scores_test)\n",
    "\n",
    "print(f\"test metrics: {json.dumps(metrics_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-fight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dropout,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "\n",
    "params = {\"mode\": \"test\", \"seed\": None}\n",
    "y_hat_test, scores_test, cost_test, data_loss_test, layers_reg_loss_test = \\\n",
    "    model.predict(x_test, y_test, **params)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-still",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner():\n",
    "    def __init__(self, build_model, objective, iterations=1, **params):\n",
    "        # objective is of Metrics for now\n",
    "        self.build_model = build_model\n",
    "        self.objective = objective\n",
    "        self.iterations = iterations\n",
    "        self.params = params\n",
    "        self.params_product = list(product(*params.values()))\n",
    "        self.params_names = list(params.keys())\n",
    "    \n",
    "    def search(self, x_train, y_train, x_val, y_val, n_epochs, batch_size):\n",
    "        # list of tuples = list(product([1,2,3],[3,4]))\n",
    "        # for tuple in list:\n",
    "        # rows in final df\n",
    "        rows = []\n",
    "        \n",
    "        #params_product = tqdm(self.params_product, file=sys.stdout)\n",
    "        \n",
    "        n_prod = len(self.params_product)\n",
    "        \n",
    "        for idx_prod, prod in enumerate(self.params_product):\n",
    "            \n",
    "            params = {}\n",
    "            for idx, param_name in enumerate(self.params_names):\n",
    "                params[param_name] = prod[idx]\n",
    "            #print(params)\n",
    "            #print(n_prod)\n",
    "            \n",
    "            # if more than 1 iterations\n",
    "            objective_list = []\n",
    "            \n",
    "            for it in range(self.iterations):\n",
    "                print(\"*\"*5)\n",
    "                print(f\"tuner: {idx_prod+1}/{n_prod} config (iter: {it+1}/{self.iterations})\")\n",
    "                # build_model with tuple params\n",
    "                model = build_model(seed=200, **params)\n",
    "                # fit model\n",
    "                history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "                # meaasure objective on model\n",
    "                scores_val = model.forward(x_val)\n",
    "                y_hat_val = np.argmax(scores_val, axis=1)\n",
    "                objective_val = self.objective.get_metrics(y_val, y_hat_val)\n",
    "                # save objective in list\n",
    "                objective_list.append(objective_val)\n",
    "                \n",
    "            # average objective in list\n",
    "            objective_mean = np.array(objective_list).mean()\n",
    "            # save tuple of params and objective as dict\n",
    "            objective_dict = {self.objective.name: objective_mean}\n",
    "            row_dict = {**params, **objective_dict}\n",
    "            rows.append(row_dict)\n",
    "            print(\"*\"*5 + \"\\n\")\n",
    "            \n",
    "        # df from list of dicts of params and objective val\n",
    "        df = pd.DataFrame(data=rows)\n",
    "        \n",
    "        # save to csv\n",
    "        date_string = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        path = os.path.join(\"tuner_results\", date_string + \".csv\")\n",
    "        \n",
    "        df.to_csv(path, encoding='utf-8', index=False)\n",
    "        \n",
    "        # argmax across rows and return best params as dict (~**params)\n",
    "        best_params = dict(df.loc[df[self.objective.name].idxmax()])\n",
    "        best_objective = best_params.pop(self.objective.name)\n",
    "        \n",
    "        return best_objective, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(seed=200, **params):\n",
    "    \n",
    "    assert \"reg_rate_l2\" in params.keys()\n",
    "    reg_rate_l2 = params[\"reg_rate_l2\"]\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    #reg_rate_l2 = 0.025\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    #seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "    print(model)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    # assignment:\n",
    "    #n_epochs = 4\n",
    "    #batch_size = 100\n",
    "\n",
    "    lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    step_size = 900\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    #history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train and val set are batch 1, 2, 3, 4, and 5, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 2\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 900\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_custom(n):\n",
    "    l_min = -5\n",
    "    l_max = -1\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    return [10 **(l_min + (l_max - l_min) * np.random.uniform(low=0, high=1)) for i in range(n)]\n",
    "\n",
    "def coarse_to_fine_custom(best_via_coarse, n):\n",
    "    half_interval = 0.2\n",
    "    low = best_via_coarse * (1-half_interval) \n",
    "    high = best_via_coarse * (1+half_interval)\n",
    "    \n",
    "    return [np.random.uniform(low=low, high=high) for i in range(n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = AccuracyMetrics()\n",
    "build_model = build_model_func\n",
    "\n",
    "# coarse\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_custom(n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")\n",
    "\n",
    "# coarse to fine\n",
    "n = 10\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": coarse_to_fine_custom(best_params[\"reg_rate_l2\"], n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-millennium",
   "metadata": {},
   "source": [
    "## best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.02, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(f\"step size of cyc. lr: {n_s} update steps\")\n",
    "\n",
    "cycle_steps = 2*n_s\n",
    "print(f\"full cycle of cyc.lr : {cycle_steps} update steps\")\n",
    "\n",
    "#print(cycle * batch_size)\n",
    "\n",
    "epochs_one_full_cycle = (cycle_steps * batch_size) / x_train.shape[0]\n",
    "print(f\"{epochs_one_full_cycle} epochs = 1 full cycle = {cycle_steps} update steps\")\n",
    "\n",
    "n_cycle = 3\n",
    "print(f\"{n_cycle} cycle = {n_cycle*epochs_one_full_cycle} epochs = {n_cycle*cycle_steps} update steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "# best obj:0.5134, with {'reg_rate_l2': 0.00036537637001811185}\n",
    "reg_rate_l2 = best_params[\"reg_rate_l2\"]\n",
    "#print(reg_rate_l2)\n",
    "#raise\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 12\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 980\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_costs(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)\n",
    "\n",
    "scores_test = model.forward(x_test)\n",
    "y_hat_test = np.argmax(scores_test, axis=1)\n",
    "acc_test = AccuracyMetrics().get_metrics(y_test, y_hat_test)\n",
    "\n",
    "print(f\"test acc: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-activation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
