{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "downtown-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from math import sqrt, ceil\n",
    "import datetime\n",
    "import sys\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "from data_utils import load_cfar10_batch, load_label_names\n",
    "from losses import CategoricalHingeLoss, CategoricalCrossEntropyLoss\n",
    "from activations import LinearActivation, ReLUActivation, SoftmaxActivation\n",
    "from initializers import NormalInitializer, XavierInitializer\n",
    "from layers import Dense\n",
    "from regularizers import L2Regularizer\n",
    "from models import Model\n",
    "from metrics import AccuracyMetrics\n",
    "from optimizers import SGDOptimizer\n",
    "from lr_schedules import LRConstantSchedule, LRExponentialDecaySchedule, LRCyclingSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "third-neighbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (10000, 32, 32, 3), val set shape: (10000, 32, 32, 3), test set shape: (10000, 32, 32, 3)\n",
      "train labels shape: (10000,), val labels shape: (10000,), test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img, y_train = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_val_img, y_val = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "#np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "#np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "later-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    plt.plot(history[\"loss_train\"], label=\"train\")\n",
    "    plt.plot(history[\"loss_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Loss vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history[\"accuracy_train\"], label=\"train\")\n",
    "    plt.plot(history[\"accuracy_val\"], label=\"val\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Accuracy vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lr(history):\n",
    "    plt.plot(history[\"lr\"], label=\"lr\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Learning rate vs. epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cosmetic-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.98it/s]\n",
      "epoch 1/50 \n",
      " \t -- train loss = 2.606658945539659, train accuracy = 0.3417 \n",
      "\t -- val loss = 2.6681031740634893, val accuracy = 0.3172 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 78.85it/s]\n",
      "epoch 2/50 \n",
      " \t -- train loss = 2.359775230345653, train accuracy = 0.411 \n",
      "\t -- val loss = 2.4748258361099564, val accuracy = 0.365 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.61it/s]\n",
      "epoch 3/50 \n",
      " \t -- train loss = 2.143277777061135, train accuracy = 0.4602 \n",
      "\t -- val loss = 2.3052493454702057, val accuracy = 0.3991 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 81.53it/s]\n",
      "epoch 4/50 \n",
      " \t -- train loss = 2.0185511405918577, train accuracy = 0.4714 \n",
      "\t -- val loss = 2.213293365780997, val accuracy = 0.4009 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.24it/s]\n",
      "epoch 5/50 \n",
      " \t -- train loss = 1.8980300519254043, train accuracy = 0.4829 \n",
      "\t -- val loss = 2.113128216837789, val accuracy = 0.4037 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 83.60it/s]\n",
      "epoch 6/50 \n",
      " \t -- train loss = 1.871807941023265, train accuracy = 0.462 \n",
      "\t -- val loss = 2.092441581343763, val accuracy = 0.3833 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.17it/s]\n",
      "epoch 7/50 \n",
      " \t -- train loss = 1.7686306660925843, train accuracy = 0.4818 \n",
      "\t -- val loss = 2.0041542711311577, val accuracy = 0.3952 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 80.19it/s]\n",
      "epoch 8/50 \n",
      " \t -- train loss = 1.6897014698308062, train accuracy = 0.4955 \n",
      "\t -- val loss = 1.9243431423959847, val accuracy = 0.4042 \n",
      "\n",
      "\n",
      "starting epoch: 9 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 75.90it/s]\n",
      "epoch 9/50 \n",
      " \t -- train loss = 1.6421543854815082, train accuracy = 0.509 \n",
      "\t -- val loss = 1.8952755535945478, val accuracy = 0.4154 \n",
      "\n",
      "\n",
      "starting epoch: 10 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 76.54it/s]\n",
      "epoch 10/50 \n",
      " \t -- train loss = 1.5862853410436997, train accuracy = 0.528 \n",
      "\t -- val loss = 1.8679048404074925, val accuracy = 0.4239 \n",
      "\n",
      "\n",
      "starting epoch: 11 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.78it/s]\n",
      "epoch 11/50 \n",
      " \t -- train loss = 1.5326399750858788, train accuracy = 0.556 \n",
      "\t -- val loss = 1.827610828261105, val accuracy = 0.4386 \n",
      "\n",
      "\n",
      "starting epoch: 12 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 77.95it/s]\n",
      "epoch 12/50 \n",
      " \t -- train loss = 1.5080649245243494, train accuracy = 0.5579 \n",
      "\t -- val loss = 1.8347482295327804, val accuracy = 0.4372 \n",
      "\n",
      "\n",
      "starting epoch: 13 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 75.06it/s]\n",
      "epoch 13/50 \n",
      " \t -- train loss = 1.4681841596199967, train accuracy = 0.5831 \n",
      "\t -- val loss = 1.8142385193190345, val accuracy = 0.4488 \n",
      "\n",
      "\n",
      "starting epoch: 14 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 71.99it/s]\n",
      "epoch 14/50 \n",
      " \t -- train loss = 1.4343588504049758, train accuracy = 0.604 \n",
      "\t -- val loss = 1.7930521850369314, val accuracy = 0.4577 \n",
      "\n",
      "\n",
      "starting epoch: 15 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.50it/s]\n",
      "epoch 15/50 \n",
      " \t -- train loss = 1.4097646362212837, train accuracy = 0.6166 \n",
      "\t -- val loss = 1.783687692968002, val accuracy = 0.4608 \n",
      "\n",
      "\n",
      "starting epoch: 16 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 72.06it/s]\n",
      "epoch 16/50 \n",
      " \t -- train loss = 1.3999868411697654, train accuracy = 0.6204 \n",
      "\t -- val loss = 1.7827858556718437, val accuracy = 0.4642 \n",
      "\n",
      "\n",
      "starting epoch: 17 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 72.17it/s]\n",
      "epoch 17/50 \n",
      " \t -- train loss = 1.4036349622299789, train accuracy = 0.619 \n",
      "\t -- val loss = 1.7852619062571728, val accuracy = 0.4622 \n",
      "\n",
      "\n",
      "starting epoch: 18 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 70.71it/s]\n",
      "epoch 18/50 \n",
      " \t -- train loss = 1.4222630446951388, train accuracy = 0.6014 \n",
      "\t -- val loss = 1.8152794307781455, val accuracy = 0.4544 \n",
      "\n",
      "\n",
      "starting epoch: 19 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.92it/s]\n",
      "epoch 19/50 \n",
      " \t -- train loss = 1.4482120148651279, train accuracy = 0.598 \n",
      "\t -- val loss = 1.8492452342494743, val accuracy = 0.4422 \n",
      "\n",
      "\n",
      "starting epoch: 20 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 72.18it/s]\n",
      "epoch 20/50 \n",
      " \t -- train loss = 1.459667343590966, train accuracy = 0.5922 \n",
      "\t -- val loss = 1.8575039328525043, val accuracy = 0.4474 \n",
      "\n",
      "\n",
      "starting epoch: 21 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.40it/s]\n",
      "epoch 21/50 \n",
      " \t -- train loss = 1.5829221933707187, train accuracy = 0.5407 \n",
      "\t -- val loss = 1.967607964690245, val accuracy = 0.4057 \n",
      "\n",
      "\n",
      "starting epoch: 22 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 70.02it/s]\n",
      "epoch 22/50 \n",
      " \t -- train loss = 1.5743227510141444, train accuracy = 0.5435 \n",
      "\t -- val loss = 1.9707909153560788, val accuracy = 0.4102 \n",
      "\n",
      "\n",
      "starting epoch: 23 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 73.13it/s]\n",
      "epoch 23/50 \n",
      " \t -- train loss = 1.596709459259242, train accuracy = 0.5247 \n",
      "\t -- val loss = 1.9557528083096178, val accuracy = 0.4069 \n",
      "\n",
      "\n",
      "starting epoch: 24 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 71.50it/s]\n",
      "epoch 24/50 \n",
      " \t -- train loss = 1.5806234963535402, train accuracy = 0.5419 \n",
      "\t -- val loss = 1.901351164994527, val accuracy = 0.4268 \n",
      "\n",
      "\n",
      "starting epoch: 25 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 71.64it/s]\n",
      "epoch 25/50 \n",
      " \t -- train loss = 1.5220875310214286, train accuracy = 0.5668 \n",
      "\t -- val loss = 1.8666845416283528, val accuracy = 0.434 \n",
      "\n",
      "\n",
      "starting epoch: 26 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 71.42it/s]\n",
      "epoch 26/50 \n",
      " \t -- train loss = 1.5127400187862352, train accuracy = 0.5639 \n",
      "\t -- val loss = 1.8772255362763324, val accuracy = 0.432 \n",
      "\n",
      "\n",
      "starting epoch: 27 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 72.12it/s]\n",
      "epoch 27/50 \n",
      " \t -- train loss = 1.4912248658394291, train accuracy = 0.5772 \n",
      "\t -- val loss = 1.8594219826957625, val accuracy = 0.4442 \n",
      "\n",
      "\n",
      "starting epoch: 28 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 71.52it/s]\n",
      "epoch 28/50 \n",
      " \t -- train loss = 1.4410945977345335, train accuracy = 0.5994 \n",
      "\t -- val loss = 1.8325245976785398, val accuracy = 0.4568 \n",
      "\n",
      "\n",
      "starting epoch: 29 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 69.69it/s]\n",
      "epoch 29/50 \n",
      " \t -- train loss = 1.4208250400977291, train accuracy = 0.6156 \n",
      "\t -- val loss = 1.832661964462716, val accuracy = 0.4542 \n",
      "\n",
      "\n",
      "starting epoch: 30 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 73.17it/s]\n",
      "epoch 30/50 \n",
      " \t -- train loss = 1.3839547828986614, train accuracy = 0.6396 \n",
      "\t -- val loss = 1.8178834246497992, val accuracy = 0.4636 \n",
      "\n",
      "\n",
      "starting epoch: 31 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 73.22it/s]\n",
      "epoch 31/50 \n",
      " \t -- train loss = 1.366518256273311, train accuracy = 0.6437 \n",
      "\t -- val loss = 1.81150622351263, val accuracy = 0.4616 \n",
      "\n",
      "\n",
      "starting epoch: 32 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 69.53it/s]\n",
      "epoch 32/50 \n",
      " \t -- train loss = 1.352939636423767, train accuracy = 0.6537 \n",
      "\t -- val loss = 1.803529357324239, val accuracy = 0.4672 \n",
      "\n",
      "\n",
      "starting epoch: 33 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 71.38it/s]\n",
      "epoch 33/50 \n",
      " \t -- train loss = 1.3606840859527278, train accuracy = 0.6535 \n",
      "\t -- val loss = 1.8125613793818085, val accuracy = 0.4663 \n",
      "\n",
      "\n",
      "starting epoch: 34 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 72.18it/s]\n",
      "epoch 34/50 \n",
      " \t -- train loss = 1.386439732178883, train accuracy = 0.6435 \n",
      "\t -- val loss = 1.8414875131289117, val accuracy = 0.4613 \n",
      "\n",
      "\n",
      "starting epoch: 35 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 70.94it/s]\n",
      "epoch 35/50 \n",
      " \t -- train loss = 1.406693419792264, train accuracy = 0.6319 \n",
      "\t -- val loss = 1.8651760939501851, val accuracy = 0.4514 \n",
      "\n",
      "\n",
      "starting epoch: 36 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 85.91it/s]\n",
      "epoch 36/50 \n",
      " \t -- train loss = 1.4126178322882967, train accuracy = 0.6228 \n",
      "\t -- val loss = 1.8831846868047704, val accuracy = 0.444 \n",
      "\n",
      "\n",
      "starting epoch: 37 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.80it/s]\n",
      "epoch 37/50 \n",
      " \t -- train loss = 1.4542017473012678, train accuracy = 0.6016 \n",
      "\t -- val loss = 1.910291058476556, val accuracy = 0.4358 \n",
      "\n",
      "\n",
      "starting epoch: 38 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.18it/s]\n",
      "epoch 38/50 \n",
      " \t -- train loss = 1.5278395374799125, train accuracy = 0.5725 \n",
      "\t -- val loss = 1.9577431563622334, val accuracy = 0.4271 \n",
      "\n",
      "\n",
      "starting epoch: 39 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 81.27it/s]\n",
      "epoch 39/50 \n",
      " \t -- train loss = 1.5360878884466156, train accuracy = 0.5638 \n",
      "\t -- val loss = 1.939090905927235, val accuracy = 0.4189 \n",
      "\n",
      "\n",
      "starting epoch: 40 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 76.33it/s]\n",
      "epoch 40/50 \n",
      " \t -- train loss = 1.5912511154756632, train accuracy = 0.5353 \n",
      "\t -- val loss = 1.967804390150363, val accuracy = 0.4078 \n",
      "\n",
      "\n",
      "starting epoch: 41 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 82.37it/s]\n",
      "epoch 41/50 \n",
      " \t -- train loss = 1.4956591909449601, train accuracy = 0.5831 \n",
      "\t -- val loss = 1.8765847597696543, val accuracy = 0.4384 \n",
      "\n",
      "\n",
      "starting epoch: 42 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 75.97it/s]\n",
      "epoch 42/50 \n",
      " \t -- train loss = 1.553248524237317, train accuracy = 0.5479 \n",
      "\t -- val loss = 1.9355618271417812, val accuracy = 0.4165 \n",
      "\n",
      "\n",
      "starting epoch: 43 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 81.62it/s]\n",
      "epoch 43/50 \n",
      " \t -- train loss = 1.504569038614227, train accuracy = 0.574 \n",
      "\t -- val loss = 1.9012276801698689, val accuracy = 0.4371 \n",
      "\n",
      "\n",
      "starting epoch: 44 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.91it/s]\n",
      "epoch 44/50 \n",
      " \t -- train loss = 1.4197870457936097, train accuracy = 0.6169 \n",
      "\t -- val loss = 1.8357587827445858, val accuracy = 0.4541 \n",
      "\n",
      "\n",
      "starting epoch: 45 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.27it/s]\n",
      "epoch 45/50 \n",
      " \t -- train loss = 1.3893077860019074, train accuracy = 0.6371 \n",
      "\t -- val loss = 1.8347048923584237, val accuracy = 0.4585 \n",
      "\n",
      "\n",
      "starting epoch: 46 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 77.02it/s]\n",
      "epoch 46/50 \n",
      " \t -- train loss = 1.3646117638141724, train accuracy = 0.6518 \n",
      "\t -- val loss = 1.8243054706350468, val accuracy = 0.4643 \n",
      "\n",
      "\n",
      "starting epoch: 47 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 82.20it/s]\n",
      "epoch 47/50 \n",
      " \t -- train loss = 1.3472578713946288, train accuracy = 0.6598 \n",
      "\t -- val loss = 1.819997916813654, val accuracy = 0.468 \n",
      "\n",
      "\n",
      "starting epoch: 48 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 79.38it/s]\n",
      "epoch 48/50 \n",
      " \t -- train loss = 1.33635109900698, train accuracy = 0.6702 \n",
      "\t -- val loss = 1.8143617536920051, val accuracy = 0.4713 \n",
      "\n",
      "\n",
      "starting epoch: 49 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 74.73it/s]\n",
      "epoch 49/50 \n",
      " \t -- train loss = 1.3425720583495646, train accuracy = 0.6705 \n",
      "\t -- val loss = 1.8205582356373913, val accuracy = 0.4685 \n",
      "\n",
      "\n",
      "starting epoch: 50 ...\n",
      "batch 100/100: 100%|██████████| 100/100 [00:01<00:00, 77.05it/s]\n",
      "epoch 50/50 \n",
      " \t -- train loss = 1.3554405917232168, train accuracy = 0.655 \n",
      "\t -- val loss = 1.8422247358706452, val accuracy = 0.4637 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 800\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-strap",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "limiting-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003235781502499629\n"
     ]
    }
   ],
   "source": [
    "l_min = -5\n",
    "l_max = -1\n",
    "seed = None#100\n",
    "np.random.seed(seed)\n",
    "l = l_min + (l_max - l_min) * np.random.uniform(low=0, high=1)\n",
    "print(10**l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "vital-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner():\n",
    "    def __init__(self, build_model, objective, iterations=1, **params):\n",
    "        # objective is of Metrics for now\n",
    "        self.build_model = build_model\n",
    "        self.objective = objective\n",
    "        self.iterations = iterations\n",
    "        self.params = params\n",
    "        self.params_product = list(product(*params.values()))\n",
    "        self.params_names = list(params.keys())\n",
    "    \n",
    "    def search(self, x_train, y_train, x_val, y_val, n_epochs, batch_size):\n",
    "        # list of tuples = list(product([1,2,3],[3,4]))\n",
    "        # for tuple in list:\n",
    "        # rows in final df\n",
    "        rows = []\n",
    "        \n",
    "        #params_product = tqdm(self.params_product, file=sys.stdout)\n",
    "        \n",
    "        for prod in self.params_product:\n",
    "            params = {}\n",
    "            for idx, param_name in enumerate(self.params_names):\n",
    "                params[param_name] = prod[idx]\n",
    "            #print(params)\n",
    "            \n",
    "            # if more than 1 iterations\n",
    "            objective_list = []\n",
    "            \n",
    "            for it in range(self.iterations):\n",
    "                # build_model with tuple params\n",
    "                model = build_model(seed=200, **params)\n",
    "                # fit model\n",
    "                history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "                # meaasure objective on model\n",
    "                scores_val = model.forward(x_val)\n",
    "                y_hat_val = np.argmax(scores_val, axis=1)\n",
    "                objective_val = self.objective.get_metrics(y_val, y_hat_val)\n",
    "                # save objective in list\n",
    "                objective_list.append(objective_val)\n",
    "                \n",
    "            # average objective in list\n",
    "            objective_mean = np.array(objective_list).mean()\n",
    "            # save tuple of params and objective as dict\n",
    "            objective_dict = {self.objective.name: objective_mean}\n",
    "            row_dict = {**params, **objective_dict}\n",
    "            rows.append(row_dict)\n",
    "        \n",
    "        # df from list of dicts of params and objective val\n",
    "        df = pd.DataFrame(data=rows)\n",
    "        \n",
    "        # save to csv\n",
    "        date_string = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        path = os.path.join(\"tuner_results\", date_string + \".csv\")\n",
    "        \n",
    "        df.to_csv(path, encoding='utf-8', index=False)\n",
    "        \n",
    "        # argmax across rows and return best params as dict (~**params)\n",
    "        best_params = dict(df.loc[df[self.objective.name].idxmax()])\n",
    "        best_objective = best_params.pop(self.objective.name)\n",
    "        \n",
    "        return best_objective, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "statistical-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(seed=200, **params):\n",
    "    \n",
    "    assert \"reg_rate_l2\" in params.keys()\n",
    "    reg_rate_l2 = params[\"reg_rate_l2\"]\n",
    "    \n",
    "    params = {\"coeff\": 1.0, \"mean\": 0.0, \"std\":None}\n",
    "\n",
    "    #reg_rate_l2 = 0.025\n",
    "\n",
    "    in_dim = x_train.shape[1]\n",
    "    out_dim = 10\n",
    "    mid_dim = 50\n",
    "\n",
    "    #seed = 200\n",
    "\n",
    "    dense_1 = \\\n",
    "        Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "              kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=ReLUActivation()\n",
    "             )\n",
    "\n",
    "    dense_2 = \\\n",
    "        Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "              kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "              bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "              kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "              activation=SoftmaxActivation()\n",
    "             )\n",
    "\n",
    "    layers = [\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ]\n",
    "\n",
    "    model = Model(layers)\n",
    "    print(model)\n",
    "\n",
    "    loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    # assignment:\n",
    "    #n_epochs = 4\n",
    "    #batch_size = 100\n",
    "\n",
    "    lr_initial = 1e-5\n",
    "    lr_max = 1e-1\n",
    "    step_size = 200#800\n",
    "    lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "    optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "    metrics = [AccuracyMetrics()]\n",
    "\n",
    "    model.compile_model(optimizer, loss, metrics)\n",
    "    #history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "corporate-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hyper_param(n):\n",
    "    l_min = -5\n",
    "    l_max = -1\n",
    "    #np.random.seed(seed)\n",
    "    \n",
    "    return [10 **(l_min + (l_max - l_min) * np.random.uniform(low=0, high=1)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bearing-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (45000, 32, 32, 3), val set shape: (5000, 32, 32, 3), test set shape: (10000, 32, 32, 3)\n",
      "train labels shape: (45000,), val labels shape: (5000,), test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# train set is batch 1, val set is batch 2, test set is test\n",
    "path = os.path.join(\"data\", \"data_batch_1\")\n",
    "x_train_img_1, y_train_1 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_2\")\n",
    "x_train_img_2, y_train_2 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_3\")\n",
    "x_train_img_3, y_train_3 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_4\")\n",
    "x_train_img_4, y_train_4 = load_cfar10_batch(path)\n",
    "\n",
    "path = os.path.join(\"data\", \"data_batch_5\")\n",
    "x_train_img_5, y_train_5 = load_cfar10_batch(path)\n",
    "\n",
    "x_train_val_img = np.vstack([x_train_img_1, x_train_img_2, x_train_img_3, x_train_img_4, x_train_img_5])\n",
    "y_train_val = np.hstack([y_train_1, y_train_2, y_train_3, y_train_4, y_train_5])\n",
    "\n",
    "x_train_img, x_val_img, y_train, y_val = train_test_split(x_train_val_img, y_train_val,\n",
    "                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "path = os.path.join(\"data\", \"test_batch\")\n",
    "x_test_img, y_test = load_cfar10_batch(path)\n",
    "\n",
    "# check counts in datasets\n",
    "print(f\"train set shape: {x_train_img.shape}, \"\n",
    "      f\"val set shape: {x_val_img.shape}, test set shape: {x_test_img.shape}\")\n",
    "print(f\"train labels shape: {y_train.shape},\"\n",
    "      f\" val labels shape: {y_val.shape}, test labels shape: {y_test.shape}\")\n",
    "\n",
    "# assert balanced dataset\n",
    "train_counts = np.unique(y_train, return_counts=True)[1]\n",
    "train_ratios = train_counts / train_counts.sum()\n",
    "\n",
    "val_counts = np.unique(y_val, return_counts=True)[1]\n",
    "val_ratios = val_counts / val_counts.sum()\n",
    "\n",
    "test_counts = np.unique(y_test, return_counts=True)[1]\n",
    "test_ratios = test_counts / test_counts.sum()\n",
    "\n",
    "# np.testing.assert_array_equal(train_ratios, val_ratios)\n",
    "# np.testing.assert_array_equal(val_ratios, test_ratios)\n",
    "\n",
    "#np.testing.assert_allclose(train_ratios, val_ratios, rtol=1e-1, atol=0)\n",
    "#np.testing.assert_allclose(val_ratios, test_ratios, rtol=1e-1, atol=0)\n",
    "\n",
    "# Pre-process data\n",
    "x_train_un = x_train_img.reshape(x_train_img.shape[0], -1)\n",
    "x_val_un = x_val_img.reshape(x_val_img.shape[0], -1)\n",
    "x_test_un = x_test_img.reshape(x_test_img.shape[0], -1)\n",
    "\n",
    "x_train = x_train_un / 255.\n",
    "x_val = x_val_un / 255.\n",
    "x_test = x_test_un / 255.\n",
    "\n",
    "mean = np.mean(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "std = np.std(x_train, axis=0).reshape(1, x_train.shape[1])\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_val = (x_val - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "chemical-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "1800\n",
      "180000\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "n_s = int(2*np.floor(x_train.shape[0] / batch_size))\n",
    "print(n_s)\n",
    "\n",
    "cycle = 2*n_s\n",
    "print(cycle)\n",
    "print(cycle * batch_size)\n",
    "\n",
    "epochs = (cycle * batch_size) / x_train.shape[0]\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "hungry-least",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 76.54it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 2.0981094489946717, train accuracy = 0.41884444444444446 \n",
      "\t -- val loss = 2.135051082065452, val accuracy = 0.4048 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 77.46it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.8367577691077885, train accuracy = 0.40344444444444444 \n",
      "\t -- val loss = 1.8916862705088437, val accuracy = 0.3854 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 76.04it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.622553747652356, train accuracy = 0.48688888888888887 \n",
      "\t -- val loss = 1.6967598385070621, val accuracy = 0.4486 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.42it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5600461150009992, train accuracy = 0.5118444444444444 \n",
      "\t -- val loss = 1.64206332771556, val accuracy = 0.4784 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.48it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.6185691763713579, train accuracy = 0.4872888888888889 \n",
      "\t -- val loss = 1.7034509985431932, val accuracy = 0.4486 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.45it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.734826714288117, train accuracy = 0.43324444444444443 \n",
      "\t -- val loss = 1.8055576323144926, val accuracy = 0.4116 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.48it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.6108524959711226, train accuracy = 0.49124444444444443 \n",
      "\t -- val loss = 1.695831510201712, val accuracy = 0.4578 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.66it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.5312270900493505, train accuracy = 0.5287333333333334 \n",
      "\t -- val loss = 1.619302756032942, val accuracy = 0.492 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coeff = 1.0\n",
    "mean = 0.0\n",
    "std = 0.01\n",
    "params = {\"coeff\":coeff, \"mean\": mean, \"std\":None}\n",
    "\n",
    "#reg_rate_l2 = 0.1\n",
    "reg_rate_l2 = 0.025\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "out_dim = 10\n",
    "mid_dim = 50\n",
    "\n",
    "seed = 200\n",
    "\n",
    "dense_1 = \\\n",
    "    Dense(in_dim=in_dim, out_dim=mid_dim, \n",
    "          kernel_initializer=XavierInitializer(seed=seed, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+1, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=ReLUActivation()\n",
    "         )\n",
    "\n",
    "dense_2 = \\\n",
    "    Dense(in_dim=mid_dim, out_dim=out_dim,\n",
    "          kernel_initializer=XavierInitializer(seed=seed+2, **params), \n",
    "          bias_initializer=XavierInitializer(seed=seed+3, **params), \n",
    "          kernel_regularizer=L2Regularizer(reg_rate=reg_rate_l2), \n",
    "          activation=SoftmaxActivation()\n",
    "         )\n",
    "\n",
    "layers = [\n",
    "    dense_1,\n",
    "    dense_2\n",
    "]\n",
    "\n",
    "model = Model(layers)\n",
    "print(model)\n",
    "\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "#lr_initial = 0.01\n",
    "#lr_schedule = LRConstantSchedule(lr_initial)\n",
    "#decay_steps = n_epochs * 2\n",
    "#decay_rate = 0.9\n",
    "#lr_schedule = LRExponentialDecaySchedule(lr_initial, decay_steps, decay_rate)\n",
    "\n",
    "lr_initial = 1e-5\n",
    "lr_max = 1e-1\n",
    "step_size = 900\n",
    "lr_schedule = LRCyclingSchedule(lr_initial, lr_max, step_size)\n",
    "optimizer = SGDOptimizer(lr_schedule=lr_schedule)\n",
    "\n",
    "metrics = [AccuracyMetrics()]\n",
    "\n",
    "model.compile_model(optimizer, loss, metrics)\n",
    "history = model.fit(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "plot_losses(history)\n",
    "plot_accuracies(history)\n",
    "plot_lr(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "structured-dance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 77.83it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.52400142557297, train accuracy = 0.4716666666666667 \n",
      "\t -- val loss = 1.5809540144570298, val accuracy = 0.4414 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.72it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.479128759319482, train accuracy = 0.48204444444444444 \n",
      "\t -- val loss = 1.5739663410607219, val accuracy = 0.4456 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 76.36it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4222555429948258, train accuracy = 0.5036666666666667 \n",
      "\t -- val loss = 1.5482117858541584, val accuracy = 0.4598 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.10it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4820999349255513, train accuracy = 0.4812222222222222 \n",
      "\t -- val loss = 1.6294109627459303, val accuracy = 0.432 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 78.50it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4540964628949926, train accuracy = 0.4926 \n",
      "\t -- val loss = 1.6111675977968503, val accuracy = 0.4436 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.89it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.346209028866905, train accuracy = 0.5321111111111111 \n",
      "\t -- val loss = 1.523316585199056, val accuracy = 0.47 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.45it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2768729727644592, train accuracy = 0.5592666666666667 \n",
      "\t -- val loss = 1.4770163796160385, val accuracy = 0.4858 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.81it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2332920515375743, train accuracy = 0.5776222222222223 \n",
      "\t -- val loss = 1.4571088409013042, val accuracy = 0.5002 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 70.93it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5260267031209704, train accuracy = 0.4725111111111111 \n",
      "\t -- val loss = 1.5868896734008604, val accuracy = 0.4498 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.85it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4860140195180525, train accuracy = 0.48084444444444446 \n",
      "\t -- val loss = 1.586340211624356, val accuracy = 0.4522 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.18it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4339729407267126, train accuracy = 0.5010444444444444 \n",
      "\t -- val loss = 1.562092889142631, val accuracy = 0.4556 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.42it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5295672125801756, train accuracy = 0.47013333333333335 \n",
      "\t -- val loss = 1.6767257883724245, val accuracy = 0.4294 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 70.67it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4513979114275877, train accuracy = 0.49446666666666667 \n",
      "\t -- val loss = 1.611807656280442, val accuracy = 0.4484 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.36it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3580666455190105, train accuracy = 0.5289555555555555 \n",
      "\t -- val loss = 1.5344182684605483, val accuracy = 0.471 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.15it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.285451362215902, train accuracy = 0.5586888888888889 \n",
      "\t -- val loss = 1.4856830812484385, val accuracy = 0.4832 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 65.83it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2432739800995498, train accuracy = 0.5760444444444445 \n",
      "\t -- val loss = 1.4671265997432243, val accuracy = 0.4964 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.16it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5138533939484276, train accuracy = 0.47217777777777775 \n",
      "\t -- val loss = 1.5726330777567092, val accuracy = 0.4418 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:07<00:00, 62.79it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4729177730736513, train accuracy = 0.4801111111111111 \n",
      "\t -- val loss = 1.5694381364709047, val accuracy = 0.4462 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.65it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4109605759814061, train accuracy = 0.5024666666666666 \n",
      "\t -- val loss = 1.5383355337195614, val accuracy = 0.4586 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.14it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.486586503887427, train accuracy = 0.47515555555555555 \n",
      "\t -- val loss = 1.630728886219798, val accuracy = 0.4342 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 79.15it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4321915215015797, train accuracy = 0.4924222222222222 \n",
      "\t -- val loss = 1.5962407877015683, val accuracy = 0.4416 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 79.82it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.334332179428542, train accuracy = 0.5338 \n",
      "\t -- val loss = 1.5141463976163017, val accuracy = 0.4638 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.32it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2609001732800067, train accuracy = 0.5596 \n",
      "\t -- val loss = 1.463790084220682, val accuracy = 0.4874 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.72it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2174807801061294, train accuracy = 0.5746 \n",
      "\t -- val loss = 1.449997077880172, val accuracy = 0.4964 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.74it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5441719842126738, train accuracy = 0.46913333333333335 \n",
      "\t -- val loss = 1.6025213383417194, val accuracy = 0.441 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.56it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4954903208561672, train accuracy = 0.4806 \n",
      "\t -- val loss = 1.585572752788508, val accuracy = 0.4484 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.88it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4369164676580288, train accuracy = 0.5058666666666667 \n",
      "\t -- val loss = 1.5648218399248497, val accuracy = 0.4602 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 67.41it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5502459024662802, train accuracy = 0.4644888888888889 \n",
      "\t -- val loss = 1.688417927138274, val accuracy = 0.4266 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 70.82it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4822901957501555, train accuracy = 0.48817777777777777 \n",
      "\t -- val loss = 1.6303430788540072, val accuracy = 0.4442 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.46it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3637626682934154, train accuracy = 0.5329111111111111 \n",
      "\t -- val loss = 1.5332553060354153, val accuracy = 0.4706 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.01it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2974829107943062, train accuracy = 0.5577555555555556 \n",
      "\t -- val loss = 1.4867152458286925, val accuracy = 0.4936 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.59it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2588618460954273, train accuracy = 0.5750444444444445 \n",
      "\t -- val loss = 1.4654016726449284, val accuracy = 0.5026 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.36it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8865329537303048, train accuracy = 0.4321333333333333 \n",
      "\t -- val loss = 1.9139475867561293, val accuracy = 0.4164 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 79.90it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.8298094160730214, train accuracy = 0.4162666666666667 \n",
      "\t -- val loss = 1.8633932244947158, val accuracy = 0.4016 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 66.96it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.820204824360511, train accuracy = 0.425 \n",
      "\t -- val loss = 1.8569450046409899, val accuracy = 0.4052 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.81it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.872346768772214, train accuracy = 0.4011777777777778 \n",
      "\t -- val loss = 1.903092682365935, val accuracy = 0.388 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 77.64it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.8602393948995488, train accuracy = 0.4022222222222222 \n",
      "\t -- val loss = 1.901441279833594, val accuracy = 0.3878 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.10it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.815744824806187, train accuracy = 0.43148888888888887 \n",
      "\t -- val loss = 1.8549492501717988, val accuracy = 0.4134 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.45it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.7875642375285052, train accuracy = 0.44864444444444446 \n",
      "\t -- val loss = 1.8258323233382607, val accuracy = 0.432 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.10it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.7729832432003358, train accuracy = 0.45568888888888887 \n",
      "\t -- val loss = 1.8130628733552376, val accuracy = 0.4354 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.36it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5141213010522827, train accuracy = 0.4713777777777778 \n",
      "\t -- val loss = 1.572678704519521, val accuracy = 0.4432 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.68it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4742282690035424, train accuracy = 0.47786666666666666 \n",
      "\t -- val loss = 1.569898549872761, val accuracy = 0.4464 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.64it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.411699758439066, train accuracy = 0.5045111111111111 \n",
      "\t -- val loss = 1.5382441398243456, val accuracy = 0.4566 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.02it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5017965601529075, train accuracy = 0.47102222222222223 \n",
      "\t -- val loss = 1.6486955248316069, val accuracy = 0.4346 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 78.33it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4459428984269078, train accuracy = 0.4892222222222222 \n",
      "\t -- val loss = 1.6111075979766927, val accuracy = 0.4386 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.80it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3347022533374822, train accuracy = 0.5323555555555556 \n",
      "\t -- val loss = 1.5114081450929178, val accuracy = 0.468 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.79it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.261167750011719, train accuracy = 0.5582666666666667 \n",
      "\t -- val loss = 1.4660605891125373, val accuracy = 0.4872 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.62it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2179012754520846, train accuracy = 0.5751777777777778 \n",
      "\t -- val loss = 1.4518567785103424, val accuracy = 0.492 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.70it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.7069646383369024, train accuracy = 0.46773333333333333 \n",
      "\t -- val loss = 1.7635379192164138, val accuracy = 0.439 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.35it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.6464083610253208, train accuracy = 0.4736 \n",
      "\t -- val loss = 1.7280988852626566, val accuracy = 0.443 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.71it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.57663260501231, train accuracy = 0.4945555555555556 \n",
      "\t -- val loss = 1.6844029803677425, val accuracy = 0.4546 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.95it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.640877734860899, train accuracy = 0.4630444444444444 \n",
      "\t -- val loss = 1.7527264420299484, val accuracy = 0.434 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.24it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5791874734357987, train accuracy = 0.48504444444444444 \n",
      "\t -- val loss = 1.701526651663448, val accuracy = 0.4464 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.73it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.495348837458057, train accuracy = 0.5181777777777777 \n",
      "\t -- val loss = 1.623887433490729, val accuracy = 0.4636 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.89it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.4380608340406342, train accuracy = 0.5436222222222222 \n",
      "\t -- val loss = 1.5771861167480694, val accuracy = 0.4866 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.54it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.4040070835272576, train accuracy = 0.5567111111111112 \n",
      "\t -- val loss = 1.5517780163279908, val accuracy = 0.498 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.30it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.757359609213861, train accuracy = 0.4666222222222222 \n",
      "\t -- val loss = 1.8085330731213785, val accuracy = 0.441 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.681018112689276, train accuracy = 0.46828888888888887 \n",
      "\t -- val loss = 1.7625906042606854, val accuracy = 0.4434 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.93it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.6070219911942312, train accuracy = 0.48506666666666665 \n",
      "\t -- val loss = 1.7051402037585652, val accuracy = 0.451 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.26it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6455559261733215, train accuracy = 0.46582222222222225 \n",
      "\t -- val loss = 1.7456248978136308, val accuracy = 0.4342 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.71it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.601659635544884, train accuracy = 0.4809777777777778 \n",
      "\t -- val loss = 1.7129992901639195, val accuracy = 0.444 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.35it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.5280310895214821, train accuracy = 0.5067333333333334 \n",
      "\t -- val loss = 1.6449536451518276, val accuracy = 0.4594 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.43it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.4742945695549214, train accuracy = 0.5324 \n",
      "\t -- val loss = 1.5975229365906158, val accuracy = 0.4814 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.42it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.4404750300313816, train accuracy = 0.5475777777777778 \n",
      "\t -- val loss = 1.571012735191273, val accuracy = 0.4942 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.17it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.52636918854963, train accuracy = 0.47142222222222224 \n",
      "\t -- val loss = 1.587367762201802, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.92it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4861037674072837, train accuracy = 0.4787111111111111 \n",
      "\t -- val loss = 1.5831507794406845, val accuracy = 0.4504 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.62it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4290953465956047, train accuracy = 0.5021777777777777 \n",
      "\t -- val loss = 1.5587424304263493, val accuracy = 0.4618 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.98it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.486278762805841, train accuracy = 0.4825111111111111 \n",
      "\t -- val loss = 1.633717851834859, val accuracy = 0.4326 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.51it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4629439161252442, train accuracy = 0.49148888888888886 \n",
      "\t -- val loss = 1.6295063315877987, val accuracy = 0.4442 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.00it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3495088909364465, train accuracy = 0.5331333333333333 \n",
      "\t -- val loss = 1.5304931974693823, val accuracy = 0.4658 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.78it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2759361081530358, train accuracy = 0.5621777777777778 \n",
      "\t -- val loss = 1.4761888628477202, val accuracy = 0.4906 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.32it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2329634349572425, train accuracy = 0.5790888888888889 \n",
      "\t -- val loss = 1.455729904029365, val accuracy = 0.4992 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.49it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8871276854645271, train accuracy = 0.4368888888888889 \n",
      "\t -- val loss = 1.9158545056792309, val accuracy = 0.4212 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.94it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.8175493663101001, train accuracy = 0.42015555555555556 \n",
      "\t -- val loss = 1.8525984843308836, val accuracy = 0.4058 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.99it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.8060027812110329, train accuracy = 0.43004444444444445 \n",
      "\t -- val loss = 1.8439525776524943, val accuracy = 0.412 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.08it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.859309983055473, train accuracy = 0.40357777777777776 \n",
      "\t -- val loss = 1.8918468167400333, val accuracy = 0.3932 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.80it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.8437020957285255, train accuracy = 0.40744444444444444 \n",
      "\t -- val loss = 1.8853222178984836, val accuracy = 0.39 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.66it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.801729905083752, train accuracy = 0.438 \n",
      "\t -- val loss = 1.8403259221057522, val accuracy = 0.4174 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.76it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.7729079389850735, train accuracy = 0.45264444444444446 \n",
      "\t -- val loss = 1.81176018201226, val accuracy = 0.4326 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.21it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.7577171990887943, train accuracy = 0.4604 \n",
      "\t -- val loss = 1.7979955382016657, val accuracy = 0.4398 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.49it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5235867833655372, train accuracy = 0.47213333333333335 \n",
      "\t -- val loss = 1.5823082635425412, val accuracy = 0.44 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.03it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4793436127186106, train accuracy = 0.483 \n",
      "\t -- val loss = 1.5747425983556536, val accuracy = 0.4498 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.64it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4251006833358104, train accuracy = 0.5018666666666667 \n",
      "\t -- val loss = 1.5514088243894328, val accuracy = 0.4566 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.68it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.492643604383825, train accuracy = 0.47626666666666667 \n",
      "\t -- val loss = 1.637851312455122, val accuracy = 0.4256 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.95it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4456268058996822, train accuracy = 0.49566666666666664 \n",
      "\t -- val loss = 1.607936709416056, val accuracy = 0.4406 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.95it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.344329580025219, train accuracy = 0.5344 \n",
      "\t -- val loss = 1.5229325558558644, val accuracy = 0.4678 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.47it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2774836321304504, train accuracy = 0.5600888888888889 \n",
      "\t -- val loss = 1.4780976748756782, val accuracy = 0.4878 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.25it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2338085957134328, train accuracy = 0.5764888888888889 \n",
      "\t -- val loss = 1.4601657472749658, val accuracy = 0.497 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.20it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8360107166254842, train accuracy = 0.46464444444444447 \n",
      "\t -- val loss = 1.8827142974019062, val accuracy = 0.4442 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.13it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.7112153963720924, train accuracy = 0.4633777777777778 \n",
      "\t -- val loss = 1.7772576829466147, val accuracy = 0.4394 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.21it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.6317743977902117, train accuracy = 0.4794 \n",
      "\t -- val loss = 1.7168765796294387, val accuracy = 0.4466 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.67it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6919580678262869, train accuracy = 0.44855555555555554 \n",
      "\t -- val loss = 1.7741225632098574, val accuracy = 0.423 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.63it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.6554024507691727, train accuracy = 0.4636222222222222 \n",
      "\t -- val loss = 1.746225650336837, val accuracy = 0.4324 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.5805422910728129, train accuracy = 0.49722222222222223 \n",
      "\t -- val loss = 1.669112609606574, val accuracy = 0.4616 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.82it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.5366172954981019, train accuracy = 0.5173111111111112 \n",
      "\t -- val loss = 1.6322860573869875, val accuracy = 0.4792 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.51it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.5095083416427109, train accuracy = 0.5292 \n",
      "\t -- val loss = 1.6132017453675345, val accuracy = 0.4862 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.81it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5160254916580462, train accuracy = 0.47128888888888887 \n",
      "\t -- val loss = 1.575811711395227, val accuracy = 0.4412 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.64it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4716183943162027, train accuracy = 0.4792666666666667 \n",
      "\t -- val loss = 1.5636063774443154, val accuracy = 0.4476 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.43it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4132281132092777, train accuracy = 0.5030666666666667 \n",
      "\t -- val loss = 1.5433420384812655, val accuracy = 0.4598 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.26it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4951176988845967, train accuracy = 0.47071111111111114 \n",
      "\t -- val loss = 1.6430670276511272, val accuracy = 0.4304 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.39it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4293291451992665, train accuracy = 0.4958222222222222 \n",
      "\t -- val loss = 1.5946429168555931, val accuracy = 0.4484 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.76it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3324921084600116, train accuracy = 0.5337111111111111 \n",
      "\t -- val loss = 1.5115356309132657, val accuracy = 0.468 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.26it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2647506753392586, train accuracy = 0.5586888888888889 \n",
      "\t -- val loss = 1.4749317141905414, val accuracy = 0.4864 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.66it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2179409608552452, train accuracy = 0.5774444444444444 \n",
      "\t -- val loss = 1.450217552516051, val accuracy = 0.4934 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.82it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5143489371382235, train accuracy = 0.4704 \n",
      "\t -- val loss = 1.5735062523929368, val accuracy = 0.4414 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.28it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.471874664244053, train accuracy = 0.48013333333333336 \n",
      "\t -- val loss = 1.5666303628949034, val accuracy = 0.4518 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.43it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.422307048049217, train accuracy = 0.4985111111111111 \n",
      "\t -- val loss = 1.5521974498622095, val accuracy = 0.456 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.43it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4941154456311054, train accuracy = 0.4711111111111111 \n",
      "\t -- val loss = 1.6387457720215952, val accuracy = 0.4332 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.42it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.424026840605913, train accuracy = 0.4965777777777778 \n",
      "\t -- val loss = 1.5915524329512063, val accuracy = 0.4418 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.20it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3280691241288716, train accuracy = 0.5352222222222223 \n",
      "\t -- val loss = 1.5095133836642232, val accuracy = 0.4712 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.82it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2614917424209522, train accuracy = 0.5588 \n",
      "\t -- val loss = 1.4669310792137582, val accuracy = 0.4806 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.65it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.216333832067604, train accuracy = 0.5772666666666667 \n",
      "\t -- val loss = 1.4486405784475638, val accuracy = 0.4958 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 68.30it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5139180165673176, train accuracy = 0.47002222222222223 \n",
      "\t -- val loss = 1.5779040189232691, val accuracy = 0.441 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.14it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.47394658383553, train accuracy = 0.4789777777777778 \n",
      "\t -- val loss = 1.5707674725109066, val accuracy = 0.4468 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.78it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.41694969458223, train accuracy = 0.5015111111111111 \n",
      "\t -- val loss = 1.5443269633193117, val accuracy = 0.4576 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.80it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4831255504519734, train accuracy = 0.476 \n",
      "\t -- val loss = 1.6313416992423861, val accuracy = 0.4356 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.76it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4230606277762265, train accuracy = 0.4987333333333333 \n",
      "\t -- val loss = 1.5833915574824515, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.37it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3338545612988368, train accuracy = 0.5328666666666667 \n",
      "\t -- val loss = 1.5139333388438463, val accuracy = 0.47 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.53it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2647421220114772, train accuracy = 0.5571333333333334 \n",
      "\t -- val loss = 1.4627216367291582, val accuracy = 0.4808 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.89it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2185998753900689, train accuracy = 0.5762666666666667 \n",
      "\t -- val loss = 1.4471786547743979, val accuracy = 0.4996 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.40it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6910862459300464, train accuracy = 0.4684888888888889 \n",
      "\t -- val loss = 1.743401379679134, val accuracy = 0.4414 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.16it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.6333191329978498, train accuracy = 0.47344444444444445 \n",
      "\t -- val loss = 1.7189221731852475, val accuracy = 0.4436 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5637824495753496, train accuracy = 0.49704444444444446 \n",
      "\t -- val loss = 1.6742748415951443, val accuracy = 0.4586 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.32it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6216797708723685, train accuracy = 0.46844444444444444 \n",
      "\t -- val loss = 1.737191023261651, val accuracy = 0.4308 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.61it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5602512661442545, train accuracy = 0.4926888888888889 \n",
      "\t -- val loss = 1.6930524802928235, val accuracy = 0.449 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.91it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4906117208689234, train accuracy = 0.5184222222222222 \n",
      "\t -- val loss = 1.626661086377259, val accuracy = 0.4658 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.88it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.426162108141781, train accuracy = 0.545 \n",
      "\t -- val loss = 1.573981435180466, val accuracy = 0.4912 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.00it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.3900284273643497, train accuracy = 0.5583555555555556 \n",
      "\t -- val loss = 1.5470320547113334, val accuracy = 0.4976 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5855761945060716, train accuracy = 0.46928888888888887 \n",
      "\t -- val loss = 1.6405324300377029, val accuracy = 0.445 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.33it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.545542625537593, train accuracy = 0.4777777777777778 \n",
      "\t -- val loss = 1.636823611100887, val accuracy = 0.4464 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.58it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4891092421179348, train accuracy = 0.4999111111111111 \n",
      "\t -- val loss = 1.6108957648597158, val accuracy = 0.4572 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.39it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5770942397983496, train accuracy = 0.4681777777777778 \n",
      "\t -- val loss = 1.7111269351820435, val accuracy = 0.436 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.73it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5107290012755155, train accuracy = 0.4915333333333333 \n",
      "\t -- val loss = 1.654611966479309, val accuracy = 0.4486 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.85it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.410761010840228, train accuracy = 0.5289111111111111 \n",
      "\t -- val loss = 1.5660907968535422, val accuracy = 0.4722 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.30it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3465930417494643, train accuracy = 0.5562444444444444 \n",
      "\t -- val loss = 1.5232363638877051, val accuracy = 0.4856 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.85it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.3101766456349115, train accuracy = 0.5728666666666666 \n",
      "\t -- val loss = 1.504977497948108, val accuracy = 0.4968 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.37it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.514888789516865, train accuracy = 0.4719555555555556 \n",
      "\t -- val loss = 1.576035405894038, val accuracy = 0.4438 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.02it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.478903577649467, train accuracy = 0.4778222222222222 \n",
      "\t -- val loss = 1.575128433351292, val accuracy = 0.4414 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.58it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4191098225682572, train accuracy = 0.5012666666666666 \n",
      "\t -- val loss = 1.5481841541471952, val accuracy = 0.4558 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.02it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4951001536395527, train accuracy = 0.47431111111111113 \n",
      "\t -- val loss = 1.6430269899865202, val accuracy = 0.432 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.05it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.434229023600343, train accuracy = 0.4951777777777778 \n",
      "\t -- val loss = 1.5987014030237179, val accuracy = 0.4452 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3440334769041433, train accuracy = 0.5300222222222222 \n",
      "\t -- val loss = 1.5247112145002932, val accuracy = 0.4666 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.23it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2706452292965527, train accuracy = 0.5585777777777777 \n",
      "\t -- val loss = 1.4751895976514864, val accuracy = 0.4886 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.45it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2260299438411888, train accuracy = 0.5744222222222222 \n",
      "\t -- val loss = 1.4587477585564566, val accuracy = 0.4946 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.31it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5096252721059062, train accuracy = 0.47244444444444444 \n",
      "\t -- val loss = 1.5673252353161098, val accuracy = 0.4468 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.67it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4635761472262567, train accuracy = 0.4818 \n",
      "\t -- val loss = 1.55565109354034, val accuracy = 0.4494 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4122559967155353, train accuracy = 0.5028222222222222 \n",
      "\t -- val loss = 1.5398076065676747, val accuracy = 0.4496 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.69it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4753036265326773, train accuracy = 0.4802222222222222 \n",
      "\t -- val loss = 1.619970896063728, val accuracy = 0.4352 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.77it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4402849615425788, train accuracy = 0.49166666666666664 \n",
      "\t -- val loss = 1.6000238591047282, val accuracy = 0.4428 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.49it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.339125192957746, train accuracy = 0.5290444444444444 \n",
      "\t -- val loss = 1.5201515758528572, val accuracy = 0.4692 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.89it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2600780403027165, train accuracy = 0.5582888888888888 \n",
      "\t -- val loss = 1.4628213928108293, val accuracy = 0.4834 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.12it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2188583755858216, train accuracy = 0.5756666666666667 \n",
      "\t -- val loss = 1.4468291365556252, val accuracy = 0.4968 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.50it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.523534018587236, train accuracy = 0.4717777777777778 \n",
      "\t -- val loss = 1.5857827578179342, val accuracy = 0.4406 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.18it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4787964757841483, train accuracy = 0.48006666666666664 \n",
      "\t -- val loss = 1.5756593119265947, val accuracy = 0.4474 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.18it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4273649012989442, train accuracy = 0.5004222222222222 \n",
      "\t -- val loss = 1.5527779172436416, val accuracy = 0.4546 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.05it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.506122959244383, train accuracy = 0.4728 \n",
      "\t -- val loss = 1.6523443409710505, val accuracy = 0.4318 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.31it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.44045785316507, train accuracy = 0.49646666666666667 \n",
      "\t -- val loss = 1.6023047317271104, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.24it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3467045918992653, train accuracy = 0.5324888888888889 \n",
      "\t -- val loss = 1.5252324256479972, val accuracy = 0.4694 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.70it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2750036173747623, train accuracy = 0.5594 \n",
      "\t -- val loss = 1.4766574320223134, val accuracy = 0.4912 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.06it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.230363265410276, train accuracy = 0.5768222222222222 \n",
      "\t -- val loss = 1.4568785005255043, val accuracy = 0.5052 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.69it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5111236031280737, train accuracy = 0.47171111111111114 \n",
      "\t -- val loss = 1.5706331079244336, val accuracy = 0.4404 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.07it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4619290816304338, train accuracy = 0.48104444444444444 \n",
      "\t -- val loss = 1.5539038502145295, val accuracy = 0.4508 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.63it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4141300940465649, train accuracy = 0.49964444444444445 \n",
      "\t -- val loss = 1.5427375245691644, val accuracy = 0.4506 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.30it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.48898396965909, train accuracy = 0.47235555555555553 \n",
      "\t -- val loss = 1.6387322015789336, val accuracy = 0.4296 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.84it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4283116645915508, train accuracy = 0.4937111111111111 \n",
      "\t -- val loss = 1.5945612851566695, val accuracy = 0.4452 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.08it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3301546125064752, train accuracy = 0.5314 \n",
      "\t -- val loss = 1.5115903619874993, val accuracy = 0.4702 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.78it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2595876717898857, train accuracy = 0.5589111111111111 \n",
      "\t -- val loss = 1.461577339837093, val accuracy = 0.4822 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.49it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2144119203518664, train accuracy = 0.5772444444444444 \n",
      "\t -- val loss = 1.4445778155843922, val accuracy = 0.4898 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.13it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5290975827133002, train accuracy = 0.4736444444444444 \n",
      "\t -- val loss = 1.5891684228644474, val accuracy = 0.441 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.33it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4958856007896482, train accuracy = 0.47733333333333333 \n",
      "\t -- val loss = 1.5905013516879922, val accuracy = 0.442 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.72it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4372649716322106, train accuracy = 0.5013111111111112 \n",
      "\t -- val loss = 1.5629300436706182, val accuracy = 0.4542 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.59it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4967119326123528, train accuracy = 0.48106666666666664 \n",
      "\t -- val loss = 1.6427214222767046, val accuracy = 0.4372 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.37it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4692553840312865, train accuracy = 0.49002222222222225 \n",
      "\t -- val loss = 1.6287242181394475, val accuracy = 0.4382 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.21it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3580644136085316, train accuracy = 0.5326222222222222 \n",
      "\t -- val loss = 1.5350883435827307, val accuracy = 0.4656 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.46it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2876055686673091, train accuracy = 0.5604444444444444 \n",
      "\t -- val loss = 1.4887764745773224, val accuracy = 0.4784 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.49it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.249768558997854, train accuracy = 0.5737333333333333 \n",
      "\t -- val loss = 1.4709567351598456, val accuracy = 0.494 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.99it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5146190956974568, train accuracy = 0.47095555555555557 \n",
      "\t -- val loss = 1.5736758254294592, val accuracy = 0.4436 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.85it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4712183053804238, train accuracy = 0.47957777777777777 \n",
      "\t -- val loss = 1.5661938542680347, val accuracy = 0.4472 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.18it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4184160942698378, train accuracy = 0.5001333333333333 \n",
      "\t -- val loss = 1.5501529353834091, val accuracy = 0.4506 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.36it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4953499077124597, train accuracy = 0.4729555555555556 \n",
      "\t -- val loss = 1.6474720696366183, val accuracy = 0.4338 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.10it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4286367542186902, train accuracy = 0.4959777777777778 \n",
      "\t -- val loss = 1.5938254922886084, val accuracy = 0.4474 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.65it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3273343355018419, train accuracy = 0.5335333333333333 \n",
      "\t -- val loss = 1.5172620248030069, val accuracy = 0.4666 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.70it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2651481961692976, train accuracy = 0.5577777777777778 \n",
      "\t -- val loss = 1.4722031788783054, val accuracy = 0.4808 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.60it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2182848373133937, train accuracy = 0.5759555555555556 \n",
      "\t -- val loss = 1.4550394743290753, val accuracy = 0.4898 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.92it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8974883292560984, train accuracy = 0.45437777777777777 \n",
      "\t -- val loss = 1.9368397711763763, val accuracy = 0.4326 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.86it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.7463499625838643, train accuracy = 0.44522222222222224 \n",
      "\t -- val loss = 1.793698187010411, val accuracy = 0.4274 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.82it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.7104123695371025, train accuracy = 0.45555555555555555 \n",
      "\t -- val loss = 1.767616010544614, val accuracy = 0.428 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.37it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.7768406479059013, train accuracy = 0.4282666666666667 \n",
      "\t -- val loss = 1.8300586529045741, val accuracy = 0.4112 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.85it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.754742751698827, train accuracy = 0.43575555555555556 \n",
      "\t -- val loss = 1.8193114776409884, val accuracy = 0.409 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.39it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.7045409982870903, train accuracy = 0.4613777777777778 \n",
      "\t -- val loss = 1.7594650878298737, val accuracy = 0.4372 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.11it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.6644911387339756, train accuracy = 0.48373333333333335 \n",
      "\t -- val loss = 1.7247218865852914, val accuracy = 0.459 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.47it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.6440844265699506, train accuracy = 0.49373333333333336 \n",
      "\t -- val loss = 1.7078024510931331, val accuracy = 0.4642 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.32it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5140005993089922, train accuracy = 0.4700666666666667 \n",
      "\t -- val loss = 1.5780023673345478, val accuracy = 0.441 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.78it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4730055415454446, train accuracy = 0.4802444444444444 \n",
      "\t -- val loss = 1.5675064597740958, val accuracy = 0.45 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.78it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4142833135462873, train accuracy = 0.5020444444444444 \n",
      "\t -- val loss = 1.541238644085451, val accuracy = 0.4568 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.41it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4864250112929047, train accuracy = 0.47502222222222223 \n",
      "\t -- val loss = 1.634745032594986, val accuracy = 0.4324 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.75it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4141302982773871, train accuracy = 0.5011111111111111 \n",
      "\t -- val loss = 1.5787725705460804, val accuracy = 0.448 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.83it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3336570458534132, train accuracy = 0.5335333333333333 \n",
      "\t -- val loss = 1.5092061627462017, val accuracy = 0.4732 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.50it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2676285458036045, train accuracy = 0.5553333333333333 \n",
      "\t -- val loss = 1.4661750482643168, val accuracy = 0.4844 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.95it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2203770995681966, train accuracy = 0.5748888888888889 \n",
      "\t -- val loss = 1.4431905682576003, val accuracy = 0.4994 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.27it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.556896472793684, train accuracy = 0.47104444444444443 \n",
      "\t -- val loss = 1.6144281143081933, val accuracy = 0.4418 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.70it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5155472132109686, train accuracy = 0.4781111111111111 \n",
      "\t -- val loss = 1.6123205695774687, val accuracy = 0.4436 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.03it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4631770178646009, train accuracy = 0.5001111111111111 \n",
      "\t -- val loss = 1.590372010616483, val accuracy = 0.453 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.57it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5133554628147894, train accuracy = 0.4814222222222222 \n",
      "\t -- val loss = 1.6536194265939004, val accuracy = 0.4416 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 77.46it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4845592694540326, train accuracy = 0.4928888888888889 \n",
      "\t -- val loss = 1.638094681761382, val accuracy = 0.444 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 78.00it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3891776224404908, train accuracy = 0.5313333333333333 \n",
      "\t -- val loss = 1.5551304562078003, val accuracy = 0.4664 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.46it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.31744817705437, train accuracy = 0.5572444444444444 \n",
      "\t -- val loss = 1.5096938537508517, val accuracy = 0.4818 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.95it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2789200899406974, train accuracy = 0.5757777777777778 \n",
      "\t -- val loss = 1.4873166218598741, val accuracy = 0.4964 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.18it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5440757695647636, train accuracy = 0.47055555555555556 \n",
      "\t -- val loss = 1.601946474557181, val accuracy = 0.4438 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.76it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5042602611279687, train accuracy = 0.4789555555555556 \n",
      "\t -- val loss = 1.5988746592135354, val accuracy = 0.4472 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.87it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.449017930205746, train accuracy = 0.5001777777777778 \n",
      "\t -- val loss = 1.5775074150681867, val accuracy = 0.459 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.54it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.512762341854923, train accuracy = 0.48002222222222224 \n",
      "\t -- val loss = 1.6516056475088787, val accuracy = 0.4346 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.44it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4708126440605744, train accuracy = 0.4926888888888889 \n",
      "\t -- val loss = 1.6222027296303982, val accuracy = 0.4472 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.22it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3727988916452427, train accuracy = 0.5300444444444444 \n",
      "\t -- val loss = 1.5412901535933838, val accuracy = 0.4704 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.85it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3018998395557742, train accuracy = 0.5595555555555556 \n",
      "\t -- val loss = 1.4886083054252066, val accuracy = 0.4866 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.78it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2621450024678071, train accuracy = 0.5758666666666666 \n",
      "\t -- val loss = 1.4723113772239633, val accuracy = 0.4992 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.17it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5525096296398149, train accuracy = 0.47153333333333336 \n",
      "\t -- val loss = 1.6154119636952013, val accuracy = 0.4426 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.05it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5124007519457703, train accuracy = 0.4772 \n",
      "\t -- val loss = 1.610001888769678, val accuracy = 0.4432 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.04it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4617225122910238, train accuracy = 0.49748888888888887 \n",
      "\t -- val loss = 1.584923025322632, val accuracy = 0.4526 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.38it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5247280729720512, train accuracy = 0.47833333333333333 \n",
      "\t -- val loss = 1.6590152146355757, val accuracy = 0.4376 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.56it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4707093260200286, train accuracy = 0.4961333333333333 \n",
      "\t -- val loss = 1.6277858070016307, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.17it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3835798696942085, train accuracy = 0.5307555555555555 \n",
      "\t -- val loss = 1.5520866429404194, val accuracy = 0.464 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.89it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.316743137433474, train accuracy = 0.5579333333333333 \n",
      "\t -- val loss = 1.4996255263460805, val accuracy = 0.4866 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.17it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2755366243662727, train accuracy = 0.5738666666666666 \n",
      "\t -- val loss = 1.4810986958478056, val accuracy = 0.4938 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.13it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5109249129897888, train accuracy = 0.47226666666666667 \n",
      "\t -- val loss = 1.567276510021002, val accuracy = 0.446 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.32it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4631229936149839, train accuracy = 0.48084444444444446 \n",
      "\t -- val loss = 1.5543536256374804, val accuracy = 0.4486 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.36it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4149504339588528, train accuracy = 0.5006888888888889 \n",
      "\t -- val loss = 1.5411595196311774, val accuracy = 0.4536 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.38it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4669777708767116, train accuracy = 0.4827111111111111 \n",
      "\t -- val loss = 1.611908731611195, val accuracy = 0.4366 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.79it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4238399155154617, train accuracy = 0.4978888888888889 \n",
      "\t -- val loss = 1.5820913427808196, val accuracy = 0.4512 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.71it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3344422374220952, train accuracy = 0.5311555555555556 \n",
      "\t -- val loss = 1.5145268444327604, val accuracy = 0.4748 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.93it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2595746495651636, train accuracy = 0.5605111111111111 \n",
      "\t -- val loss = 1.4626299084941372, val accuracy = 0.481 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.83it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2185872992766111, train accuracy = 0.5755111111111111 \n",
      "\t -- val loss = 1.4452591269137633, val accuracy = 0.4972 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.51it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6406662483981127, train accuracy = 0.4694888888888889 \n",
      "\t -- val loss = 1.6972485762946228, val accuracy = 0.4426 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.12it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5913760605906673, train accuracy = 0.47731111111111113 \n",
      "\t -- val loss = 1.6793040058550712, val accuracy = 0.446 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.69it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5315591195387765, train accuracy = 0.49951111111111113 \n",
      "\t -- val loss = 1.6444885669587181, val accuracy = 0.4542 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.37it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5889512685964922, train accuracy = 0.4745333333333333 \n",
      "\t -- val loss = 1.7170953842950507, val accuracy = 0.4358 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.36it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5649128541899469, train accuracy = 0.4836666666666667 \n",
      "\t -- val loss = 1.7000269884436956, val accuracy = 0.4412 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.42it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4567990190420195, train accuracy = 0.5245333333333333 \n",
      "\t -- val loss = 1.601469841360328, val accuracy = 0.4722 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.21it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3930649036437925, train accuracy = 0.5510666666666667 \n",
      "\t -- val loss = 1.552050502864062, val accuracy = 0.4862 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.75it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.3567238542742686, train accuracy = 0.5645777777777777 \n",
      "\t -- val loss = 1.531485817524036, val accuracy = 0.5014 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.65it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8603170558897153, train accuracy = 0.46413333333333334 \n",
      "\t -- val loss = 1.9089202681226578, val accuracy = 0.4402 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.18it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.7198709916250114, train accuracy = 0.4616222222222222 \n",
      "\t -- val loss = 1.782624102375521, val accuracy = 0.4354 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.84it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.647462432257831, train accuracy = 0.47302222222222223 \n",
      "\t -- val loss = 1.721855451429825, val accuracy = 0.4392 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.28it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6980188982500917, train accuracy = 0.4478888888888889 \n",
      "\t -- val loss = 1.7738336570551965, val accuracy = 0.4256 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.04it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.678253030075922, train accuracy = 0.4582222222222222 \n",
      "\t -- val loss = 1.7581184542988564, val accuracy = 0.4276 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.53it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.609708545102425, train accuracy = 0.48891111111111113 \n",
      "\t -- val loss = 1.6855519031672561, val accuracy = 0.4524 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.92it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.5616359139359286, train accuracy = 0.5125777777777778 \n",
      "\t -- val loss = 1.6467970383965724, val accuracy = 0.4752 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.82it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.5425278520671208, train accuracy = 0.5222 \n",
      "\t -- val loss = 1.631044053278678, val accuracy = 0.4842 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.55it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5297704133987355, train accuracy = 0.4693111111111111 \n",
      "\t -- val loss = 1.587732633633897, val accuracy = 0.442 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.98it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4876212716663537, train accuracy = 0.481 \n",
      "\t -- val loss = 1.5836992766170164, val accuracy = 0.4492 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.71it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.425621837368263, train accuracy = 0.5056 \n",
      "\t -- val loss = 1.555802208563241, val accuracy = 0.458 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.39it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5035817881439109, train accuracy = 0.47773333333333334 \n",
      "\t -- val loss = 1.6486925861811714, val accuracy = 0.4362 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.79it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4507009914081062, train accuracy = 0.49675555555555556 \n",
      "\t -- val loss = 1.615820218097327, val accuracy = 0.4488 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.99it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.349799935049968, train accuracy = 0.5347555555555555 \n",
      "\t -- val loss = 1.5290694969497842, val accuracy = 0.4688 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.76it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2876119338668248, train accuracy = 0.5585555555555556 \n",
      "\t -- val loss = 1.482768637351544, val accuracy = 0.4872 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.58it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2440511199326265, train accuracy = 0.5746444444444444 \n",
      "\t -- val loss = 1.4652268581239005, val accuracy = 0.497 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.20it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.7362948140543062, train accuracy = 0.4666222222222222 \n",
      "\t -- val loss = 1.7878054255507667, val accuracy = 0.4432 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.64it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.65857003817247, train accuracy = 0.47426666666666667 \n",
      "\t -- val loss = 1.7378544074673739, val accuracy = 0.448 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.00it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5872745534249146, train accuracy = 0.49104444444444445 \n",
      "\t -- val loss = 1.6894977479698408, val accuracy = 0.4532 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.60it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6726877515790333, train accuracy = 0.45315555555555553 \n",
      "\t -- val loss = 1.77934173241583, val accuracy = 0.4276 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.62it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.6068465645690144, train accuracy = 0.4766666666666667 \n",
      "\t -- val loss = 1.724614623402982, val accuracy = 0.4412 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.26it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.5088331508992148, train accuracy = 0.5144 \n",
      "\t -- val loss = 1.6287610782571944, val accuracy = 0.4678 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.44it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.4543500699379608, train accuracy = 0.5364888888888889 \n",
      "\t -- val loss = 1.5884924560253189, val accuracy = 0.488 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.78it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.4204160527194079, train accuracy = 0.5518444444444445 \n",
      "\t -- val loss = 1.5612994266480846, val accuracy = 0.4994 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.71it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.563111875365658, train accuracy = 0.4687777777777778 \n",
      "\t -- val loss = 1.61876568525953, val accuracy = 0.4422 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.45it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5244715398844908, train accuracy = 0.47675555555555554 \n",
      "\t -- val loss = 1.6161284553309578, val accuracy = 0.4474 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.67it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4678118310334118, train accuracy = 0.498 \n",
      "\t -- val loss = 1.5937874377850718, val accuracy = 0.4556 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.36it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5298074350505595, train accuracy = 0.4778222222222222 \n",
      "\t -- val loss = 1.666847071212059, val accuracy = 0.4426 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.13it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4736423309810087, train accuracy = 0.5004666666666666 \n",
      "\t -- val loss = 1.6330701274259056, val accuracy = 0.4474 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.84it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3893483569408875, train accuracy = 0.5319333333333334 \n",
      "\t -- val loss = 1.5608554937416657, val accuracy = 0.4712 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.42it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3206276311064435, train accuracy = 0.5570666666666667 \n",
      "\t -- val loss = 1.5127673034619975, val accuracy = 0.4854 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.22it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2809898745927835, train accuracy = 0.5765333333333333 \n",
      "\t -- val loss = 1.4929613262467973, val accuracy = 0.4922 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.77it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5558228728307866, train accuracy = 0.47171111111111114 \n",
      "\t -- val loss = 1.6127308532443192, val accuracy = 0.4476 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.94it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.515541856396308, train accuracy = 0.4801111111111111 \n",
      "\t -- val loss = 1.6062858479347288, val accuracy = 0.4466 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.71it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4662673260236638, train accuracy = 0.4960222222222222 \n",
      "\t -- val loss = 1.5942945917666576, val accuracy = 0.4518 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.21it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5585087916174303, train accuracy = 0.46613333333333334 \n",
      "\t -- val loss = 1.7053406410215548, val accuracy = 0.4314 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.97it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.492672123900364, train accuracy = 0.48993333333333333 \n",
      "\t -- val loss = 1.6507576288044519, val accuracy = 0.4378 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.75it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3893254306312988, train accuracy = 0.5302444444444444 \n",
      "\t -- val loss = 1.5590665907422605, val accuracy = 0.4748 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.19it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.32095777164187, train accuracy = 0.5555333333333333 \n",
      "\t -- val loss = 1.5138655746668597, val accuracy = 0.4854 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.78it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2805148205380685, train accuracy = 0.5750222222222222 \n",
      "\t -- val loss = 1.4953040355690272, val accuracy = 0.497 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.61it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8863300276241906, train accuracy = 0.4385777777777778 \n",
      "\t -- val loss = 1.9156428504817238, val accuracy = 0.4218 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.75it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.8138335025675674, train accuracy = 0.4212444444444444 \n",
      "\t -- val loss = 1.848519689933804, val accuracy = 0.4088 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.88it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.7999787927044373, train accuracy = 0.4318 \n",
      "\t -- val loss = 1.8406606383588553, val accuracy = 0.4126 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.63it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.8546088302901222, train accuracy = 0.4054888888888889 \n",
      "\t -- val loss = 1.8900891390292685, val accuracy = 0.391 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 70.56it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.837286350696866, train accuracy = 0.4116222222222222 \n",
      "\t -- val loss = 1.8816257526885078, val accuracy = 0.3926 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.27it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.7964634408846378, train accuracy = 0.43877777777777777 \n",
      "\t -- val loss = 1.837581714743251, val accuracy = 0.421 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.65it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.7652383592640468, train accuracy = 0.45644444444444443 \n",
      "\t -- val loss = 1.8065363156519623, val accuracy = 0.4358 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 75.13it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.7504207054918113, train accuracy = 0.4624666666666667 \n",
      "\t -- val loss = 1.7934554761814443, val accuracy = 0.444 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.47it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6214698893759438, train accuracy = 0.4686222222222222 \n",
      "\t -- val loss = 1.6785009697749373, val accuracy = 0.4416 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 69.67it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5759946944464323, train accuracy = 0.4797777777777778 \n",
      "\t -- val loss = 1.666360068467974, val accuracy = 0.4456 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 70.70it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5238055944073412, train accuracy = 0.49893333333333334 \n",
      "\t -- val loss = 1.6436865448034383, val accuracy = 0.4536 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.61it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5830693555027253, train accuracy = 0.4762888888888889 \n",
      "\t -- val loss = 1.7134854626181337, val accuracy = 0.4322 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.76it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5416566513807046, train accuracy = 0.4895777777777778 \n",
      "\t -- val loss = 1.682302395967935, val accuracy = 0.4446 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 74.45it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4419403570177995, train accuracy = 0.5281555555555556 \n",
      "\t -- val loss = 1.5913828397855336, val accuracy = 0.4738 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.81it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3830036903788243, train accuracy = 0.5515555555555556 \n",
      "\t -- val loss = 1.5479731273447712, val accuracy = 0.4832 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 71.48it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.3454754352795417, train accuracy = 0.5661111111111111 \n",
      "\t -- val loss = 1.5289390885487388, val accuracy = 0.5018 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.24it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5145980153135101, train accuracy = 0.47146666666666665 \n",
      "\t -- val loss = 1.5736830859743716, val accuracy = 0.4442 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 69.74it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.473883737577499, train accuracy = 0.47733333333333333 \n",
      "\t -- val loss = 1.5688217628607033, val accuracy = 0.449 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 72.68it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4225052068384094, train accuracy = 0.49664444444444444 \n",
      "\t -- val loss = 1.5462730491436796, val accuracy = 0.4544 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 76.34it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5058337520190737, train accuracy = 0.46682222222222225 \n",
      "\t -- val loss = 1.6454447843822892, val accuracy = 0.4284 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 77.53it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4286607733227275, train accuracy = 0.49535555555555555 \n",
      "\t -- val loss = 1.582960351019819, val accuracy = 0.442 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 76.17it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3336642171143407, train accuracy = 0.5324888888888889 \n",
      "\t -- val loss = 1.5081614311383276, val accuracy = 0.4702 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 76.69it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2576864997612436, train accuracy = 0.5618444444444445 \n",
      "\t -- val loss = 1.4563947352035054, val accuracy = 0.4876 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 86.66it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2167435697148572, train accuracy = 0.5770666666666666 \n",
      "\t -- val loss = 1.4426664537231144, val accuracy = 0.4924 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.75it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6131872152198619, train accuracy = 0.4680888888888889 \n",
      "\t -- val loss = 1.6704555268491954, val accuracy = 0.4366 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.53it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5667277571569542, train accuracy = 0.47735555555555553 \n",
      "\t -- val loss = 1.657014845096486, val accuracy = 0.443 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.37it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5159640664833935, train accuracy = 0.4967111111111111 \n",
      "\t -- val loss = 1.6348308723459624, val accuracy = 0.4496 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.13it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6008199824657612, train accuracy = 0.46144444444444443 \n",
      "\t -- val loss = 1.7307073980631043, val accuracy = 0.4284 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.26it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5404427153828655, train accuracy = 0.48573333333333335 \n",
      "\t -- val loss = 1.6846923082805403, val accuracy = 0.4448 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.40it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4355753091701142, train accuracy = 0.5292666666666667 \n",
      "\t -- val loss = 1.5899032547725525, val accuracy = 0.4682 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.45it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3753675400370013, train accuracy = 0.5526444444444445 \n",
      "\t -- val loss = 1.5447918611005294, val accuracy = 0.4904 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.89it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.335895344234972, train accuracy = 0.5703333333333334 \n",
      "\t -- val loss = 1.5248443806554692, val accuracy = 0.5056 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.54it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5088619766346472, train accuracy = 0.4719555555555556 \n",
      "\t -- val loss = 1.5693609544883198, val accuracy = 0.439 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.40it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4640574522286112, train accuracy = 0.4802222222222222 \n",
      "\t -- val loss = 1.5562131689691914, val accuracy = 0.4502 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.20it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4144183277422158, train accuracy = 0.5 \n",
      "\t -- val loss = 1.540077662582838, val accuracy = 0.4544 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.45it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4859908506761974, train accuracy = 0.47171111111111114 \n",
      "\t -- val loss = 1.6319759690110507, val accuracy = 0.4288 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.83it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4313523304103233, train accuracy = 0.4933777777777778 \n",
      "\t -- val loss = 1.5935124484872987, val accuracy = 0.4408 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.88it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3276217544721234, train accuracy = 0.5307777777777778 \n",
      "\t -- val loss = 1.5020687351800817, val accuracy = 0.4738 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.02it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.258690186585918, train accuracy = 0.5608222222222222 \n",
      "\t -- val loss = 1.464007805562655, val accuracy = 0.4882 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.85it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2149693740243919, train accuracy = 0.5766222222222223 \n",
      "\t -- val loss = 1.4476813147244807, val accuracy = 0.4968 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.90it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.893981420838373, train accuracy = 0.44606666666666667 \n",
      "\t -- val loss = 1.9276438383472803, val accuracy = 0.4312 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.48it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.777465402945786, train accuracy = 0.4333111111111111 \n",
      "\t -- val loss = 1.817889997876954, val accuracy = 0.421 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.61it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.7545194166235984, train accuracy = 0.4450222222222222 \n",
      "\t -- val loss = 1.8024456047546675, val accuracy = 0.4242 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.10it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.8120157301970534, train accuracy = 0.41773333333333335 \n",
      "\t -- val loss = 1.853565707839899, val accuracy = 0.4062 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.79it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.7938904908520334, train accuracy = 0.4238222222222222 \n",
      "\t -- val loss = 1.8470631404346383, val accuracy = 0.4038 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.28it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.7545858537613446, train accuracy = 0.4518 \n",
      "\t -- val loss = 1.8035166286791002, val accuracy = 0.4274 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.33it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.7186906767138563, train accuracy = 0.46942222222222224 \n",
      "\t -- val loss = 1.7671211406995642, val accuracy = 0.4504 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.67it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.7026028900943286, train accuracy = 0.47935555555555553 \n",
      "\t -- val loss = 1.7541926624099087, val accuracy = 0.456 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.58it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5205844729313567, train accuracy = 0.47175555555555554 \n",
      "\t -- val loss = 1.5796248047952515, val accuracy = 0.4422 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.16it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4774331913729015, train accuracy = 0.4806888888888889 \n",
      "\t -- val loss = 1.5708774958270426, val accuracy = 0.45 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.41it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4231332036689075, train accuracy = 0.5006888888888889 \n",
      "\t -- val loss = 1.551175084075929, val accuracy = 0.4582 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.22it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5037189534412918, train accuracy = 0.4728222222222222 \n",
      "\t -- val loss = 1.6535413886574692, val accuracy = 0.4254 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.08it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4346225446787224, train accuracy = 0.49824444444444443 \n",
      "\t -- val loss = 1.5945288596837723, val accuracy = 0.4484 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.49it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3392246390026774, train accuracy = 0.5348888888888889 \n",
      "\t -- val loss = 1.5155501271182368, val accuracy = 0.4732 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.89it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2683078132568595, train accuracy = 0.5614222222222223 \n",
      "\t -- val loss = 1.4715751291339305, val accuracy = 0.4916 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.50it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2260381100595215, train accuracy = 0.5775555555555556 \n",
      "\t -- val loss = 1.45326400303306, val accuracy = 0.4936 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.48it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.514593422933789, train accuracy = 0.46995555555555557 \n",
      "\t -- val loss = 1.574261805970807, val accuracy = 0.4402 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.71it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.477184064401989, train accuracy = 0.4778 \n",
      "\t -- val loss = 1.5730616583467638, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.24it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4269121100222297, train accuracy = 0.4954222222222222 \n",
      "\t -- val loss = 1.5552880475869046, val accuracy = 0.4508 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.27it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5067699990349024, train accuracy = 0.46775555555555554 \n",
      "\t -- val loss = 1.6544837493091948, val accuracy = 0.4286 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.43it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4229472218732397, train accuracy = 0.49995555555555554 \n",
      "\t -- val loss = 1.5916668097574627, val accuracy = 0.4482 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.96it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3323606468237594, train accuracy = 0.5332888888888889 \n",
      "\t -- val loss = 1.5182793219984556, val accuracy = 0.4694 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.25it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.262264310703413, train accuracy = 0.5605333333333333 \n",
      "\t -- val loss = 1.4732441716961195, val accuracy = 0.4836 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.83it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2155790151199644, train accuracy = 0.5767111111111111 \n",
      "\t -- val loss = 1.4577819107990808, val accuracy = 0.4922 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.99it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.530382720170362, train accuracy = 0.47102222222222223 \n",
      "\t -- val loss = 1.5937611916234662, val accuracy = 0.447 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.19it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4865387295191335, train accuracy = 0.4812 \n",
      "\t -- val loss = 1.5877957874256265, val accuracy = 0.449 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.75it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.434529105789161, train accuracy = 0.499 \n",
      "\t -- val loss = 1.5638998594743139, val accuracy = 0.459 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.46it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5173238659327453, train accuracy = 0.4732 \n",
      "\t -- val loss = 1.6631566014079953, val accuracy = 0.431 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.55it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4599262525466383, train accuracy = 0.4931333333333333 \n",
      "\t -- val loss = 1.6248179051917109, val accuracy = 0.441 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.26it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3578468334070128, train accuracy = 0.5278888888888889 \n",
      "\t -- val loss = 1.5300114130469522, val accuracy = 0.468 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.55it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2825990319387874, train accuracy = 0.5594888888888889 \n",
      "\t -- val loss = 1.4869889989153693, val accuracy = 0.4858 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.70it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2400468674102785, train accuracy = 0.5768666666666666 \n",
      "\t -- val loss = 1.4636562334055645, val accuracy = 0.4982 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.05it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5100875814329475, train accuracy = 0.4710888888888889 \n",
      "\t -- val loss = 1.5689741551041483, val accuracy = 0.4408 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.68it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4662751312905125, train accuracy = 0.48057777777777777 \n",
      "\t -- val loss = 1.5591174983775493, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.45it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4171058071377949, train accuracy = 0.49935555555555555 \n",
      "\t -- val loss = 1.548388808803562, val accuracy = 0.458 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.53it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.479693063036994, train accuracy = 0.4762222222222222 \n",
      "\t -- val loss = 1.6279104654166983, val accuracy = 0.4336 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.29it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4271291795975876, train accuracy = 0.49755555555555553 \n",
      "\t -- val loss = 1.5903864257452864, val accuracy = 0.4448 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.68it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3343054840295319, train accuracy = 0.5308444444444445 \n",
      "\t -- val loss = 1.5135450639217014, val accuracy = 0.4688 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.51it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2588946291925323, train accuracy = 0.5595111111111111 \n",
      "\t -- val loss = 1.4592514130264993, val accuracy = 0.49 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.95it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2146405838598626, train accuracy = 0.5764 \n",
      "\t -- val loss = 1.444905010941008, val accuracy = 0.4958 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.96it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.508783168989992, train accuracy = 0.4723777777777778 \n",
      "\t -- val loss = 1.5692963150152301, val accuracy = 0.4388 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.01it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4662677697098143, train accuracy = 0.4803777777777778 \n",
      "\t -- val loss = 1.561038494188304, val accuracy = 0.45 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.37it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4096263372465443, train accuracy = 0.5023111111111112 \n",
      "\t -- val loss = 1.536546241581181, val accuracy = 0.4564 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.34it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.481549176907191, train accuracy = 0.4756 \n",
      "\t -- val loss = 1.6349671195754731, val accuracy = 0.4302 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.09it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4261103606015728, train accuracy = 0.49691111111111114 \n",
      "\t -- val loss = 1.5898332371049737, val accuracy = 0.4458 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.44it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3270091428979245, train accuracy = 0.5324888888888889 \n",
      "\t -- val loss = 1.501860881385782, val accuracy = 0.4692 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.63it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2573423626980904, train accuracy = 0.5600444444444445 \n",
      "\t -- val loss = 1.4588007420644629, val accuracy = 0.4874 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.56it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2152797915825995, train accuracy = 0.5761555555555555 \n",
      "\t -- val loss = 1.4473986454606727, val accuracy = 0.5002 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.61it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5189857571673981, train accuracy = 0.47213333333333335 \n",
      "\t -- val loss = 1.5808301785491925, val accuracy = 0.4424 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.23it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4752068726823113, train accuracy = 0.4790888888888889 \n",
      "\t -- val loss = 1.5715816897071984, val accuracy = 0.4486 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.78it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4155513686226044, train accuracy = 0.5040666666666667 \n",
      "\t -- val loss = 1.5420975307487415, val accuracy = 0.4594 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.13it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5139485635620489, train accuracy = 0.46942222222222224 \n",
      "\t -- val loss = 1.6609722849973951, val accuracy = 0.4324 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.11it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4462087158987476, train accuracy = 0.4920888888888889 \n",
      "\t -- val loss = 1.6106274037452577, val accuracy = 0.4466 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.16it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3402973359561892, train accuracy = 0.5318444444444445 \n",
      "\t -- val loss = 1.5107127967050689, val accuracy = 0.4746 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.89it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2690653308559279, train accuracy = 0.5589111111111111 \n",
      "\t -- val loss = 1.470230208171118, val accuracy = 0.4958 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.71it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2264956934995535, train accuracy = 0.5780222222222222 \n",
      "\t -- val loss = 1.453838528778181, val accuracy = 0.498 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 77.65it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6344999667783107, train accuracy = 0.47215555555555555 \n",
      "\t -- val loss = 1.6937024029374823, val accuracy = 0.4464 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 79.58it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.590661385880948, train accuracy = 0.47646666666666665 \n",
      "\t -- val loss = 1.6808208608204982, val accuracy = 0.444 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.95it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5372810385066833, train accuracy = 0.49548888888888887 \n",
      "\t -- val loss = 1.6505994793266137, val accuracy = 0.4528 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.08it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5850609086583463, train accuracy = 0.4752 \n",
      "\t -- val loss = 1.713693874977991, val accuracy = 0.4322 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.60it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5549629125717725, train accuracy = 0.4878222222222222 \n",
      "\t -- val loss = 1.6892241747200463, val accuracy = 0.4416 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.05it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.454837059237989, train accuracy = 0.5262 \n",
      "\t -- val loss = 1.6005061186992087, val accuracy = 0.4662 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.53it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3897686718177786, train accuracy = 0.5526444444444445 \n",
      "\t -- val loss = 1.5555964985694306, val accuracy = 0.4896 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.57it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.3538620568969473, train accuracy = 0.5676666666666667 \n",
      "\t -- val loss = 1.5334711833791814, val accuracy = 0.5006 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.20it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6348983138248754, train accuracy = 0.47002222222222223 \n",
      "\t -- val loss = 1.6919556171371704, val accuracy = 0.4472 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.63it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5933655329226464, train accuracy = 0.4745333333333333 \n",
      "\t -- val loss = 1.6821423149322374, val accuracy = 0.445 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.14it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5335313162665494, train accuracy = 0.49735555555555555 \n",
      "\t -- val loss = 1.6494296261540324, val accuracy = 0.4592 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.81it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.620406142728559, train accuracy = 0.46357777777777776 \n",
      "\t -- val loss = 1.7462315319162414, val accuracy = 0.4288 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.37it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5641794932688915, train accuracy = 0.4842444444444444 \n",
      "\t -- val loss = 1.6996098084047324, val accuracy = 0.4394 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.07it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4564760969874964, train accuracy = 0.5223111111111111 \n",
      "\t -- val loss = 1.6013701749532123, val accuracy = 0.4648 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.37it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.392078931496111, train accuracy = 0.5498888888888889 \n",
      "\t -- val loss = 1.5547733456653687, val accuracy = 0.4852 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.60it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.355888850362413, train accuracy = 0.5655111111111111 \n",
      "\t -- val loss = 1.5360796923305637, val accuracy = 0.4962 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.97it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5193586122098395, train accuracy = 0.4699111111111111 \n",
      "\t -- val loss = 1.5840295833989013, val accuracy = 0.4356 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.90it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4776637234348753, train accuracy = 0.4785333333333333 \n",
      "\t -- val loss = 1.5729974304275551, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.03it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4188050267959955, train accuracy = 0.5034222222222222 \n",
      "\t -- val loss = 1.541515598273623, val accuracy = 0.462 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.23it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4796048330306077, train accuracy = 0.47906666666666664 \n",
      "\t -- val loss = 1.623306507265193, val accuracy = 0.4376 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.71it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4328842923963576, train accuracy = 0.49793333333333334 \n",
      "\t -- val loss = 1.5879660460637466, val accuracy = 0.4528 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.54it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3426341382926557, train accuracy = 0.5293555555555556 \n",
      "\t -- val loss = 1.5183082805304515, val accuracy = 0.4658 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.38it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2627109766352995, train accuracy = 0.5606 \n",
      "\t -- val loss = 1.460879607072279, val accuracy = 0.4926 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.54it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2182630237697465, train accuracy = 0.5771111111111111 \n",
      "\t -- val loss = 1.4462957327249408, val accuracy = 0.503 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.02it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5397240728079, train accuracy = 0.47084444444444445 \n",
      "\t -- val loss = 1.6037902804400852, val accuracy = 0.4406 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.78it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4998631070636113, train accuracy = 0.476 \n",
      "\t -- val loss = 1.5949364545377516, val accuracy = 0.4406 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.91it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4503328878335924, train accuracy = 0.49704444444444446 \n",
      "\t -- val loss = 1.5781378198051605, val accuracy = 0.4544 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.21it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5438510116584574, train accuracy = 0.4643333333333333 \n",
      "\t -- val loss = 1.6896956502519194, val accuracy = 0.4264 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.86it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4743868570667586, train accuracy = 0.48844444444444446 \n",
      "\t -- val loss = 1.6341271219503712, val accuracy = 0.4382 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.49it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.376991233373303, train accuracy = 0.5262 \n",
      "\t -- val loss = 1.5586015999973177, val accuracy = 0.4662 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.20it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3030992723021904, train accuracy = 0.5565333333333333 \n",
      "\t -- val loss = 1.4982397682488682, val accuracy = 0.4878 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.31it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2633354700355741, train accuracy = 0.5752222222222222 \n",
      "\t -- val loss = 1.4791690954917718, val accuracy = 0.4962 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.54it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6211060103808084, train accuracy = 0.4703555555555556 \n",
      "\t -- val loss = 1.6790678830710815, val accuracy = 0.4432 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.57it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.576725560713286, train accuracy = 0.4766666666666667 \n",
      "\t -- val loss = 1.6654630055542572, val accuracy = 0.4422 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.76it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5138722509776839, train accuracy = 0.5012888888888889 \n",
      "\t -- val loss = 1.6320300668924654, val accuracy = 0.4598 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.74it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5785342790620585, train accuracy = 0.4738222222222222 \n",
      "\t -- val loss = 1.6988748304671641, val accuracy = 0.4418 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.69it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5418110166096923, train accuracy = 0.4879777777777778 \n",
      "\t -- val loss = 1.6761546172122685, val accuracy = 0.445 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.00it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4453682439174185, train accuracy = 0.5280444444444444 \n",
      "\t -- val loss = 1.600362930426648, val accuracy = 0.4674 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.57it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3803158611258002, train accuracy = 0.5533555555555556 \n",
      "\t -- val loss = 1.5509730385544478, val accuracy = 0.485 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.66it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.339291131822982, train accuracy = 0.5716888888888889 \n",
      "\t -- val loss = 1.5182115620716248, val accuracy = 0.503 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.21it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.8854735848249948, train accuracy = 0.43855555555555553 \n",
      "\t -- val loss = 1.9135768920002278, val accuracy = 0.422 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.46it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.811856038287335, train accuracy = 0.42146666666666666 \n",
      "\t -- val loss = 1.8474744146069788, val accuracy = 0.4082 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.07it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.799336093607975, train accuracy = 0.4332888888888889 \n",
      "\t -- val loss = 1.8416265347415288, val accuracy = 0.4112 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.00it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.8556126652370435, train accuracy = 0.4060666666666667 \n",
      "\t -- val loss = 1.8900510857163573, val accuracy = 0.398 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.30it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.8407613658190451, train accuracy = 0.4096444444444444 \n",
      "\t -- val loss = 1.8855569391019293, val accuracy = 0.3912 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.19it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.7988708911540547, train accuracy = 0.4382888888888889 \n",
      "\t -- val loss = 1.8402880395660373, val accuracy = 0.4188 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.65it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.7681847661168026, train accuracy = 0.45595555555555556 \n",
      "\t -- val loss = 1.8095254071514673, val accuracy = 0.4368 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.99it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.75355868685855, train accuracy = 0.4626 \n",
      "\t -- val loss = 1.796406543855785, val accuracy = 0.4368 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.85it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6154576602308046, train accuracy = 0.46924444444444446 \n",
      "\t -- val loss = 1.6723480386901883, val accuracy = 0.4446 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.62it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5723202350898415, train accuracy = 0.47986666666666666 \n",
      "\t -- val loss = 1.6617737139218252, val accuracy = 0.4454 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.41it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5168011834082216, train accuracy = 0.4957111111111111 \n",
      "\t -- val loss = 1.639253902976198, val accuracy = 0.4532 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.46it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5787186577645345, train accuracy = 0.47173333333333334 \n",
      "\t -- val loss = 1.7038288459739508, val accuracy = 0.4354 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.20it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.539933165255654, train accuracy = 0.48966666666666664 \n",
      "\t -- val loss = 1.6849320492653301, val accuracy = 0.4484 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.88it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4394666897233075, train accuracy = 0.5271555555555556 \n",
      "\t -- val loss = 1.5923098205147348, val accuracy = 0.4632 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.65it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.371853476924321, train accuracy = 0.5544888888888889 \n",
      "\t -- val loss = 1.5419748019489832, val accuracy = 0.4832 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.01it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.336476837467757, train accuracy = 0.5709555555555555 \n",
      "\t -- val loss = 1.5180403138212422, val accuracy = 0.4998 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.45it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.7954899854306812, train accuracy = 0.4672222222222222 \n",
      "\t -- val loss = 1.8458730951595173, val accuracy = 0.4418 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.27it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.6893522424055436, train accuracy = 0.47113333333333335 \n",
      "\t -- val loss = 1.7633647032120559, val accuracy = 0.4448 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.16it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.6210320738936206, train accuracy = 0.48377777777777775 \n",
      "\t -- val loss = 1.7089756044094522, val accuracy = 0.445 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.24it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6639021969114283, train accuracy = 0.45713333333333334 \n",
      "\t -- val loss = 1.7580195572221018, val accuracy = 0.4316 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.84it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.6326109325206257, train accuracy = 0.4707777777777778 \n",
      "\t -- val loss = 1.7363226868701531, val accuracy = 0.4368 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.79it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.545293663239774, train accuracy = 0.5058444444444444 \n",
      "\t -- val loss = 1.6441299674842722, val accuracy = 0.4696 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.59it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.499885941087873, train accuracy = 0.5260888888888889 \n",
      "\t -- val loss = 1.6092409892039548, val accuracy = 0.4836 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.12it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.469486315870131, train accuracy = 0.5397777777777778 \n",
      "\t -- val loss = 1.5861913700463655, val accuracy = 0.491 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.80it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.6053358488714744, train accuracy = 0.4700666666666667 \n",
      "\t -- val loss = 1.662110967206931, val accuracy = 0.4422 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.76it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.5710268042355287, train accuracy = 0.47431111111111113 \n",
      "\t -- val loss = 1.6593411156465963, val accuracy = 0.4434 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.96it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.5153072123391502, train accuracy = 0.4943777777777778 \n",
      "\t -- val loss = 1.63052482983455, val accuracy = 0.4472 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.34it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5828962935880435, train accuracy = 0.46844444444444444 \n",
      "\t -- val loss = 1.7141529733930287, val accuracy = 0.4344 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.67it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.5263751773287766, train accuracy = 0.48886666666666667 \n",
      "\t -- val loss = 1.6769165269468345, val accuracy = 0.442 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.00it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.4312154740953809, train accuracy = 0.5268222222222222 \n",
      "\t -- val loss = 1.5951822321560836, val accuracy = 0.4666 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.07it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.369145672385472, train accuracy = 0.5545111111111111 \n",
      "\t -- val loss = 1.548551740312686, val accuracy = 0.4832 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.93it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.3301352108027957, train accuracy = 0.5696 \n",
      "\t -- val loss = 1.5296397808847466, val accuracy = 0.4952 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.30it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5445321292271863, train accuracy = 0.4706444444444444 \n",
      "\t -- val loss = 1.6028157976991337, val accuracy = 0.4412 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.93it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4996481094722176, train accuracy = 0.4815111111111111 \n",
      "\t -- val loss = 1.593363323943122, val accuracy = 0.4476 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.32it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4491834563305324, train accuracy = 0.5014888888888889 \n",
      "\t -- val loss = 1.5730497702173152, val accuracy = 0.4576 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.55it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5283028409776744, train accuracy = 0.47588888888888886 \n",
      "\t -- val loss = 1.6642808641217113, val accuracy = 0.4368 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.29it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4677819098042624, train accuracy = 0.49533333333333335 \n",
      "\t -- val loss = 1.6112808884783854, val accuracy = 0.4468 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.89it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3731105964820127, train accuracy = 0.5313555555555556 \n",
      "\t -- val loss = 1.534594155499925, val accuracy = 0.4756 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.05it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.3038907527023282, train accuracy = 0.5585333333333333 \n",
      "\t -- val loss = 1.4898074708047477, val accuracy = 0.481 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.01it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2607632997591591, train accuracy = 0.5754 \n",
      "\t -- val loss = 1.4666318193754813, val accuracy = 0.5016 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.52it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.768356175231658, train accuracy = 0.4678888888888889 \n",
      "\t -- val loss = 1.8179162395538628, val accuracy = 0.4428 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.65it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.6800991064939186, train accuracy = 0.47017777777777775 \n",
      "\t -- val loss = 1.7567591060647076, val accuracy = 0.4388 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.70it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.6005358307061464, train accuracy = 0.4873111111111111 \n",
      "\t -- val loss = 1.6995323796026547, val accuracy = 0.4544 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.00it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6640615772106941, train accuracy = 0.45993333333333336 \n",
      "\t -- val loss = 1.764594804354258, val accuracy = 0.4276 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.44it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.6077910924392722, train accuracy = 0.4777777777777778 \n",
      "\t -- val loss = 1.719753547349847, val accuracy = 0.438 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.11it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.529756038119347, train accuracy = 0.5086222222222222 \n",
      "\t -- val loss = 1.6391305634467814, val accuracy = 0.4714 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.57it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.4793282801970038, train accuracy = 0.5333555555555556 \n",
      "\t -- val loss = 1.5984242929427088, val accuracy = 0.4824 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.72it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.4485856420286312, train accuracy = 0.5472 \n",
      "\t -- val loss = 1.573683872942497, val accuracy = 0.493 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.13it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5194415115343989, train accuracy = 0.4694 \n",
      "\t -- val loss = 1.5781659091460865, val accuracy = 0.4404 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.63it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4719158016003169, train accuracy = 0.4796222222222222 \n",
      "\t -- val loss = 1.5660750918057842, val accuracy = 0.451 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.09it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.416088495983685, train accuracy = 0.5013333333333333 \n",
      "\t -- val loss = 1.540491906226345, val accuracy = 0.4576 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.69it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5163789969667312, train accuracy = 0.46844444444444444 \n",
      "\t -- val loss = 1.655983625631202, val accuracy = 0.4292 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.09it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4332505862197744, train accuracy = 0.4986888888888889 \n",
      "\t -- val loss = 1.5876609623929787, val accuracy = 0.4504 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.70it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3353530095218993, train accuracy = 0.5319777777777778 \n",
      "\t -- val loss = 1.508510748720293, val accuracy = 0.4714 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.60it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2607136174925724, train accuracy = 0.5626 \n",
      "\t -- val loss = 1.4603417223124158, val accuracy = 0.49 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.75it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.220317458595164, train accuracy = 0.5770444444444445 \n",
      "\t -- val loss = 1.4479932737509051, val accuracy = 0.5032 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.32it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.517975723472779, train accuracy = 0.47124444444444447 \n",
      "\t -- val loss = 1.57784374069092, val accuracy = 0.4478 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.37it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4755250555989747, train accuracy = 0.47931111111111113 \n",
      "\t -- val loss = 1.5680326981621577, val accuracy = 0.4498 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.41it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.420853835053486, train accuracy = 0.5035333333333334 \n",
      "\t -- val loss = 1.5442205743354507, val accuracy = 0.4574 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.29it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.5192271848294332, train accuracy = 0.4693111111111111 \n",
      "\t -- val loss = 1.6664605666226502, val accuracy = 0.4292 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.77it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4488706933445399, train accuracy = 0.49264444444444444 \n",
      "\t -- val loss = 1.6112888918677528, val accuracy = 0.4476 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.06it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.345323977104349, train accuracy = 0.5297555555555555 \n",
      "\t -- val loss = 1.523379375284246, val accuracy = 0.4694 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.84it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2706889746959351, train accuracy = 0.563 \n",
      "\t -- val loss = 1.4726055682144426, val accuracy = 0.4918 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.98it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2293060560434395, train accuracy = 0.5760666666666666 \n",
      "\t -- val loss = 1.4537473450430671, val accuracy = 0.4956 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.13it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.5184101691402727, train accuracy = 0.47155555555555556 \n",
      "\t -- val loss = 1.579481272052476, val accuracy = 0.4452 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 80.67it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4777517667950522, train accuracy = 0.47864444444444443 \n",
      "\t -- val loss = 1.575175311416555, val accuracy = 0.446 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.07it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4163994688105694, train accuracy = 0.505 \n",
      "\t -- val loss = 1.5481618715835908, val accuracy = 0.4636 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.54it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4947705931777842, train accuracy = 0.4698222222222222 \n",
      "\t -- val loss = 1.6459269149844933, val accuracy = 0.4306 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.47it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.429858526038069, train accuracy = 0.4998888888888889 \n",
      "\t -- val loss = 1.5890414124442835, val accuracy = 0.4458 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.20it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.336355225497438, train accuracy = 0.5337111111111111 \n",
      "\t -- val loss = 1.5091734555034357, val accuracy = 0.4716 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.10it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2679362542999022, train accuracy = 0.5605555555555556 \n",
      "\t -- val loss = 1.4656195014271562, val accuracy = 0.49 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.49it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2245532389128233, train accuracy = 0.5762888888888889 \n",
      "\t -- val loss = 1.4471405268076134, val accuracy = 0.5006 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 84.70it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.52520254147815, train accuracy = 0.4713333333333333 \n",
      "\t -- val loss = 1.5852435192269565, val accuracy = 0.4454 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.30it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.4812861686223324, train accuracy = 0.48188888888888887 \n",
      "\t -- val loss = 1.5785007691727568, val accuracy = 0.449 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.08it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.4392497157466952, train accuracy = 0.4998444444444444 \n",
      "\t -- val loss = 1.5721273195616945, val accuracy = 0.4462 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.57it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.4975048246195812, train accuracy = 0.4793777777777778 \n",
      "\t -- val loss = 1.6434475844312728, val accuracy = 0.4368 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.86it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.4605339111978597, train accuracy = 0.4914222222222222 \n",
      "\t -- val loss = 1.625123675712259, val accuracy = 0.4424 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.01it/s]\n",
      "epoch 6/8 \n",
      " \t -- train loss = 1.3497319550188953, train accuracy = 0.5328888888888889 \n",
      "\t -- val loss = 1.5261073546280763, val accuracy = 0.4732 \n",
      "\n",
      "\n",
      "starting epoch: 7 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 85.11it/s]\n",
      "epoch 7/8 \n",
      " \t -- train loss = 1.2817524275712926, train accuracy = 0.5608666666666666 \n",
      "\t -- val loss = 1.4891188225498475, val accuracy = 0.483 \n",
      "\n",
      "\n",
      "starting epoch: 8 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.11it/s]\n",
      "epoch 8/8 \n",
      " \t -- train loss = 1.2425564071589794, train accuracy = 0.5758888888888889 \n",
      "\t -- val loss = 1.470138790568686, val accuracy = 0.4934 \n",
      "\n",
      "\n",
      "model summary: \n",
      "layer 0: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.018042195912175808^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: relu\n",
      "\n",
      "layer 1: dense: \n",
      "\t w -- init:Xavier ~ 1.0 x N(0.0, 0.1414213562373095^2), reg: l2\n",
      "\t b -- init: Xavier ~ 1.0 x N(0.0, 1.0^2)\n",
      "\t activation: softmax\n",
      "\n",
      "\n",
      "starting epoch: 1 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 82.97it/s]\n",
      "epoch 1/8 \n",
      " \t -- train loss = 1.802594095529947, train accuracy = 0.46457777777777776 \n",
      "\t -- val loss = 1.8518801945786003, val accuracy = 0.4412 \n",
      "\n",
      "\n",
      "starting epoch: 2 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 83.17it/s]\n",
      "epoch 2/8 \n",
      " \t -- train loss = 1.6976591801984788, train accuracy = 0.46644444444444444 \n",
      "\t -- val loss = 1.7702805822742846, val accuracy = 0.4364 \n",
      "\n",
      "\n",
      "starting epoch: 3 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 81.17it/s]\n",
      "epoch 3/8 \n",
      " \t -- train loss = 1.621237041895319, train accuracy = 0.48213333333333336 \n",
      "\t -- val loss = 1.7130816696167719, val accuracy = 0.4446 \n",
      "\n",
      "\n",
      "starting epoch: 4 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:05<00:00, 78.59it/s]\n",
      "epoch 4/8 \n",
      " \t -- train loss = 1.6714486885759574, train accuracy = 0.45544444444444443 \n",
      "\t -- val loss = 1.7609452871190285, val accuracy = 0.4298 \n",
      "\n",
      "\n",
      "starting epoch: 5 ...\n",
      "batch 450/450: 100%|██████████| 450/450 [00:06<00:00, 73.74it/s]\n",
      "epoch 5/8 \n",
      " \t -- train loss = 1.6307481039119422, train accuracy = 0.47117777777777775 \n",
      "\t -- val loss = 1.7321868613406937, val accuracy = 0.4362 \n",
      "\n",
      "\n",
      "starting epoch: 6 ...\n",
      "batch 345/450:  76%|███████▋  | 344/450 [00:04<00:01, 72.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-5c28c7e93e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"reg_rate_l2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_hyper_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbest_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"best obj:{best_objective:.4f}, with {best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-810cdabff6a3>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, x_train, y_train, x_val, y_val, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# meaasure objective on model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mscores_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_val, y_val, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mlayers_reg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reg_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mscores_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reg_loss_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/layers.py\u001b[0m in \u001b[0;36mget_reg_loss_w\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_reg_grad_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/KTH/Git Stuff/nn-blocks/regularizers.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "objective = AccuracyMetrics()\n",
    "build_model = build_model_func\n",
    "n = 100\n",
    "n_epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "params = {\"reg_rate_l2\": sample_hyper_param(n=n)}\n",
    "tuner = Tuner(build_model, objective, iterations=1, **params)\n",
    "best_objective, best_params = tuner.search(x_train, y_train, x_val, y_val, n_epochs, batch_size)\n",
    "\n",
    "print(f\"best obj:{best_objective:.4f}, with {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-parks",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_blocks_env",
   "language": "python",
   "name": "nn_blocks_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
