[![build](https://github.com/mark-antal-csizmadia/nn-blocks/actions/workflows/main.yml/badge.svg)](https://github.com/mark-antal-csizmadia/nn-blocks/actions/workflows/main.yml)

# nn-blocks

A neural network library built from scratch, without dedicated deep learning packages. Training and testing deep neural networks and utilizing deep learning best practices for multi-class classification with fully connected neural networks and text generation with recurrent neural networks.

- <a href="https://nbviewer.jupyter.org/github/mark-antal-csizmadia/nn-blocks/blob/main/one-layer.ipynb">
    <img align="center" src="https://img.shields.io/badge/Jupyter-one%5Flayer.ipynb-informational?style=flat&logo=Jupyter&logoColor=F37626&color=blue" />
  </a>
  
  + one layer networks with Hinge and cross entropy losses
  + cyclical learning rate schedule for improved learning
  + exploring the effects of the initial learning rate of the cyclical learning rate schedule and L2 regularization strength on model performance, without hyperparameter search


- <a href="https://nbviewer.jupyter.org/github/mark-antal-csizmadia/nn-blocks/blob/main/two-layer.ipynb">
    <img align="center" src="https://img.shields.io/badge/Jupyter-two%5Flayer.ipynb-informational?style=flat&logo=Jupyter&logoColor=F37626&color=blue" />
  </a>
    
  + two layer networks with cross entropy loss
  + Xavier initialization
  + Bayesian hyperparameter search with hyperopt
    

- k-layer with bn
    + k-layer
    + all data
    + bn
    + dropout
    + imgaug
    
- rnn
    + rnn stuff
    + grad clip and loss smoothing
    
- regression
    + regression stuff
